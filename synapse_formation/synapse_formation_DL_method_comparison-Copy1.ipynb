{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of synapses\n",
    "\n",
    "In this jupyter notebook, we provide an example of how to extract explainable features and run a classification for the imaging flow cytometry dataset provided by:\n",
    "\n",
    "scifAI: An explainable AI python framework for the analysis of multi-channel imaging flow cytometry data\n",
    "\n",
    "\n",
    "We assume you have already installed the library. Otherwise you can install it using \n",
    "\n",
    "`!pip -q install <Path to the cloned module>`\n",
    "\n",
    "This notebook provides an example for deep learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we import a series of needed modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scifAI\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from imageio import imread\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from IPython.core.debugger import Tracer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# Compare Algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scifAI.dl.utils import calculate_weights, train_validation_test_split\n",
    "from scifAI.dl.dataset import DatasetGenerator\n",
    "from scifAI.dl.custom_transforms import ShuffleChannel\n",
    "from scifAI.dl.models import PretrainedModel, resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iflai.dl.models import PretrainedModel\n",
    "from skorch.callbacks import LRScheduler,Checkpoint,EpochScoring,EarlyStopping\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we provide a function for visualizing the result of the confusion matrix which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (plot_confusion_matrix, \n",
    "                             matthews_corrcoef, \n",
    "                             classification_report,\n",
    "                             confusion_matrix, \n",
    "                             accuracy_score, \n",
    "                             balanced_accuracy_score, \n",
    "                             cohen_kappa_score, \n",
    "                             f1_score,  \n",
    "                             precision_score, recall_score)\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "def classification_complete_report(y_true, y_pred, plot = True ): \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(15*\"----\")\n",
    "    print(\"matthews correlation coeff: %.2f\" % (matthews_corrcoef(y_true, y_pred)) )\n",
    "    print(\"Cohen Kappa score: %.2f\" % (cohen_kappa_score(y_true, y_pred)) )\n",
    "    print(\"Accuracy: %.2f & balanced Accuracy: %.2f\" % (accuracy_score(y_true, y_pred), balanced_accuracy_score(y_true, y_pred)) )\n",
    "    print(\"macro F1 score: %.2f & micro F1 score: %.2f\" % (f1_score(y_true, y_pred, average = \"macro\"), f1_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Precision score: %.2f & micro Precision score: %.2f\" % (precision_score(y_true, y_pred, average = \"macro\"), precision_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Recall score: %.2f & micro Recall score: %.2f\" % (recall_score(y_true, y_pred, average = \"macro\"), recall_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(15*\"----\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets calculate the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 261 ms, total: 2.19 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metadata = pd.read_csv(\"/pstore/data/DS4/synapse_data_features/metadata_subset.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>experiment</th>\n",
       "      <th>donor</th>\n",
       "      <th>condition</th>\n",
       "      <th>object_number</th>\n",
       "      <th>set</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>4147</td>\n",
       "      <td>train</td>\n",
       "      <td>B_cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>57117</td>\n",
       "      <td>train</td>\n",
       "      <td>B_T_cell_in_one_layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>48521</td>\n",
       "      <td>test</td>\n",
       "      <td>B_T_cell_in_one_layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>30947</td>\n",
       "      <td>train</td>\n",
       "      <td>Multiplets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>38634</td>\n",
       "      <td>train</td>\n",
       "      <td>B_cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>22111</td>\n",
       "      <td>train</td>\n",
       "      <td>Multiplets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>28850</td>\n",
       "      <td>validation</td>\n",
       "      <td>No_cell_cell_interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>11855</td>\n",
       "      <td>test</td>\n",
       "      <td>T_cell_with_signaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>18171</td>\n",
       "      <td>train</td>\n",
       "      <td>No_cell_cell_interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>15013</td>\n",
       "      <td>train</td>\n",
       "      <td>T_cell_with_B_cell_fragments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file    experiment  \\\n",
       "0     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "1     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "2     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "3     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "4     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "...                                                 ...           ...   \n",
       "5216  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5217  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5218  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5219  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5220  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "\n",
       "        donor condition  object_number         set  \\\n",
       "0     Donor_1      +SEA           4147       train   \n",
       "1     Donor_1      +SEA          57117       train   \n",
       "2     Donor_1      +SEA          48521        test   \n",
       "3     Donor_1      +SEA          30947       train   \n",
       "4     Donor_1      +SEA          38634       train   \n",
       "...       ...       ...            ...         ...   \n",
       "5216  Donor_9      -SEA          22111       train   \n",
       "5217  Donor_9      -SEA          28850  validation   \n",
       "5218  Donor_9      -SEA          11855        test   \n",
       "5219  Donor_9      -SEA          18171       train   \n",
       "5220  Donor_9      -SEA          15013       train   \n",
       "\n",
       "                             label  \n",
       "0                           B_cell  \n",
       "1            B_T_cell_in_one_layer  \n",
       "2            B_T_cell_in_one_layer  \n",
       "3                       Multiplets  \n",
       "4                           B_cell  \n",
       "...                            ...  \n",
       "5216                    Multiplets  \n",
       "5217      No_cell_cell_interaction  \n",
       "5218         T_cell_with_signaling  \n",
       "5219      No_cell_cell_interaction  \n",
       "5220  T_cell_with_B_cell_fragments  \n",
       "\n",
       "[5221 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_index = metadata.set.isin([\"train\",\"validation\",\"test\"]) \n",
    "\n",
    "metadata = metadata.loc[row_index,:].reset_index(drop = True)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>experiment</th>\n",
       "      <th>donor</th>\n",
       "      <th>condition</th>\n",
       "      <th>object_number</th>\n",
       "      <th>set</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>4147</td>\n",
       "      <td>train</td>\n",
       "      <td>B_cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>57117</td>\n",
       "      <td>train</td>\n",
       "      <td>B_T_cell_in_one_layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>48521</td>\n",
       "      <td>test</td>\n",
       "      <td>B_T_cell_in_one_layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>30947</td>\n",
       "      <td>train</td>\n",
       "      <td>Multiplets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_1</td>\n",
       "      <td>Donor_1</td>\n",
       "      <td>+SEA</td>\n",
       "      <td>38634</td>\n",
       "      <td>train</td>\n",
       "      <td>B_cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>22111</td>\n",
       "      <td>train</td>\n",
       "      <td>Multiplets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>28850</td>\n",
       "      <td>validation</td>\n",
       "      <td>No_cell_cell_interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>11855</td>\n",
       "      <td>test</td>\n",
       "      <td>T_cell_with_signaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>18171</td>\n",
       "      <td>train</td>\n",
       "      <td>No_cell_cell_interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>/pstore/data/DS4/synapse_formation_full_data/E...</td>\n",
       "      <td>Experiment_4</td>\n",
       "      <td>Donor_9</td>\n",
       "      <td>-SEA</td>\n",
       "      <td>15013</td>\n",
       "      <td>train</td>\n",
       "      <td>T_cell_with_B_cell_fragments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file    experiment  \\\n",
       "0     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "1     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "2     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "3     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "4     /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_1   \n",
       "...                                                 ...           ...   \n",
       "5216  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5217  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5218  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5219  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "5220  /pstore/data/DS4/synapse_formation_full_data/E...  Experiment_4   \n",
       "\n",
       "        donor condition  object_number         set  \\\n",
       "0     Donor_1      +SEA           4147       train   \n",
       "1     Donor_1      +SEA          57117       train   \n",
       "2     Donor_1      +SEA          48521        test   \n",
       "3     Donor_1      +SEA          30947       train   \n",
       "4     Donor_1      +SEA          38634       train   \n",
       "...       ...       ...            ...         ...   \n",
       "5216  Donor_9      -SEA          22111       train   \n",
       "5217  Donor_9      -SEA          28850  validation   \n",
       "5218  Donor_9      -SEA          11855        test   \n",
       "5219  Donor_9      -SEA          18171       train   \n",
       "5220  Donor_9      -SEA          15013       train   \n",
       "\n",
       "                             label  \n",
       "0                           B_cell  \n",
       "1            B_T_cell_in_one_layer  \n",
       "2            B_T_cell_in_one_layer  \n",
       "3                       Multiplets  \n",
       "4                           B_cell  \n",
       "...                            ...  \n",
       "5216                    Multiplets  \n",
       "5217      No_cell_cell_interaction  \n",
       "5218         T_cell_with_signaling  \n",
       "5219      No_cell_cell_interaction  \n",
       "5220  T_cell_with_B_cell_fragments  \n",
       "\n",
       "[5221 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_cell' 'B_T_cell_in_one_layer' 'Multiplets' 'Synapses_with_signaling'\n",
      " 'T_cell_with_B_cell_fragments' 'T_cell' 'No_cell_cell_interaction'\n",
      " 'Synapses_without_signaling' 'T_cell_with_signaling']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(metadata.label.unique())\n",
    "metadata.label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we have 31280  files with various labels. first we need to get rid of `unknown` labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = metadata.label != \"unknown\"\n",
    "\n",
    "metadata = metadata.loc[row_index,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets plot a random image per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import crop\n",
    "import h5py\n",
    "\n",
    "def crop_pad_h_w(image_dummy,reshape_size):\n",
    "    if image_dummy.shape[0] < reshape_size:\n",
    "        h1_pad = ( reshape_size - image_dummy.shape[0])/2\n",
    "        h1_pad = int(h1_pad)\n",
    "        h2_pad =  reshape_size - h1_pad - image_dummy.shape[0]\n",
    "        h1_crop = 0\n",
    "        h2_crop = 0\n",
    "    else:\n",
    "        h1_pad = 0\n",
    "        h2_pad = 0\n",
    "        h1_crop = ( reshape_size - image_dummy.shape[0])/2\n",
    "        h1_crop = abs(int(h1_crop))\n",
    "        h2_crop = image_dummy.shape[0]- reshape_size  - h1_crop\n",
    "\n",
    "    if image_dummy.shape[1] < reshape_size:\n",
    "        w1_pad = (reshape_size - image_dummy.shape[1])/2\n",
    "        w1_pad = int(w1_pad)\n",
    "        w2_pad = reshape_size - w1_pad - image_dummy.shape[1]\n",
    "        w1_crop = 0\n",
    "        w2_crop = 0\n",
    "    else:\n",
    "        w1_pad = 0\n",
    "        w2_pad = 0\n",
    "        w1_crop = (reshape_size - image_dummy.shape[1])/2\n",
    "        w1_crop = abs(int(w1_crop))\n",
    "        w2_crop = image_dummy.shape[1]- reshape_size  - w1_crop\n",
    "\n",
    "    h = [h1_crop, h2_crop, h1_pad, h2_pad]\n",
    "    w = [w1_crop, w2_crop, w1_pad, w2_pad] \n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all random seeds to the specific value, so the results are more reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = \"models\"\n",
    "log_dir = \"logs\"\n",
    "scaling_factor = 4095.\n",
    "reshape_size = 160\n",
    "train_transform = [\n",
    "        transforms.RandomResizedCrop(reshape_size, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         AddGaussianNoise(mean=0., std=0.005),\n",
    "        ]\n",
    "test_transform = [ ]\n",
    "num_classes = len(metadata.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels = np.asarray([\"BF\",\"Antibody\",\"CD18\",\"F-Actin\",\"MHCII\",\"CD3\",\"P-CD3zeta\",\"Live-Dead\"])\n",
    "selected_channels = np.arange(len(channels))\n",
    "num_channels = len(selected_channels)\n",
    "num_of_all_channels = len(channels)\n",
    "all_channels = np.arange(num_of_all_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>channel</th>\n",
       "      <th>PXPermute</th>\n",
       "      <th>DeepLift</th>\n",
       "      <th>LRP</th>\n",
       "      <th>GuidedGradCAM</th>\n",
       "      <th>IntegratedGradients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BF</td>\n",
       "      <td>-0.903509</td>\n",
       "      <td>5.130000e-05</td>\n",
       "      <td>881.029748</td>\n",
       "      <td>-8.440000e-09</td>\n",
       "      <td>3.560000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>-0.875932</td>\n",
       "      <td>7.140000e-06</td>\n",
       "      <td>39.048419</td>\n",
       "      <td>-6.990000e-09</td>\n",
       "      <td>1.760000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>CD18</td>\n",
       "      <td>-0.891383</td>\n",
       "      <td>3.430000e-06</td>\n",
       "      <td>-58.621100</td>\n",
       "      <td>-9.480000e-09</td>\n",
       "      <td>2.070000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>-0.853113</td>\n",
       "      <td>2.640000e-06</td>\n",
       "      <td>-188.193848</td>\n",
       "      <td>-3.980000e-09</td>\n",
       "      <td>1.860000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>-0.683771</td>\n",
       "      <td>-1.320000e-06</td>\n",
       "      <td>-93.692622</td>\n",
       "      <td>-7.710000e-09</td>\n",
       "      <td>-6.530000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>CD3</td>\n",
       "      <td>-0.670556</td>\n",
       "      <td>-1.390000e-06</td>\n",
       "      <td>42.120468</td>\n",
       "      <td>-1.530000e-08</td>\n",
       "      <td>2.070000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>-0.647172</td>\n",
       "      <td>2.840000e-06</td>\n",
       "      <td>-522.217394</td>\n",
       "      <td>3.050000e-10</td>\n",
       "      <td>1.680000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>-0.909705</td>\n",
       "      <td>8.390000e-06</td>\n",
       "      <td>12.706195</td>\n",
       "      <td>-8.140000e-10</td>\n",
       "      <td>5.830000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>BF</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>1.560000e-05</td>\n",
       "      <td>-10.388792</td>\n",
       "      <td>9.190000e-09</td>\n",
       "      <td>1.940000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>0.065707</td>\n",
       "      <td>7.200000e-06</td>\n",
       "      <td>-2.661150</td>\n",
       "      <td>1.930000e-09</td>\n",
       "      <td>7.070000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>CD18</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>1.810000e-06</td>\n",
       "      <td>2.364701</td>\n",
       "      <td>-6.540000e-09</td>\n",
       "      <td>1.770000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>0.090999</td>\n",
       "      <td>3.090000e-06</td>\n",
       "      <td>1.074309</td>\n",
       "      <td>-1.370000e-08</td>\n",
       "      <td>2.800000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>0.125450</td>\n",
       "      <td>-1.540000e-06</td>\n",
       "      <td>-0.066088</td>\n",
       "      <td>4.720000e-09</td>\n",
       "      <td>-1.290000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>CD3</td>\n",
       "      <td>0.296838</td>\n",
       "      <td>4.780000e-06</td>\n",
       "      <td>1.933981</td>\n",
       "      <td>-3.760000e-08</td>\n",
       "      <td>1.140000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>0.250630</td>\n",
       "      <td>9.410000e-06</td>\n",
       "      <td>3.005504</td>\n",
       "      <td>9.670000e-09</td>\n",
       "      <td>6.730000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>0.015609</td>\n",
       "      <td>2.210000e-06</td>\n",
       "      <td>1.204784</td>\n",
       "      <td>4.290000e-09</td>\n",
       "      <td>-5.490000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>BF</td>\n",
       "      <td>-0.002056</td>\n",
       "      <td>5.140000e-05</td>\n",
       "      <td>881.029748</td>\n",
       "      <td>-8.440000e-09</td>\n",
       "      <td>3.560000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>0.026193</td>\n",
       "      <td>7.120000e-06</td>\n",
       "      <td>39.048419</td>\n",
       "      <td>-6.990000e-09</td>\n",
       "      <td>1.760000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>CD18</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>3.420000e-06</td>\n",
       "      <td>-58.621100</td>\n",
       "      <td>-9.480000e-09</td>\n",
       "      <td>2.070000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>0.047570</td>\n",
       "      <td>2.630000e-06</td>\n",
       "      <td>-188.193848</td>\n",
       "      <td>-3.980000e-09</td>\n",
       "      <td>1.860000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>0.217425</td>\n",
       "      <td>-1.310000e-06</td>\n",
       "      <td>-93.692622</td>\n",
       "      <td>-7.710000e-09</td>\n",
       "      <td>-6.530000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>CD3</td>\n",
       "      <td>0.233971</td>\n",
       "      <td>-1.390000e-06</td>\n",
       "      <td>42.120468</td>\n",
       "      <td>-1.530000e-08</td>\n",
       "      <td>2.070000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>0.254399</td>\n",
       "      <td>2.820000e-06</td>\n",
       "      <td>-522.217394</td>\n",
       "      <td>3.050000e-10</td>\n",
       "      <td>1.680000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>-0.008448</td>\n",
       "      <td>8.390000e-06</td>\n",
       "      <td>12.706195</td>\n",
       "      <td>-8.140000e-10</td>\n",
       "      <td>5.830000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>BF</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>6.460000e-05</td>\n",
       "      <td>-697.783782</td>\n",
       "      <td>-1.490000e-08</td>\n",
       "      <td>6.750000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>1.160000e-05</td>\n",
       "      <td>11.136168</td>\n",
       "      <td>-1.870000e-08</td>\n",
       "      <td>1.460000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>CD18</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>2.160000e-06</td>\n",
       "      <td>-0.632308</td>\n",
       "      <td>-5.940000e-08</td>\n",
       "      <td>3.260000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>-8.130000e-09</td>\n",
       "      <td>-4.006168</td>\n",
       "      <td>-1.450000e-08</td>\n",
       "      <td>-3.130000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>0.182179</td>\n",
       "      <td>2.190000e-06</td>\n",
       "      <td>23.999712</td>\n",
       "      <td>5.010000e-08</td>\n",
       "      <td>-2.400000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>CD3</td>\n",
       "      <td>0.142352</td>\n",
       "      <td>-2.990000e-07</td>\n",
       "      <td>11.102955</td>\n",
       "      <td>-1.380000e-07</td>\n",
       "      <td>1.190000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>0.264751</td>\n",
       "      <td>1.540000e-06</td>\n",
       "      <td>-30.546750</td>\n",
       "      <td>-6.820000e-10</td>\n",
       "      <td>-1.540000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>7.980000e-06</td>\n",
       "      <td>2.766866</td>\n",
       "      <td>1.790000e-08</td>\n",
       "      <td>5.420000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>BF</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>3.180000e-05</td>\n",
       "      <td>160.555301</td>\n",
       "      <td>-6.710000e-09</td>\n",
       "      <td>4.860000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>0.051572</td>\n",
       "      <td>5.890000e-06</td>\n",
       "      <td>58.510809</td>\n",
       "      <td>-2.640000e-08</td>\n",
       "      <td>-4.230000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>CD18</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>1.450000e-06</td>\n",
       "      <td>-3.551677</td>\n",
       "      <td>-1.280000e-08</td>\n",
       "      <td>3.090000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>0.060149</td>\n",
       "      <td>2.540000e-06</td>\n",
       "      <td>-4.275520</td>\n",
       "      <td>-3.180000e-08</td>\n",
       "      <td>5.550000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>0.162335</td>\n",
       "      <td>1.270000e-06</td>\n",
       "      <td>-18.356707</td>\n",
       "      <td>-2.310000e-08</td>\n",
       "      <td>1.630000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>CD3</td>\n",
       "      <td>0.305563</td>\n",
       "      <td>2.380000e-06</td>\n",
       "      <td>-8.504885</td>\n",
       "      <td>-1.350000e-07</td>\n",
       "      <td>-9.240000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>0.257310</td>\n",
       "      <td>1.410000e-06</td>\n",
       "      <td>-22.061176</td>\n",
       "      <td>2.460000e-08</td>\n",
       "      <td>-7.440000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>-0.000348</td>\n",
       "      <td>1.770000e-05</td>\n",
       "      <td>-2.286231</td>\n",
       "      <td>-1.460000e-08</td>\n",
       "      <td>2.110000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>BF</td>\n",
       "      <td>-0.002370</td>\n",
       "      <td>6.710000e-05</td>\n",
       "      <td>356.112996</td>\n",
       "      <td>-6.650000e-09</td>\n",
       "      <td>4.980000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>Antibody</td>\n",
       "      <td>0.032653</td>\n",
       "      <td>1.620000e-05</td>\n",
       "      <td>18.292057</td>\n",
       "      <td>-8.200000e-09</td>\n",
       "      <td>6.740000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>CD18</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>1.630000e-06</td>\n",
       "      <td>0.757391</td>\n",
       "      <td>-1.010000e-08</td>\n",
       "      <td>8.150000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>F-Actin</td>\n",
       "      <td>0.072690</td>\n",
       "      <td>4.860000e-07</td>\n",
       "      <td>0.698691</td>\n",
       "      <td>-1.560000e-08</td>\n",
       "      <td>3.900000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>0.159444</td>\n",
       "      <td>-3.720000e-06</td>\n",
       "      <td>-9.520512</td>\n",
       "      <td>-1.670000e-08</td>\n",
       "      <td>-2.700000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>CD3</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>-7.570000e-06</td>\n",
       "      <td>-4.722511</td>\n",
       "      <td>-3.610000e-08</td>\n",
       "      <td>-1.070000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>P-CD3zeta</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>1.100000e-05</td>\n",
       "      <td>131.724315</td>\n",
       "      <td>2.640000e-08</td>\n",
       "      <td>5.870000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>Live-Dead</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>1.480000e-06</td>\n",
       "      <td>81.851013</td>\n",
       "      <td>-1.330000e-08</td>\n",
       "      <td>5.770000e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold    channel  PXPermute      DeepLift         LRP  GuidedGradCAM  \\\n",
       "0      1         BF  -0.903509  5.130000e-05  881.029748  -8.440000e-09   \n",
       "1      1   Antibody  -0.875932  7.140000e-06   39.048419  -6.990000e-09   \n",
       "2      1       CD18  -0.891383  3.430000e-06  -58.621100  -9.480000e-09   \n",
       "3      1    F-Actin  -0.853113  2.640000e-06 -188.193848  -3.980000e-09   \n",
       "4      1      MHCII  -0.683771 -1.320000e-06  -93.692622  -7.710000e-09   \n",
       "5      1        CD3  -0.670556 -1.390000e-06   42.120468  -1.530000e-08   \n",
       "6      1  P-CD3zeta  -0.647172  2.840000e-06 -522.217394   3.050000e-10   \n",
       "7      1  Live-Dead  -0.909705  8.390000e-06   12.706195  -8.140000e-10   \n",
       "8      2         BF   0.000334  1.560000e-05  -10.388792   9.190000e-09   \n",
       "9      2   Antibody   0.065707  7.200000e-06   -2.661150   1.930000e-09   \n",
       "10     2       CD18   0.017831  1.810000e-06    2.364701  -6.540000e-09   \n",
       "11     2    F-Actin   0.090999  3.090000e-06    1.074309  -1.370000e-08   \n",
       "12     2      MHCII   0.125450 -1.540000e-06   -0.066088   4.720000e-09   \n",
       "13     2        CD3   0.296838  4.780000e-06    1.933981  -3.760000e-08   \n",
       "14     2  P-CD3zeta   0.250630  9.410000e-06    3.005504   9.670000e-09   \n",
       "15     2  Live-Dead   0.015609  2.210000e-06    1.204784   4.290000e-09   \n",
       "16     1         BF  -0.002056  5.140000e-05  881.029748  -8.440000e-09   \n",
       "17     1   Antibody   0.026193  7.120000e-06   39.048419  -6.990000e-09   \n",
       "18     1       CD18   0.009469  3.420000e-06  -58.621100  -9.480000e-09   \n",
       "19     1    F-Actin   0.047570  2.630000e-06 -188.193848  -3.980000e-09   \n",
       "20     1      MHCII   0.217425 -1.310000e-06  -93.692622  -7.710000e-09   \n",
       "21     1        CD3   0.233971 -1.390000e-06   42.120468  -1.530000e-08   \n",
       "22     1  P-CD3zeta   0.254399  2.820000e-06 -522.217394   3.050000e-10   \n",
       "23     1  Live-Dead  -0.008448  8.390000e-06   12.706195  -8.140000e-10   \n",
       "24     4         BF   0.005561  6.460000e-05 -697.783782  -1.490000e-08   \n",
       "25     4   Antibody   0.018815  1.160000e-05   11.136168  -1.870000e-08   \n",
       "26     4       CD18   0.019357  2.160000e-06   -0.632308  -5.940000e-08   \n",
       "27     4    F-Actin   0.060871 -8.130000e-09   -4.006168  -1.450000e-08   \n",
       "28     4      MHCII   0.182179  2.190000e-06   23.999712   5.010000e-08   \n",
       "29     4        CD3   0.142352 -2.990000e-07   11.102955  -1.380000e-07   \n",
       "30     4  P-CD3zeta   0.264751  1.540000e-06  -30.546750  -6.820000e-10   \n",
       "31     4  Live-Dead  -0.002916  7.980000e-06    2.766866   1.790000e-08   \n",
       "32     5         BF   0.005900  3.180000e-05  160.555301  -6.710000e-09   \n",
       "33     5   Antibody   0.051572  5.890000e-06   58.510809  -2.640000e-08   \n",
       "34     5       CD18   0.009567  1.450000e-06   -3.551677  -1.280000e-08   \n",
       "35     5    F-Actin   0.060149  2.540000e-06   -4.275520  -3.180000e-08   \n",
       "36     5      MHCII   0.162335  1.270000e-06  -18.356707  -2.310000e-08   \n",
       "37     5        CD3   0.305563  2.380000e-06   -8.504885  -1.350000e-07   \n",
       "38     5  P-CD3zeta   0.257310  1.410000e-06  -22.061176   2.460000e-08   \n",
       "39     5  Live-Dead  -0.000348  1.770000e-05   -2.286231  -1.460000e-08   \n",
       "40     3         BF  -0.002370  6.710000e-05  356.112996  -6.650000e-09   \n",
       "41     3   Antibody   0.032653  1.620000e-05   18.292057  -8.200000e-09   \n",
       "42     3       CD18  -0.005101  1.630000e-06    0.757391  -1.010000e-08   \n",
       "43     3    F-Actin   0.072690  4.860000e-07    0.698691  -1.560000e-08   \n",
       "44     3      MHCII   0.159444 -3.720000e-06   -9.520512  -1.670000e-08   \n",
       "45     3        CD3   0.253565 -7.570000e-06   -4.722511  -3.610000e-08   \n",
       "46     3  P-CD3zeta   0.250994  1.100000e-05  131.724315   2.640000e-08   \n",
       "47     3  Live-Dead  -0.005865  1.480000e-06   81.851013  -1.330000e-08   \n",
       "\n",
       "    IntegratedGradients  \n",
       "0          3.560000e-05  \n",
       "1          1.760000e-06  \n",
       "2          2.070000e-06  \n",
       "3          1.860000e-06  \n",
       "4         -6.530000e-07  \n",
       "5          2.070000e-06  \n",
       "6          1.680000e-06  \n",
       "7          5.830000e-06  \n",
       "8          1.940000e-05  \n",
       "9          7.070000e-06  \n",
       "10         1.770000e-06  \n",
       "11         2.800000e-06  \n",
       "12        -1.290000e-06  \n",
       "13         1.140000e-05  \n",
       "14         6.730000e-06  \n",
       "15        -5.490000e-08  \n",
       "16         3.560000e-05  \n",
       "17         1.760000e-06  \n",
       "18         2.070000e-06  \n",
       "19         1.860000e-06  \n",
       "20        -6.530000e-07  \n",
       "21         2.070000e-06  \n",
       "22         1.680000e-06  \n",
       "23         5.830000e-06  \n",
       "24         6.750000e-05  \n",
       "25         1.460000e-06  \n",
       "26         3.260000e-06  \n",
       "27        -3.130000e-06  \n",
       "28        -2.400000e-06  \n",
       "29         1.190000e-05  \n",
       "30        -1.540000e-05  \n",
       "31         5.420000e-06  \n",
       "32         4.860000e-05  \n",
       "33        -4.230000e-07  \n",
       "34         3.090000e-06  \n",
       "35         5.550000e-07  \n",
       "36         1.630000e-06  \n",
       "37        -9.240000e-08  \n",
       "38        -7.440000e-06  \n",
       "39         2.110000e-05  \n",
       "40         4.980000e-05  \n",
       "41         6.740000e-06  \n",
       "42         8.150000e-07  \n",
       "43         3.900000e-07  \n",
       "44        -2.700000e-06  \n",
       "45        -1.070000e-05  \n",
       "46         5.870000e-06  \n",
       "47         5.770000e-06  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_importance = pd.read_csv(\"channel_importance.csv\")\n",
    "channel_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "interpretation_methods = [\"PXPermute\", \"DeepLift\", \"LRP\", \"GuidedGradCAM\",\"IntegratedGradients\"]\n",
    "channel_importance.loc[:, interpretation_methods] = MinMaxScaler().fit_transform(channel_importance.loc[:, interpretation_methods])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='variable', ylabel='value'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(data = pd.melt(channel_importance, id_vars=[\"fold\",\"channel\"]), \n",
    "            x = \"variable\", \n",
    "            y = \"value\",\n",
    "            hue = \"channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BF', 'Antibody', 'CD18', 'F-Actin', 'MHCII', 'CD3', 'P-CD3zeta',\n",
       "       'Live-Dead'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_importance.channel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_map = {\n",
    "    'BF':\"Ch0\", \n",
    "    'Antibody':\"Ch1\", \n",
    "    'CD18':\"Ch2\", \n",
    "    'F-Actin':\"Ch3\", \n",
    "    'MHCII':\"Ch4\", \n",
    "    'CD3':\"Ch5\", \n",
    "    'P-CD3zeta':\"Ch6\",\n",
    "    'Live-Dead':\"Ch7\"\n",
    "}\n",
    "\n",
    "channel_importance.channel = channel_importance.channel.replace(channel_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='variable', ylabel='value'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp8ElEQVR4nO3de3gU9fn38fedEIigiIJWBTFooSCCIJEeIopaz1aQ0haLRa3Fn/VAtSq1xcsn2vqz4q/1UKyVqkXUioU+Uqq2+ms5WR5bARUURAVFCTWCBKgocryfP2Z2s4RNspvN7CH5vK4r18zOzM7cu5nde7/fmbnH3B0REZFMFOU6ABERKXxKJiIikjElExERyZiSiYiIZEzJREREMtYm1wGkq0uXLl5WVpbrMERECsrixYs/cveDolp/wSWTsrIyFi1alOswREQKipm9F+X61c0lIiIZiyyZmNnDZrbOzF6vZ76Z2b1mttLMlprZcVHFIiIi0YqyZTIFOLOB+WcBPcO/y4D7I4xFREQiFNkxE3efb2ZlDSwyDJjqQT2Xf5pZJzM71N0/iComERGAHTt2UFVVxWeffZbrUJpdaWkp3bp1o6SkJKvbzeUB+K7AmoTHVeG0vZKJmV1G0Hqhe/fuWQlORFquqqoq9ttvP8rKyjCzXIfTbNydDRs2UFVVRY8ePbK67YI4AO/uk9293N3LDzoosjPbRKSV+Oyzz+jcuXOLSiQAZkbnzp1z0uLKZTJZCxye8LhbOE1EJHItLZHE5Op15TKZzALGhGd1fQnYrOMlIiKFKcpTg58AXgS+YGZVZnapmV1uZpeHizwLvAOsBH4LXBFVLLkyfvx4xowZw/jx43Mdiog0g4svvpgZM2bkZNurV6/mmGOOycm2UxHl2VwXNDLfgSuj2n4+qK6uZu1a9dyJSMtXEAfgRURyYerUqfTv359jjz2W73znOwDMnz+fr3zlKxx55JHxVsqWLVs49dRTOe644+jXrx9/+tOfgKA10adPH8aOHUvfvn05/fTT2bp1KwBDhw7lRz/6EYMHD6ZXr1688MILAOzatYsbbriB448/nv79+/PAAw/k4JWnT8kkB9T9JZL/li1bxs9+9jNmz57NkiVLuOeeewD44IMP+Mc//sHTTz/NjTfeCATXdjz11FO8/PLLzJkzh+uuu47YLdHffvttrrzySpYtW0anTp344x//GN/Gzp07eemll7j77ru55ZZbAHjooYfYf//9WbhwIQsXLuS3v/0t7777bpZfffoKrtBjS6DuL5H8N3v2bL7xjW/QpUsXAA488EAAhg8fTlFREUcffTQffvghEFzf8ZOf/IT58+dTVFTE2rVr4/N69OjBgAEDABg0aBCrV6+Ob2PEiBF7TX/++edZunRpvNWzefNm3n77bXr16hX1S86IkomISBratWsXH4+1Ph5//HHWr1/P4sWLKSkpoaysLH6tR+LyxcXF8W6uxHnFxcXs3Lkzvs5f/epXnHHGGXtsNzEJ5SN1c4mIJHHKKacwffp0NmzYAEBNTU29y27evJmDDz6YkpIS5syZw3vvNb3a+xlnnMH999/Pjh07AHjrrbf45JNPmry+bFHLREQkib59+zJhwgROOukkiouLGThwYL3Ljh49mq997Wv069eP8vJyevfu3eTtfu9732P16tUcd9xxuDsHHXQQM2fObPL6ssVizbRCUV5e7oVyc6wxY8awdu1aunbtytSpUxudLiLZ8cYbb9CnT59chxGZZK/PzBa7e3lU21Q3l4iIZEzJREREMqZkIiIiGVMyERGRjCmZiIhIxnRqcAs3fvx4qqurOeSQQ5g4cWKuwxGRFkrJpIVT6RaRxg26oXlP0V9855iUlquuruaaa65h4cKFdOrUic997nMMHz6cWbNm8fTTT++1/LvvvsuoUaPYsGEDgwYN4tFHH6Vt27bNGntTqZsrQyraKCJN4e6cf/75DB06lFWrVrF48WJuv/32eE2vZH70ox9x7bXXsnLlSg444AAeeuihLEbcMCWTDMV++VdXV+c6FBEpIHPmzKGkpITLL788Pu3YY49lyJAhbNmyhZEjR9K7d29Gjx6Nu+PuzJ49m5EjRwJw0UUX5dWV8ermEhHJgddff51BgwYlnffKK6+wbNkyDjvsMCoqKliwYAG9e/emU6dOtGkTfG1369Ytr7qwlUyk1dDJCFIoBg8eTLdu3QAYMGAAq1evzqjeVzaom0taDXVJSj7p27cvixcvTjqvbtn6nTt30rlzZzZt2hQvVV9VVUXXrl2zEmsqlExERHLglFNOYdu2bUyePDk+benSpfHb99ZlZpx88snxm2Y98sgjDBs2LCuxpkLdXCLS6qV6Km9zMjOeeuoprrnmGu644w5KS0spKytj+PDh9T7njjvuYNSoUdx0000MHDiQSy+9NHsBN0LJREQkRw477DD+8Ic/7DV97Nix8fFJkybFx4888kheeumlrMSWLnVziYhIxpRMREQkY+rmEpFWraqqih07dlBSUhI/HVfSp2QiIpHK9+t7duzYwfbt23MdRsFTMhGRSKnYaOugYyYiIpIxtUxEJO9ku2tsnyeD4onvN9P6ut/8WkrLpVuCftKkSdx9992sWrWK9evX06VLl2aKOHNKJln0xm2zAdheszU+jE3rM+GUnMUlkm9aQ9dYrAT9RRddxLRp0wBYsmQJs2bNqvc5FRUVnHvuuQwdOjRLUaZO3VwiIjmQbgl6gIEDB1JWVpajiBumlkkzu+3CkfHxmnWbg2H1B9x24UhG9LkiV2GJSJ5JtwT9CSeckOUI06OWiYhInomVoC8qKoqXoM93kSYTMzvTzN40s5VmdmOS+d3NbI6ZvWJmS83s7CjjERHJF+mWoM93kSUTMysG7gPOAo4GLjCzo+ssdhPwB3cfCIwCfh1VPJnQfd5FpLmlW4I+30V5zGQwsNLd3wEws2nAMGB5wjIOdAzH9wf+HWE8TdYaziwRac22fmsGbdu2pUePHlnbZlNK0N97771MnDiR6upq+vfvz9lnn82DDz6YtZgbEmUy6QqsSXhcBXyxzjKVwPNmdjXQAfhqhPGIiOSVdEvQjxs3jnHjxmUltnTl+gD8BcAUd+8GnA08amZ7xWRml5nZIjNbtH79+qwHKSIiDYuyZbIWODzhcbdwWqJLgTMB3P1FMysFugDrEhdy98nAZIDy8nLPJKh8LzonIlKIokwmC4GeZtaDIImMAr5dZ5n3gVOBKWbWBygFIm166PiHSP5SlYjCFVk3l7vvBK4CngPeIDhra5mZ3Wpm54WLXQeMNbMlwBPAxR671FNERApGpFfAu/uzwLN1pt2cML4cqIgyBhFpWdRVnZ9UTkVECoq6qvOTkkkTTbruzwBs+uiT+DA2TUQKy3ef/26zrm/B1QtSWi7dEvSjR49m0aJFlJSUMHjwYB544AFKSkqaNfamyvWpwXlFV7qLSLbEStAPHTqUVatWsXjxYm6//XY+/PDDep8zevRoVqxYwWuvvcbWrVvz5oJFUDLZQ6z5XF1dnetQRKSFa0oJ+rPPPhszw8wYPHgwVVVVuQp/Ly2ym6uQD9AVcuwikrpMStDv2LGDRx99lHvuuSdb4TaqRbZMCrmFUcixi0jzaKwE/RVXXMGJJ57IkCFDchNgEi0ymYiI5LumlqC/5ZZbWL9+Pb/85S8jjzEdSiYirZBONsm9ppSgf/DBB3nuued44oknKCrKr6/vFnnMREQalu/XanQu3X+PYdQePv3hgihBf/nll3PEEUfw5S9/GYARI0Zw880317t8NimZtFCDbpgKwH4ffUwx8P5HHzPohqksvnNMbgOTViHxmqu612Jd9YuvNfr8qwbWLePXMqVbgj6f77iYX+0kEZF6VFZWUllZSU1NDQA1NTVUVlbmNiiJUzIREZGMqZtLRPaga52kKZRMRGQP+X5wXvKTurlERCRjapmItCI6y6/W8jUfAVC8cxcA23fuYvmajzj68C65DKtgKZmISMYK/ThL9djL4uPzmmF9J81PbS3plqC/9NJLWbRoEe5Or169mDJlCvvuu28zRJw5dXOJSEoaumpeNeXS15QS9HfddRdLlixh6dKldO/efY9rUHJNyUREUqKE0byaUoK+Y8eOQJCItm7dipnlJPZklExERHKgsRL0d999N8uXL+edd95hwYLaOzdecsklHHLIIaxYsYKrr746W+E2SslERApKu3bt2GefffaorNvSNFSC/ne/+x3//ve/6dOnD08++WTugqxDB+AbMO/EkwDY2qYYzNhaVRWfxvHX5zAykeb1/q394uM7aw4E2rCz5j3ev7Uf3W9+LXeBJdGvX7/GFyoAffv2ZcaMGUnnNVSCPjZt1KhRTJw4kUsuuSTSOFOllolIK7S7bQd2tevI7rYdch1Kq5VuCXp3Z+XKlfHxWbNm0bt376zEmgq1TERaoU96np7rEPLKIb+dDEXF7CrtlLXrTNItQe/uXHTRRfznP//B3Tn22GO5//77sxJrKpRMRKTJ6u0KVjdwStItQZ94ID7ftIpkUkj9wSIihUjHTEREJGOtomUiIrnToW3HPYbSMimZiEikKo4akesQJAuUTESkQRW/qgCg7aa2FFHEmk1r4tP+W18hEmr1e0LsQwF7f1j0QRERSU2L+ras714NT+2X27hECkmX0t3AznDYOvzv5Lfi47ObYX1X/eJrKS2Xbgn6mHHjxvHwww+zZcuWZoi2ebSoZFLIKisrAaipqYkPKysr49NFsuX6/ptyHUKrECtBf9FFFzFt2jQAlixZwqxZsxp83qJFi9i4cWM2QkyLTg0WkZxo6P4orUFTStDv2rWLG264IS9vQBZpMjGzM83sTTNbaWY31rPMN81suZktM7PfRxmPiOSP1n5/lKaUoJ80aRLnnXcehx56aDZDTUlk3VxmVgzcB5wGVAELzWyWuy9PWKYn8GOgwt03mtnBUcUjrZfuex69TuEv59hQMhMrQQ/ES9AfeeSRTJ8+nblz5+Y2uHpEecxkMLDS3d8BMLNpwDBgecIyY4H73H0jgLuvizAeEYnIhbtaz8H65pJuCfpXXnmFlStX8vnPfx6ATz/9lM9//vPxSsK5FmU3V1dgTcLjqnBaol5ALzNbYGb/NLMzk63IzC4zs0Vmtmj9+vURhSsikj3plqA/55xzqK6uZvXq1axevZr27dvnTSKB3J/N1QboCQwFugHzzayfu29KXMjdJwOTAcrLy9WOFpFmddplvfK+BH2+izKZrAUOT3jcLZyWqAr4l7vvAN41s7cIksvCCOMSEckL6ZagT5RP15hAtN1cC4GeZtbDzNoCo4C6J1DPJGiVYGZdCLq93okwJhERiUBkLRN332lmVwHPAcXAw+6+zMxuBRa5+6xw3ulmthzYBdzg7huiiilfdC7df4+hiEihi/SYibs/CzxbZ9rNCeMO/DD8azWuGvjtXIcgItKscn0AvkUrLS7aYygi0lIpmURoYGdVmBTJd25FWFEwlKZr9N0zs8+Z2UNm9pfw8dFmdmn0oYmIRG93u47sKu3E7na6E2QmUmmZTAF+B0wIH78FPAk8FFFMIiJZ9dSPa4stPtUM65vwWPIr2+tKtwT9xRdfzLx589h//+DknSlTpjBgwIBmiDhzqSSTLu7+BzP7McTP0toVcVwiIi1aU0vQ33nnnYwcOTIbIaYllU7CT8ysM+AAZvYlYHOkUYmItHBNKUGfz1JJJj8kuNjwKDNbAEwFro40KhHJO97e2d1hN94+/7/YCkFTStADTJgwgf79+3Pttdeybdu2bIXbqEaTibu/DJwEfAX4L6Cvuy+NOjARyS87Knaw/bTt7KjYketQWrxYCfqioqJ4CXqA22+/nRUrVrBw4UJqamq44447chtoglTO5hoDfBsYBBwHXBBOE6BD2450aNeJDm11JoiIpK5v374sXrw46bxkJegBDj30UMyMdu3acckll/DSSy9lJdZUpNLNdXzC3xCgEjgvwpgKSsVRIzi9z8VUHDUi16GISAFJtwQ9wAcffAAEB+9nzpzJMcccE3mcqWr0bC533+P4iJl1AqZFFZA0r91tO+wxFJG9nX/7b+Lj+VyCfvTo0axfvx53Z8CAAfzmN7+pd9lsa8oV8J8APZo7EInGJz1Pz3UIIlKPdEvQz549OytxNUWjycTM/kx4WjBBt9jRwN6vXkREWq1UWib/kzC+E3jP3asiikdERApQKsdM5mUjEBERKVz1JhMz+5ja7q09ZhHcikTnwoqICNBAMnF31U8XEZGUpHw2l5kdDJTGHrv7+5FEJCIiBSeVs7nOA34BHAasA44A3gD6RhuaiEh22NTaClFvNMP6+kw4JaXl0i1B7+7cdNNNTJ8+neLiYr7//e8zbty4Zog4c6m0TH4KfAn4m7sPNLOTgQujDUtEpGVrSgn6KVOmsGbNGlasWEFRURHr1q3LVriNSiWZ7HD3DWZWZGZF7j7HzO6OOjCRbBo/fjzV1dUccsghTJw4Mdfh7CXf45P01VeCfuPGjfz9739n5MiR8crCjz32GGbG/fffz+9//3uKioJKWAcffHCuwt9LKrW5NpnZvsALwONmdg/BVfAiLUZ1dTVr166luro616Ekle/xSfqaUoJ+1apVPPnkk5SXl3PWWWfx9ttvZzPkBqWSTOYA+wM/AP4KrAK+FmVQIi3Z+PHjGTNmDOPHj09purQ+9ZWg37ZtG6WlpSxatIixY8fy3e9+N7eBJkglmbQBngfmAvsBT7r7hiiDEmnJ6mtlqPXRujSlBH23bt0YMSKoUH7++eezdGn+3FoqlZtj3eLufYErgUOBeWb2t8gjE4nY+7f2i//trHkPgJ017/H+rf1yHJm0Bk0pQT98+HDmzJkDwLx58+jVq1fkcaYqnarB64BqYAOQP0d9RArAbReOjI/XrNscDKs/4LYLRzLhsRn1Pq+ysjJYtqYmPqysrIxPl+bhY/rHx/O5BP2NN97I6NGjueuuu9h333158MEHsxJrKlK5zuQK4JvAQcB0YKy7L486MJF80NBZVE2dJxKTbgn6Tp068cwzz2QltnSl0jI5HLjG3V+NOBaRnOlSuhvYGQ5rxY5jJNPUeSItUSpVg3+cjUBEcun6/ptyHUKDYgdkEw/MiuSTptxpUUQyUFpctMcwFf366aQAyW9KJiJJVPyqAoC2m9pSRBFrNq2JT1tw9YKM1j2wswpyS8uT+k8jERGReqhlkoJO7nsMRZKZdN2f4+ObPvokPkycLtJSRZpMzOxM4B6gGHjQ3X9ez3JfB2YAx7v7oihjaooLd+1ufCERKVh/eGhS4wulIdXrgNItQT9kyBA+/vhjANatW8fgwYOZOXNmM0bedJElEzMrBu4DTgOqgIVmNqvuNSpmth9B3a9/RRWLiEi+aUoJ+sSr47/+9a8zbNiwyONMVZTHTAYDK939HXffDkwDkr3ynwJ3AJ9FGItIk3h7Z3eH3Xh7dXFK86qvBP2QIUPYsmULI0eOpHfv3owePRqv08X+n//8h9mzZzd4tXy2RdnN1RVYk/C4Cvhi4gJmdhxwuLs/Y2Y31LciM7sMuAyge/fuGQVV38VpIsnsqNix17R5J54EwNY2xWDG1qqqYNrx12c7vIIUKy1Tt6wMwIg+V+QsrmxrrAT9smXLOOyww6ioqGDBggWccMIJ8fkzZ87k1FNPpWPHjtkKt1E5OwBvZkXAL4GLG1vW3ScDkwHKy8sz+omY7xeniYjEStAD8RL0icnkiSee4Hvf+16uwksqym6utQSlWGK6hdNi9gOOAeaa2WqCWwPPMrPyCGMSyYoObTvSoV0nOrTNn1+Okl+aUoIe4KOPPuKll17inHPOiTzGdESZTBYCPc2sh5m1BUYB8SNL7r7Z3bu4e5m7lwH/BM7Lx7O5RNJVcdQITu9zMRVHjch1KJKnmlKCHmDGjBmce+65lJaWRh1iWiLr5nL3nWZ2FfAcwanBD7v7MjO7FVjk7vWfspAj3t7ZjQ62irQ237z0qvh4PpegB5g2bRo33nhjVmJMR6THTNz9WeDZOtNurmfZoVHGkopkB1tFRKKSbgl6gLlz50YdVpO0yCvgd7ftsMdQRESi1SKTySc9T891CE2mUuMiUohaZDIpZCo1LpId7o6Z5TqMZlf3AsdsUdVgkSbq5M6B7ioAWoBKS0vZ/unHOfvijYq7s2HDhpyc6aWWiUgTqQBo4erWrRvzn/8n3TqVUrdxYlvW5yaoJti4cSO7du2iuLiYAw44AAgSZeyCx2xSMhGRVqekpIR7576bdN7iO8dkOZqmGzNmDGvXrqVr165MnTo1p7Gom0tERDKmlolIjr1x22wAttdsjQ9j00QKhVomIiKSMSUTERHJmLq5pNVQZQSR6CiZSKtRyJURRPKdurlERCRjSiYiIpIxJRMREcmYkomIiGRMyURERDKms7lERHJs/PjxVFdXc8ghhzBx4sRch9MkSiYiIjlWXV3N2rVrcx1GRtTNJSIiGVMyERGRjKmbqxVrCf20Iq1NZWVlfLympiY+rKys3GNetimZtGItoZ9WRPKDkonkLbWcRAqHkonkLbWcRAqHkomISD3UOk6dkomISD3UOk6dTg0WEZGMqWUiIpLHCqWrTclERCSPFUpXm7q5REQkY2qZtDLv39ovPr6z5kCgDTtr3uP9W/vR/ebXcheYSAEplK6nbFIyERFJkMoPrubqerrtwpEA1KzbHAyrP4hPm/DYjIzXn02RdnOZ2Zlm9qaZrTSzG5PM/6GZLTezpWb2dzM7Isp4RESiNn78eMaMGcP48eMj31a7du3YZ599aNeuXeTbakxkLRMzKwbuA04DqoCFZjbL3ZcnLPYKUO7un5rZ94GJwLeiiknyj7oLanUu3X+PoeSfil9VANB2U1uKKGLNpjXxaQuuXgBk94B5v379Gl8oS6Ls5hoMrHT3dwDMbBowDIgnE3efk7D8P4ELI4xH8lChnKmSDVcN/HauQxBpsii7uboCaxIeV4XT6nMp8JcI45ECESulXbe8tojkr7w4AG9mFwLlwEn1zL8MuAyge/fuWYxMRGRv3t7ZzW68vec6lLwRZTJZCxye8LhbOG0PZvZVYAJwkrtvS7Yid58MTAYoLy/Xf09EsqJL6W5gZzistaNix17Lzjsx+C28tU0xmLG1qop5J57ESfPnxZdpyccIo0wmC4GeZtaDIImMAvboFDazgcADwJnuvi7CWCSJ+j4oIhK4vv+mZl1fSz5GGFkycfedZnYV8BxQDDzs7svM7FZgkbvPAu4E9gWmmxnA++5+XlQxyZ6a+4MiIq1XpMdM3P1Z4Nk6025OGP9qlNsXEZHsyIsD8CIiLdWk6/4cH9/00SfxYeL0ZN64bTYA22u2xoexaflIyUREpBl1ct9j2FoomUhOFNqvLpFUXbirdZ7QohL0IiKSMbVMRESypEPbjnsMWxIlE8lbsUqo+VARVaQ5VBw1ItchREbJRPbS0FW62byCN58qoopEqbS4aI9hIVIykb00dJVuS76CVyRXBnbeL9chZEzJRLKiJdckEhElE0mQyo1/mkotGpGWTclEUlJfRVRgj6qoItI6KZlITulWtSItg5KJ7KWpN/5pynER3apWpGVQMpG9JLvxTyp0XETS0RJOh5VaSiaSltZaxE6aX0s4HVZqKZlIWlprETsRaZjalyIikjG1TCRjsZv81L3xz+YPH4kvU7NuczCs/oDbLhzJiD5XZD9QEYmMWiYiIpIxtUxERPJYoVyLpWQiIpLHCuVaLHVziYhIxpRMREQkY+rmkmbTkm9JKiINUzKRZtOSb0kqIg1TN5eIiGRMyURERDKmbi7JClWIFWnZlEwkK1QhVqRl089EERHJmJKJiIhkTMlEREQypmQiIiIZUzIREZGMRZpMzOxMM3vTzFaa2Y1J5rczsyfD+f8ys7Io4xERkWhElkzMrBi4DzgLOBq4wMyOrrPYpcBGd/88cBdwR1TxiIhIdKJsmQwGVrr7O+6+HZgGDKuzzDAgdm/XGcCpZmYRxiQiIhEwd49mxWYjgTPd/Xvh4+8AX3T3qxKWeT1cpip8vCpc5qM667oMuCx8+AXgzUiCTk8X4KNGl2od9F4E9D7U0ntRK1/eiyPc/aCoVl4QV8C7+2Rgcq7jSGRmi9y9PNdx5AO9FwG9D7X0XtRqLe9FlN1ca4HDEx53C6clXcbM2gD7AxsijElERCIQZTJZCPQ0sx5m1hYYBcyqs8ws4KJwfCQw26PqdxMRkchE1s3l7jvN7CrgOaAYeNjdl5nZrcAid58FPAQ8amYrgRqChFMo8qrbLcf0XgT0PtTSe1GrVbwXkR2AFxGR1kNXwIuISMaUTEREJGMtKpmY2S4ze9XMXjez6WbW3swON7N3zezAcJkDwsdl4d/W8DnLzew3Zpa198TMhprZV7K1vYTtxt6nZWa2xMyui+J1m9lcMyuvM63czO4Nx9uZ2d/CWL5lZj9p7hiak5ltSTKt0szWJuxDFyTMmxLua6+a2ctm9uVmjudzZvZ7M3vHzBab2Ytmdn4jz3nWzDrV8zquT3P7WxLGe5rZ02a2KoxljpmdmM76kqx/ipltDcdLzOznZvZ2+F6+aGZnhfOuMbMvmZmb2Zl11uFm9ljC4zZmtt7Mnm5CPGVm9u0mvo6RCdv/7/B1vBr+TUh3nXXWPzT2eszsvGSlq1JcTyczu6KpcbSoZAJsdfcB7n4MsB243N3XAPcDPw+X+Tkw2d1Xh49XufsAoD9B2ZfhqWwoPJU5U0OBrCcTat+nvsBpBCVv/k82Nuzui9x9XPhwYDhtgLs/CeR1MmnAXeE+NAx4wMxKEubdEM67EXiguTYYVoqYCcx39yPdfRDBCSzdGnqeu5/t7puaK44wllLgGYLP1VFhLFcDRyZZtqmfm58ChwLHuPtxBJ/T2O07rwG+DfwDuKDO8z4BjjGzfcLHp7H3JQqpxlcWbicTPwMOA/qF+8UQoKTuQhZI+/vZ3We5+88bXzKpTkCTkwnu3mL+gC0J45cDvw7HS4ClBDvdMqAknF4GvJ7wnJ8D44GDgD8SnN68EKgI51cCjwILgCfCx48ALwDvASOAicBrwF8TtrMa6BKOlwNzw21XE+zYrxLsVEm3G+X7FD4+kuD6HiM48+7OcPtLgf9KWO6GhOm3JLyHK4DHgTcIyuK0D+fNBcrrbGso8DRwMLAS2By+/unArnD88VzvS6m8bwn7xPUJj6uBg8PxKcDIcLwU+LQZYzkVmFfPvIuBSQmPnwaGJtkXJwBvEXwJPxF7HcBR4f67ONy3e4fTewAvhvv3z2LvB0GNvUcaiLWSPT83ZeF6Xw7/vhIuZ8AkggoXfwOeBbYC7cP95IVw/4rtbwaMI/jhuD2M7d/AOeH4y8DOcH8eCZwdrqcKeBd4Os34/pmwv15LPZ+Vel7HyPB1bAD2q+d9KgufM5Xge+oIgh/Ci8LHtyQse2b4PrwM3JvwWuL/exr+HnuY4PP5DjAunD4tfL9fDV/XocD88PHrwJAG98lcf0Cj+LATnPL8J+D7CfPOABw4rc4/7/VwvH34hp8F/B44IZzeHXgj4Z+wGNgn4fE/CJLVscCnwFnhvKeA4Uk+wOXA3Hq+iJJuN6r3qc60TcDnCMrW3BROaxfuyD2A0wlOcTSCFu3TwInhe+gJO+rD1H4pzaWeZFJ3vL648umvnvetMuH1Hge8kDBvCrXJ5BvAv5oxlnEELaJk8y6mkWQCDCJICu2BjgSJPfY6/g70DMe/SHD9FwTXhY0Jx6+k9vP2S+AHDcRa93PTHigNx3sSXCoAwY+x/yX4kj4s3Ce3EvQavE3wRd4t3P9eTPisfEDQQoPgy3M50CF8vI2g4Oz/BdaE84YS/Ih7Os346u6v9X1Wkr2OkeHreKWB96kM2A18KWHageGwmODz1J/gh8maMDYD/kDyZNLQ99j/C2PuQpDgStj7x/V1wISE7SdNgrG/giinkoZ9zOzVcPwFgutYYs4i2OmOIfhHxxwVPseBP7n7X8zsEeBoq6052dHM9g3HZ7n71oTn/8Xdd5jZawRv+F/D6a8R/HPS8dVk23X3vfrqI3Q60D/Wx0tQlaBnOP104JVw+r7h9PeBNe6+IJz+GMEX3f9kLeLcu9bMLgF6AV+rM+9OM7sJWE/wCz4SZnYfcALBL/T7UnjKEOApd/80fP6scLgvQdfr9IT9sF04rAC+Ho4/Sj1Vvs3sKYJ94y13HxFOTvzclACTzGwAQWu0Vzj9ROAJd98F/NvMZhN8bmNe8to6fq8SfL7+AXQg+PEGwS/4c4EFYfwlBF++XyDo8poZLvfvOmGnEl9d9X1Wkr2OvYT7zA+AztR2d7/n7v9MWOybFtQmbEPQUjiaIJm+6+5vh+t5jNrahYmSfp+E48+4+zZgm5mtI/ghWddC4OGw23amu79az/sAFEhtrjRs9aAfcg/hTnEa8CXgH2Y2zd0/CGevSvKcIoJfB5/VWQ8EO2SibQDuvtvMdniYxgl+YcTe353UHp8qbSD+pNuNmpkdSfChWUfwS+dqd3+uzjJnALe7+wN1ppcRJOJEre3ipbvc/X/M7DzgITM7KuF/eIO7z4hgm8uo/WLH3a80sy4Ev44T9zdoeJ+rqwjYlOxzFNtUPbHED7a7+/nhiReJPygSPzfXAh8StOaLgMb295UE3aKrEqbtAtpYcKuL9sD1ZvaDcLyIoEvmYzPb4u6XmtmvgbEE3Vidk2yjKfHV91k5u4HX0d3M9nP3j939d8DvLCh4W1w3DjPrAVwPHO/uG81sCun/L+v7HtuWMGkXSXKBu88PT6I4B5hiZr9096kNbaxFCw9U3g9c4+7vE/QFNvar+XmCA4ixdQzIMIzVBN0KkPAFAHxM7UHEKLbbKDM7CPgNQdPYCSoWfD92ENnMeplZh3D6d2O/bMysq5kdHK6mu9WeqRQ7ENoUO+ocvC4oHlR1WERtiaAozQZKzez7CdPah8PVwAAzKzKzwwluB1HXfGC4me1jZvsRtqjc/T/Au2b2DYgfCD42fM4CaqtUjE5Y1++BijCZ1o0lmf2BD9x9N/Adar9I5wPfMrNiMzsUODmM6VOC4w59LSjNBMGX6vEEx462EnTnlAF9Cb4o/yu2MTPrRdAVt4XgMwfBr/x046v7ea3vs9LQ63iIoNVTGj6nGGhLch0JkstmM/scta20FUCZmR0VPq570kFMut8ne7w+MzsC+NDdfws8SNCNW68Wn0wIfo287+6xrq1fA33M7KQGnjMOKDezpWa2nOBgfiZuAe4xs0UEvwJi/gycH54eOCSC7dZnn3CbywgOED4fxgjBTrMceDn8xfQA0Mbdnyf40ngx7NKbQe2O9yZwpZm9ARxAkLxjnjGzqvBveiNxTQaWmtnjzfAao9A+4bVUmdkPkyxzK/DDppyJk44w8Q8HTrLg9OOXCE4G+RHBl/67BP/HewkO0tZ9/svAk8AS4C8EXRoxo4FLzWwJQatjWDj9BwT/59eArgnr2krQtXS5BacpvwjcRHCQPplfAxeF6+9N7a/xpwiOjSwnOAj9YsJzHiJIEsvD/fJcgiRyAcGZZH81sznuvh64HbjJzJYC+xCcQLASGEPQDf0Awedwc5rxLQV2WXA6/bXU81lp5HVMIOhuf93MXiHojn+EvbvdcPclBN3KKwg+ewvC6Z8RdGs9Y2YvE/QoJJPW94m7byDoHnzdzO4kOEa0JIzzW8A9DT1f5VQkI2E319MenI4tkrcsPP4Y9lbcB7zt7nflOq6WojW0TEREAMaGB+6XEXRlNdt1P6KWiYiINAO1TEREJGNKJiIikjElExERyZiSiUgGrJ4qvHWWSVrBwBKqyYoUupZ2BbxIVoSnl5q713e1s0iropaJtGoW3CPjyoTHlWZ2k5n93YL7ZrxmZsPCeWVm9qaZTSWoonq4ma0Oy5hgZjMtuJfHMgvqKSVu565w+t/DqgN14xhkZvPC5z8XXjktUjCUTKS1exL4ZsLjbxJckXy+B/fNOBn4RdgSgaCQ36/dva+7v1dnXd/14F4e5cA4M4vVgOpAUHm2LzCPOveOCctx/IqgwvAggsrLtzXbKxTJAnVzSavm7q+Y2cFmdhjB/R82EtyT5C4LitztJigdEquqWreqa6JxVnunw8MJEs+GcB1PhtMfIyiHnugLhNWsw5xVTFByQ6RgKJmIBDfmGgkcQvClP5ogsQwKby+wmtpqrXWrRgPBrVMJSn5/2d0/NbO51F/hte6VwgYsc/dmva2vSDapm0skSCCjCBLKdIJSG+vCRHIywR3vGrM/sDFMJL0JbncQUxSuG5JXVX4TOMjCyssW3O+8b5NfjUgOKJlIq+fuywgqIK8N73PzOEG11dcIKs2uSGE1fyW4v8YbBLd/TuwK+wQYHFaWPYWgsnDi9rcTJJs7wkq1r1J7sySRgqDaXCIikjG1TEREJGNKJiIikjElExERyZiSiYiIZEzJREREMqZkIiIiGVMyERGRjP1/cCeqbmoW56QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(data = pd.melt(channel_importance, id_vars=[\"fold\",\"channel\"]), \n",
    "            x = \"variable\", \n",
    "            y = \"value\",\n",
    "            hue = \"channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_channels(channel_importance, method, num_top_channels):\n",
    "    grouped_importance = channel_importance.loc[:,[\"channel\",method]].groupby(\"channel\").mean()\n",
    "    grouped_importance = grouped_importance.sort_values(by=method, ascending = False) \n",
    "    selected_channels = grouped_importance.index[:num_top_channels]\n",
    "    selected_channels = [int(ch.replace(\"Ch\",\"\")) for ch in selected_channels]\n",
    "    return selected_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(dataloader, selected_channels):\n",
    "    nmb_channels = len(selected_channels)\n",
    "\n",
    "    statistics = dict()\n",
    "    statistics[\"p01\"] = torch.zeros(nmb_channels)\n",
    "    statistics[\"p99\"] = torch.zeros(nmb_channels)\n",
    "    for _, data_l in enumerate(tqdm(dataloader), 0):\n",
    "        image, _ = data_l\n",
    "        for n in range(nmb_channels):\n",
    "            statistics[\"p01\"][n] += torch.quantile(image[:, n, :, :], 0.01)\n",
    "            statistics[\"p99\"][n] += torch.quantile(image[:, n, :, :], 0.99)\n",
    "\n",
    "    # averaging\n",
    "    for k in statistics:\n",
    "        statistics[k] = statistics[k].div_(len(dataloader))\n",
    "\n",
    "    print('statistics used: %s' % (str(statistics)))\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "    \n",
    "class MinMaxScaler(object):\n",
    "    def __init__(self, min_in , max_in, min_out, max_out):\n",
    "        self.min_in = min_in.reshape(-1,1,1)\n",
    "        self.max_in = max_in.reshape(-1,1,1)\n",
    "        self.min_out = min_out\n",
    "        self.max_out = max_out\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \n",
    "        tensor_ = (tensor - self.min_in)/(self.max_in - self.min_in)\n",
    "        tensor_ = tensor_*(self.max_out - self.min_out) + self.min_out\n",
    "        tensor_[tensor_<self.min_out]= self.min_out\n",
    "        tensor_[tensor_>self.max_out]= self.max_out\n",
    "        return tensor_\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(min_out={0}, max_out={1})'.format(self.min_out, self.max_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PXPermute [6, 5, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.0087, 0.0115, 0.0085]), 'p99': tensor([0.0113, 0.0872, 0.0719])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.8832\u001b[0m       \u001b[32m0.5526\u001b[0m            \u001b[35m0.4944\u001b[0m            \u001b[31m0.5526\u001b[0m        \u001b[94m1.9112\u001b[0m     +  7.7581\n",
      "      2        \u001b[36m0.5434\u001b[0m       \u001b[32m0.6746\u001b[0m            \u001b[35m0.6335\u001b[0m            \u001b[31m0.6746\u001b[0m        \u001b[94m1.1205\u001b[0m     +  6.9801\n",
      "      3        \u001b[36m0.4376\u001b[0m       \u001b[32m0.7823\u001b[0m            \u001b[35m0.7637\u001b[0m            \u001b[31m0.7823\u001b[0m        \u001b[94m0.5969\u001b[0m     +  7.1767\n",
      "      4        \u001b[36m0.3436\u001b[0m       \u001b[32m0.8469\u001b[0m            \u001b[35m0.8452\u001b[0m            \u001b[31m0.8469\u001b[0m        \u001b[94m0.4382\u001b[0m     +  6.9280\n",
      "      5        \u001b[36m0.3433\u001b[0m       0.8074            0.8024            0.8074        0.4913        6.8157\n",
      "      6        \u001b[36m0.3266\u001b[0m       0.7895            0.7831            0.7895        0.6075        6.7818\n",
      "      7        0.3562       0.7237            0.6860            0.7237        0.9132        6.7890\n",
      "      8        0.4005       0.8230            0.8229            0.8230        0.5544        6.7893\n",
      "      9        0.3377       0.7679            0.7530            0.7679        0.8119        6.7760\n",
      "     10        0.3897       0.8026            0.7926            0.8026        0.5078        6.7613\n",
      "     11        \u001b[36m0.3239\u001b[0m       \u001b[32m0.8529\u001b[0m            \u001b[35m0.8461\u001b[0m            \u001b[31m0.8529\u001b[0m        \u001b[94m0.3899\u001b[0m     +  6.7728\n",
      "     12        \u001b[36m0.2798\u001b[0m       0.8445            0.8397            0.8445        0.4486        6.8170\n",
      "     13        \u001b[36m0.2220\u001b[0m       0.8397            0.8374            0.8397        0.4966        6.7813\n",
      "     14        0.2744       0.8361            0.8291            0.8361        0.5304        6.7587\n",
      "     15        0.2654       \u001b[32m0.8624\u001b[0m            \u001b[35m0.8589\u001b[0m            \u001b[31m0.8624\u001b[0m        0.3914     +  6.7754\n",
      "     16        0.2481       0.8038            0.8035            0.8038        0.6312        6.8417\n",
      "     17        \u001b[36m0.1927\u001b[0m       0.8098            0.8039            0.8098        0.6053        6.8440\n",
      "     18        0.1977       0.8589            0.8582            0.8589        0.4312        6.8642\n",
      "     19        0.2215       0.8134            0.8086            0.8134        0.6377        6.7662\n",
      "     20        \u001b[36m0.1889\u001b[0m       0.8612            0.8543            0.8612        0.4962        6.8150\n",
      "     21        \u001b[36m0.1702\u001b[0m       0.7990            0.7861            0.7990        0.6991        6.7589\n",
      "     22        \u001b[36m0.1499\u001b[0m       0.8409            0.8353            0.8409        0.5566        6.8179\n",
      "     23        0.1783       0.8397            0.8351            0.8397        0.5514        6.7608\n",
      "     24        \u001b[36m0.1472\u001b[0m       \u001b[32m0.8804\u001b[0m            \u001b[35m0.8785\u001b[0m            \u001b[31m0.8804\u001b[0m        0.3962     +  6.7663\n",
      "     25        \u001b[36m0.1327\u001b[0m       0.8481            0.8440            0.8481        0.5729        6.8507\n",
      "     26        \u001b[36m0.1291\u001b[0m       0.8541            0.8504            0.8541        0.4906        6.8026\n",
      "     27        0.1394       0.8756            0.8716            0.8756        0.4020        6.8234\n",
      "     28        \u001b[36m0.1041\u001b[0m       0.8457            0.8386            0.8457        0.5768        6.7765\n",
      "     29        0.1287       0.8361            0.8262            0.8361        0.6562        6.8057\n",
      "     30        0.1047       \u001b[32m0.8900\u001b[0m            \u001b[35m0.8879\u001b[0m            \u001b[31m0.8900\u001b[0m        \u001b[94m0.3841\u001b[0m     +  6.7693\n",
      "     31        0.1052       0.8385            0.8338            0.8385        0.6506        6.9164\n",
      "     32        \u001b[36m0.0953\u001b[0m       0.8852            0.8831            0.8852        0.4551        6.8422\n",
      "     33        0.1732       0.8098            0.8059            0.8098        0.6934        6.8367\n",
      "     34        0.1043       0.8433            0.8374            0.8433        0.6153        6.7638\n",
      "     35        0.1729       0.8708            0.8683            0.8708        0.4030        6.7672\n",
      "     36        0.1241       0.8804            0.8786            0.8804        0.4668        6.7727\n",
      "     37        0.1005       0.8816            0.8807            0.8816        0.4508        6.7648\n",
      "     38        \u001b[36m0.0867\u001b[0m       0.8816            0.8803            0.8816        0.4570        6.7559\n",
      "     39        0.1001       0.8014            0.7893            0.8014        0.8022        6.7694\n",
      "     40        0.1277       0.8122            0.7920            0.8122        1.0281        6.7996\n",
      "     41        0.1472       \u001b[32m0.8911\u001b[0m            \u001b[35m0.8899\u001b[0m            \u001b[31m0.8911\u001b[0m        0.3940     +  6.7870\n",
      "     42        0.0891       0.8684            0.8627            0.8684        0.5528        6.9343\n",
      "     43        0.1522       0.8768            0.8738            0.8768        0.4249        6.9169\n",
      "     44        0.1223       0.8541            0.8487            0.8541        0.5214        6.8385\n",
      "     45        \u001b[36m0.0551\u001b[0m       0.8876            0.8853            0.8876        0.4100        6.7592\n",
      "     46        0.0694       0.8852            0.8825            0.8852        0.4165        6.7745\n",
      "     47        0.0780       0.8672            0.8622            0.8672        0.4686        6.7653\n",
      "     48        0.0695       0.8780            0.8737            0.8780        0.4536        6.7328\n",
      "     49        \u001b[36m0.0357\u001b[0m       \u001b[32m0.8935\u001b[0m            \u001b[35m0.8907\u001b[0m            \u001b[31m0.8935\u001b[0m        0.4299     +  6.7971\n",
      "     50        0.0358       0.8923            0.8900            0.8923        0.3970        6.7789\n",
      "     51        \u001b[36m0.0209\u001b[0m       \u001b[32m0.8959\u001b[0m            \u001b[35m0.8950\u001b[0m            \u001b[31m0.8959\u001b[0m        0.4115     +  6.7589\n",
      "     52        0.1139       0.8828            0.8790            0.8828        0.4463        6.8601\n",
      "     53        0.0591       0.8792            0.8764            0.8792        0.4473        6.9114\n",
      "     54        0.0753       0.8935            0.8906            0.8935        0.4547        6.8363\n",
      "     55        0.0348       0.8923            0.8906            0.8923        0.4147        6.7833\n",
      "     56        \u001b[36m0.0182\u001b[0m       0.8828            0.8806            0.8828        0.4318        6.8119\n",
      "     57        0.0493       0.8959            \u001b[35m0.8953\u001b[0m            0.8959        0.4361     +  6.8145\n",
      "     58        0.0250       0.8935            0.8928            0.8935        0.4096        6.8528\n",
      "     59        0.0471       0.8876            0.8866            0.8876        0.4842        6.7731\n",
      "     60        0.0414       0.8900            0.8876            0.8900        0.4566        6.7727\n",
      "     61        0.0196       \u001b[32m0.8983\u001b[0m            \u001b[35m0.8963\u001b[0m            \u001b[31m0.8983\u001b[0m        0.4319     +  6.7634\n",
      "     62        0.0186       0.8959            0.8945            0.8959        0.4656        6.8235\n",
      "     63        \u001b[36m0.0136\u001b[0m       \u001b[32m0.9019\u001b[0m            \u001b[35m0.9008\u001b[0m            \u001b[31m0.9019\u001b[0m        0.4301     +  6.8095\n",
      "     64        \u001b[36m0.0094\u001b[0m       0.8900            0.8883            0.8900        0.4472        6.7982\n",
      "     65        \u001b[36m0.0079\u001b[0m       0.8923            0.8899            0.8923        0.4541        6.7269\n",
      "     66        0.0167       0.8923            0.8908            0.8923        0.4411        6.7668\n",
      "     67        0.0090       0.8947            0.8932            0.8947        0.4362        6.7648\n",
      "     68        0.0186       0.9007            0.8989            0.9007        0.4440        6.7800\n",
      "     69        0.0079       \u001b[32m0.9031\u001b[0m            \u001b[35m0.9014\u001b[0m            \u001b[31m0.9031\u001b[0m        0.4237     +  6.7913\n",
      "     70        \u001b[36m0.0045\u001b[0m       0.8947            0.8927            0.8947        0.4379        6.7837\n",
      "     71        \u001b[36m0.0044\u001b[0m       0.8983            0.8968            0.8983        0.4321        6.8582\n",
      "     72        0.0058       0.8935            0.8918            0.8935        0.4535        6.7507\n",
      "     73        0.0074       0.8888            0.8863            0.8888        0.4689        6.7817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74        \u001b[36m0.0031\u001b[0m       0.8971            0.8955            0.8971        0.4424        6.7956\n",
      "     75        0.0047       0.8900            0.8882            0.8900        0.4561        6.7785\n",
      "     76        0.0096       0.9031            \u001b[35m0.9020\u001b[0m            0.9031        0.4422     +  6.7583\n",
      "     77        0.0209       0.8911            0.8875            0.8911        0.5274        6.8476\n",
      "     78        0.0574       0.8804            0.8777            0.8804        0.5323        6.8323\n",
      "     79        0.0235       0.8888            0.8866            0.8888        0.5047        6.7900\n",
      "     80        0.0305       0.8923            0.8907            0.8923        0.4856        6.7647\n",
      "     81        0.0148       0.8995            0.8976            0.8995        0.4190        6.7598\n",
      "     82        0.0074       0.8971            0.8948            0.8971        0.4384        6.7684\n",
      "     83        0.0067       0.9031            0.9013            0.9031        0.4212        6.7477\n",
      "     84        0.0159       0.8995            0.8983            0.8995        0.4674        6.7618\n",
      "     85        0.0164       0.8959            0.8937            0.8959        0.4566        6.8212\n",
      "     86        0.0076       0.9019            0.9003            0.9019        0.4350        6.8187\n",
      "     87        0.0041       0.8995            0.8976            0.8995        0.4394        6.7377\n",
      "     88        0.0058       0.9007            0.8989            0.9007        0.4577        6.7627\n",
      "     89        0.0053       0.8959            0.8942            0.8959        0.4673        6.7773\n",
      "     90        0.0045       0.8971            0.8957            0.8971        0.4542        6.7525\n",
      "     91        0.0036       0.9007            0.8997            0.9007        0.4518        6.7577\n",
      "     92        0.0038       0.8995            0.8982            0.8995        0.4482        6.7187\n",
      "     93        0.0034       0.8983            0.8967            0.8983        0.4623        6.7842\n",
      "     94        \u001b[36m0.0027\u001b[0m       0.8983            0.8966            0.8983        0.4652        6.7625\n",
      "     95        0.0029       0.8995            0.8978            0.8995        0.4585        6.7505\n",
      "     96        0.0034       0.9031            0.9015            0.9031        0.4519        6.7582\n",
      "     97        \u001b[36m0.0025\u001b[0m       0.8971            0.8953            0.8971        0.4592        6.7977\n",
      "     98        0.0025       0.8995            0.8977            0.8995        0.4488        6.7865\n",
      "     99        0.0033       0.8983            0.8966            0.8983        0.4572        6.7624\n",
      "    100        0.0035       0.8983            0.8970            0.8983        0.4547        6.7695\n",
      "    101        0.0041       0.8959            0.8941            0.8959        0.4681        6.7704\n",
      "    102        0.0035       0.8995            0.8975            0.8995        0.4568        6.8008\n",
      "    103        0.0051       0.8983            0.8966            0.8983        0.4588        6.7594\n",
      "    104        \u001b[36m0.0023\u001b[0m       0.8995            0.8982            0.8995        0.4772        6.7927\n",
      "    105        0.0024       0.8983            0.8966            0.8983        0.4713        6.7792\n",
      "    106        0.0038       0.8995            0.8980            0.8995        0.4784        6.7669\n",
      "    107        \u001b[36m0.0022\u001b[0m       0.8983            0.8965            0.8983        0.4714        6.7593\n",
      "    108        0.0026       0.9007            0.8991            0.9007        0.4754        6.7456\n",
      "    109        \u001b[36m0.0020\u001b[0m       0.9007            0.8992            0.9007        0.4685        6.7914\n",
      "    110        \u001b[36m0.0019\u001b[0m       0.9007            0.8992            0.9007        0.4671        6.7717\n",
      "    111        0.0060       0.9007            0.8991            0.9007        0.4861        6.7414\n",
      "    112        0.0027       0.9007            0.8992            0.9007        0.4672        6.7916\n",
      "    113        0.0020       0.9031            0.9013            0.9031        0.4621        6.7511\n",
      "    114        0.0021       0.9019            0.9004            0.9019        0.4599        6.7713\n",
      "    115        \u001b[36m0.0019\u001b[0m       0.9031            0.9014            0.9031        0.4612        6.7238\n",
      "    116        0.0022       0.9031            0.9014            0.9031        0.4699        6.7320\n",
      "    117        0.0026       0.8947            0.8931            0.8947        0.4711        6.7807\n",
      "    118        0.0033       0.8959            0.8942            0.8959        0.4762        6.7712\n",
      "    119        0.0043       0.8983            0.8964            0.8983        0.4635        6.7958\n",
      "    120        0.0022       0.8995            0.8978            0.8995        0.4722        6.7880\n",
      "    121        0.0027       0.8983            0.8962            0.8983        0.4846        6.7759\n",
      "    122        0.0023       0.9007            0.8990            0.9007        0.4708        6.7805\n",
      "    123        0.0019       0.8959            0.8944            0.8959        0.4765        6.7802\n",
      "    124        0.0019       0.8983            0.8969            0.8983        0.4783        6.7623\n",
      "    125        0.0035       0.8983            0.8967            0.8983        0.4815        6.7443\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.0087, 0.0115, 0.0085]), 'p99': tensor([0.0113, 0.0860, 0.0693])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.8200\u001b[0m       \u001b[32m0.5048\u001b[0m            \u001b[35m0.4381\u001b[0m            \u001b[31m0.5048\u001b[0m        \u001b[94m2.9406\u001b[0m     +  6.9649\n",
      "      2        \u001b[36m0.5120\u001b[0m       \u001b[32m0.6483\u001b[0m            \u001b[35m0.5586\u001b[0m            \u001b[31m0.6483\u001b[0m        \u001b[94m1.2620\u001b[0m     +  6.8535\n",
      "      3        \u001b[36m0.4048\u001b[0m       \u001b[32m0.7333\u001b[0m            \u001b[35m0.7293\u001b[0m            \u001b[31m0.7333\u001b[0m        \u001b[94m0.7665\u001b[0m     +  6.8253\n",
      "      4        0.5009       0.6579            0.6376            0.6579        1.1724        6.7765\n",
      "      5        \u001b[36m0.3648\u001b[0m       \u001b[32m0.8361\u001b[0m            \u001b[35m0.8304\u001b[0m            \u001b[31m0.8361\u001b[0m        \u001b[94m0.4871\u001b[0m     +  6.7979\n",
      "      6        \u001b[36m0.3107\u001b[0m       \u001b[32m0.8589\u001b[0m            \u001b[35m0.8573\u001b[0m            \u001b[31m0.8589\u001b[0m        \u001b[94m0.3902\u001b[0m     +  6.7651\n",
      "      7        \u001b[36m0.2929\u001b[0m       \u001b[32m0.8636\u001b[0m            0.8573            \u001b[31m0.8636\u001b[0m        0.4080        6.7689\n",
      "      8        0.3001       0.8469            0.8430            0.8469        0.4739        6.7819\n",
      "      9        \u001b[36m0.2562\u001b[0m       0.8062            0.8011            0.8062        0.5184        6.7653\n",
      "     10        0.3088       0.8122            0.7922            0.8122        0.5319        6.7612\n",
      "     11        0.2788       0.8385            0.8375            0.8385        0.4345        6.7745\n",
      "     12        0.2632       0.8397            0.8374            0.8397        0.4972        6.7615\n",
      "     13        \u001b[36m0.2403\u001b[0m       0.7811            0.7734            0.7811        0.7253        6.7751\n",
      "     14        \u001b[36m0.1995\u001b[0m       0.8445            0.8412            0.8445        0.5261        6.7593\n",
      "     15        0.2083       0.8230            0.8161            0.8230        0.6551        6.7417\n",
      "     16        0.2084       0.8481            0.8406            0.8481        0.5320        6.7630\n",
      "     17        0.2161       0.8337            0.8219            0.8337        0.5869        6.7869\n",
      "     18        0.2369       0.8373            0.8315            0.8373        0.5174        6.7698\n",
      "     19        0.2340       \u001b[32m0.8780\u001b[0m            \u001b[35m0.8716\u001b[0m            \u001b[31m0.8780\u001b[0m        \u001b[94m0.3866\u001b[0m     +  6.7456\n",
      "     20        \u001b[36m0.1981\u001b[0m       0.8206            0.8128            0.8206        0.6557        6.8458\n",
      "     21        0.2100       0.8541            0.8479            0.8541        0.4497        6.8248\n",
      "     22        \u001b[36m0.1479\u001b[0m       0.8696            0.8642            0.8696        0.4893        6.8028\n",
      "     23        \u001b[36m0.1310\u001b[0m       0.8349            0.8265            0.8349        0.6163        6.7543\n",
      "     24        0.1962       0.8373            0.8346            0.8373        0.5587        6.7402\n",
      "     25        0.1595       0.8696            0.8657            0.8696        0.3935        6.7632\n",
      "     26        \u001b[36m0.1208\u001b[0m       0.8672            0.8617            0.8672        0.4978        6.7729\n",
      "     27        \u001b[36m0.1010\u001b[0m       0.8624            0.8573            0.8624        0.5365        6.7672\n",
      "     28        0.2255       0.8385            0.8314            0.8385        0.5629        6.7373\n",
      "     29        0.1860       0.8349            0.8300            0.8349        0.6121        6.7737\n",
      "     30        \u001b[36m0.0923\u001b[0m       0.8624            0.8555            0.8624        0.5771        6.7533\n",
      "     31        \u001b[36m0.0746\u001b[0m       0.8373            0.8291            0.8373        0.6317        6.7717\n",
      "     32        0.1351       0.8708            0.8656            0.8708        0.4835        6.7703\n",
      "     33        \u001b[36m0.0746\u001b[0m       0.8505            0.8437            0.8505        0.5439        6.7521\n",
      "     34        0.2631       0.8648            0.8602            0.8648        0.4735        6.7443\n",
      "     35        0.1484       0.8409            0.8339            0.8409        0.6004        6.7789\n",
      "     36        0.1409       \u001b[32m0.8804\u001b[0m            \u001b[35m0.8768\u001b[0m            \u001b[31m0.8804\u001b[0m        0.3989     +  6.7664\n",
      "     37        0.1013       0.8756            0.8688            0.8756        0.5006        6.8188\n",
      "     38        0.1077       0.8660            0.8623            0.8660        0.4622        6.8200\n",
      "     39        0.0757       0.8301            0.8262            0.8301        0.6768        6.8072\n",
      "     40        \u001b[36m0.0629\u001b[0m       0.8744            0.8682            0.8744        0.4929        6.7603\n",
      "     41        \u001b[36m0.0482\u001b[0m       \u001b[32m0.8816\u001b[0m            \u001b[35m0.8779\u001b[0m            \u001b[31m0.8816\u001b[0m        0.4152     +  6.7237\n",
      "     42        \u001b[36m0.0359\u001b[0m       0.8816            \u001b[35m0.8789\u001b[0m            0.8816        0.4452     +  6.7682\n",
      "     43        0.0407       0.8684            0.8633            0.8684        0.4950        6.8476\n",
      "     44        \u001b[36m0.0296\u001b[0m       0.8720            0.8673            0.8720        0.4917        6.8115\n",
      "     45        \u001b[36m0.0195\u001b[0m       \u001b[32m0.8828\u001b[0m            0.8777            \u001b[31m0.8828\u001b[0m        0.4702        6.8340\n",
      "     46        \u001b[36m0.0162\u001b[0m       0.8768            0.8732            0.8768        0.4917        6.8033\n",
      "     47        \u001b[36m0.0118\u001b[0m       \u001b[32m0.8852\u001b[0m            \u001b[35m0.8795\u001b[0m            \u001b[31m0.8852\u001b[0m        0.5401     +  6.7481\n",
      "     48        \u001b[36m0.0117\u001b[0m       0.8828            0.8782            0.8828        0.5088        6.8112\n",
      "     49        0.0224       0.8804            0.8755            0.8804        0.5133        6.7813\n",
      "     50        0.0233       0.8852            0.8780            0.8852        0.5094        6.7744\n",
      "     51        0.0298       0.8696            0.8676            0.8696        0.5389        6.7968\n",
      "     52        0.0344       0.8768            0.8717            0.8768        0.5859        6.7501\n",
      "     53        0.0630       0.8744            0.8694            0.8744        0.5787        6.7267\n",
      "     54        0.0352       0.8648            0.8597            0.8648        0.5569        6.7509\n",
      "     55        0.0774       0.8565            0.8524            0.8565        0.7161        6.7451\n",
      "     56        0.0240       \u001b[32m0.8876\u001b[0m            \u001b[35m0.8837\u001b[0m            \u001b[31m0.8876\u001b[0m        0.5270     +  6.7746\n",
      "     57        0.0134       0.8852            0.8802            0.8852        0.5122        6.8209\n",
      "     58        0.0509       0.8828            0.8788            0.8828        0.5108        6.7658\n",
      "     59        0.0203       0.8876            0.8826            0.8876        0.5255        6.7723\n",
      "     60        0.0377       0.8840            0.8801            0.8840        0.5122        6.7741\n",
      "     61        0.0391       0.8732            0.8673            0.8732        0.5604        6.7810\n",
      "     62        0.0344       0.8840            0.8805            0.8840        0.5165        6.7574\n",
      "     63        0.0168       0.8876            0.8826            0.8876        0.5034        6.7531\n",
      "     64        0.0125       0.8852            0.8806            0.8852        0.4878        6.7398\n",
      "     65        \u001b[36m0.0102\u001b[0m       \u001b[32m0.8911\u001b[0m            \u001b[35m0.8861\u001b[0m            \u001b[31m0.8911\u001b[0m        0.4752     +  6.7894\n",
      "     66        \u001b[36m0.0067\u001b[0m       0.8900            0.8853            0.8900        0.4746        6.8163\n",
      "     67        0.0071       0.8876            0.8827            0.8876        0.4876        6.8055\n",
      "     68        \u001b[36m0.0054\u001b[0m       0.8864            0.8813            0.8864        0.4855        6.7755\n",
      "     69        0.0056       \u001b[32m0.8935\u001b[0m            \u001b[35m0.8894\u001b[0m            \u001b[31m0.8935\u001b[0m        0.4969     +  6.7618\n",
      "     70        0.0064       0.8852            0.8806            0.8852        0.5066        6.8283\n",
      "     71        \u001b[36m0.0046\u001b[0m       0.8876            0.8832            0.8876        0.4928        6.8432\n",
      "     72        0.0066       0.8911            0.8864            0.8911        0.5047        6.7369\n",
      "     73        0.0087       0.8840            0.8793            0.8840        0.5187        6.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74        0.0094       0.8792            0.8753            0.8792        0.5291        6.7797\n",
      "     75        0.0052       0.8864            0.8816            0.8864        0.5147        6.7496\n",
      "     76        \u001b[36m0.0045\u001b[0m       0.8888            0.8836            0.8888        0.5297        6.7534\n",
      "     77        \u001b[36m0.0041\u001b[0m       0.8888            0.8840            0.8888        0.5273        6.7430\n",
      "     78        \u001b[36m0.0036\u001b[0m       0.8911            0.8869            0.8911        0.5392        6.7606\n",
      "     79        0.0044       0.8804            0.8743            0.8804        0.5548        6.7905\n",
      "     80        0.0037       0.8864            0.8809            0.8864        0.5537        6.7664\n",
      "     81        \u001b[36m0.0036\u001b[0m       0.8864            0.8813            0.8864        0.5456        6.7901\n",
      "     82        \u001b[36m0.0033\u001b[0m       0.8911            0.8865            0.8911        0.5475        6.7631\n",
      "     83        0.0035       0.8876            0.8826            0.8876        0.5568        6.7639\n",
      "     84        0.0128       0.8828            0.8788            0.8828        0.5926        6.7424\n",
      "     85        0.0285       0.8816            0.8780            0.8816        0.5871        6.7480\n",
      "     86        0.0071       0.8840            0.8791            0.8840        0.5546        6.7792\n",
      "     87        0.0049       0.8828            0.8781            0.8828        0.5643        6.7793\n",
      "     88        0.0054       0.8804            0.8760            0.8804        0.5629        6.7864\n",
      "     89        0.0046       0.8852            0.8806            0.8852        0.5509        6.7639\n",
      "     90        \u001b[36m0.0031\u001b[0m       0.8828            0.8775            0.8828        0.5557        6.7643\n",
      "     91        0.0036       0.8828            0.8779            0.8828        0.5619        6.7898\n",
      "     92        0.0084       0.8852            0.8795            0.8852        0.5583        6.7673\n",
      "     93        0.0076       0.8840            0.8788            0.8840        0.5387        6.7793\n",
      "     94        0.0048       0.8864            0.8812            0.8864        0.5570        6.7661\n",
      "     95        0.0042       0.8923            0.8860            0.8923        0.5517        6.7482\n",
      "     96        \u001b[36m0.0030\u001b[0m       0.8840            0.8792            0.8840        0.5434        6.7792\n",
      "     97        0.0035       0.8888            0.8840            0.8888        0.5561        6.7886\n",
      "     98        \u001b[36m0.0027\u001b[0m       0.8792            0.8735            0.8792        0.5643        6.7584\n",
      "     99        0.0038       0.8768            0.8717            0.8768        0.5759        6.7602\n",
      "    100        0.0028       0.8768            0.8709            0.8768        0.5794        6.7700\n",
      "    101        0.0030       0.8852            0.8805            0.8852        0.5680        6.7807\n",
      "    102        0.0035       0.8900            0.8858            0.8900        0.5624        6.7705\n",
      "    103        \u001b[36m0.0020\u001b[0m       0.8840            0.8792            0.8840        0.5698        6.7486\n",
      "    104        0.0033       0.8864            0.8825            0.8864        0.5550        6.7812\n",
      "    105        0.0244       0.8792            0.8753            0.8792        0.5925        6.7901\n",
      "    106        0.0036       0.8828            0.8774            0.8828        0.6122        6.7496\n",
      "    107        0.0028       0.8864            0.8814            0.8864        0.5874        6.7544\n",
      "    108        0.0025       0.8911            0.8865            0.8911        0.5782        6.7785\n",
      "    109        0.0033       0.8864            0.8816            0.8864        0.5711        6.7759\n",
      "    110        0.0024       0.8852            0.8814            0.8852        0.5985        6.7738\n",
      "    111        0.0029       0.8828            0.8787            0.8828        0.5983        6.7471\n",
      "    112        0.0024       0.8792            0.8745            0.8792        0.6071        6.7960\n",
      "    113        \u001b[36m0.0016\u001b[0m       0.8876            0.8837            0.8876        0.5871        6.7542\n",
      "    114        0.0022       0.8876            0.8836            0.8876        0.5913        6.7938\n",
      "    115        0.0032       0.8864            0.8827            0.8864        0.5978        6.7493\n",
      "    116        0.0017       0.8852            0.8813            0.8852        0.6042        6.7832\n",
      "    117        0.0024       0.8840            0.8799            0.8840        0.5996        6.7474\n",
      "    118        0.0022       0.8816            0.8776            0.8816        0.6078        6.7908\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.0088, 0.0115, 0.0085]), 'p99': tensor([0.0113, 0.0863, 0.0717])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.8722\u001b[0m       \u001b[32m0.5622\u001b[0m            \u001b[35m0.4818\u001b[0m            \u001b[31m0.5622\u001b[0m        \u001b[94m2.5125\u001b[0m     +  6.9441\n",
      "      2        \u001b[36m0.5172\u001b[0m       \u001b[32m0.8038\u001b[0m            \u001b[35m0.7825\u001b[0m            \u001b[31m0.8038\u001b[0m        \u001b[94m0.5345\u001b[0m     +  6.8378\n",
      "      3        \u001b[36m0.4606\u001b[0m       0.7081            0.6811            0.7081        0.8359        6.7868\n",
      "      4        \u001b[36m0.4120\u001b[0m       \u001b[32m0.8158\u001b[0m            \u001b[35m0.8101\u001b[0m            \u001b[31m0.8158\u001b[0m        \u001b[94m0.5019\u001b[0m     +  6.7851\n",
      "      5        \u001b[36m0.3579\u001b[0m       0.7727            0.7601            0.7727        0.6563        6.7652\n",
      "      6        \u001b[36m0.3329\u001b[0m       0.7919            0.7874            0.7919        0.6194        6.7966\n",
      "      7        \u001b[36m0.3276\u001b[0m       \u001b[32m0.8457\u001b[0m            \u001b[35m0.8448\u001b[0m            \u001b[31m0.8457\u001b[0m        \u001b[94m0.4408\u001b[0m     +  6.7671\n",
      "      8        \u001b[36m0.2953\u001b[0m       0.8074            0.7935            0.8074        0.6420        6.7863\n",
      "      9        0.3351       0.7715            0.7509            0.7715        0.8588        6.8063\n",
      "     10        0.3132       \u001b[32m0.8529\u001b[0m            0.8424            \u001b[31m0.8529\u001b[0m        \u001b[94m0.4133\u001b[0m        6.7670\n",
      "     11        \u001b[36m0.2597\u001b[0m       0.8493            \u001b[35m0.8471\u001b[0m            0.8493        \u001b[94m0.3876\u001b[0m     +  6.7739\n",
      "     12        0.2660       \u001b[32m0.8995\u001b[0m            \u001b[35m0.8958\u001b[0m            \u001b[31m0.8995\u001b[0m        \u001b[94m0.2827\u001b[0m     +  6.8114\n",
      "     13        \u001b[36m0.2379\u001b[0m       0.8445            0.8333            0.8445        0.4805        6.8754\n",
      "     14        \u001b[36m0.2050\u001b[0m       0.8589            0.8562            0.8589        0.3708        6.8162\n",
      "     15        0.3314       0.8014            0.7941            0.8014        0.5340        6.7856\n",
      "     16        0.2489       0.8828            0.8807            0.8828        0.3381        6.7528\n",
      "     17        \u001b[36m0.1944\u001b[0m       0.8756            0.8702            0.8756        0.3927        6.7854\n",
      "     18        \u001b[36m0.1596\u001b[0m       0.8768            0.8664            0.8768        0.3873        6.7820\n",
      "     19        \u001b[36m0.1380\u001b[0m       0.8624            0.8534            0.8624        0.4093        6.7787\n",
      "     20        0.1772       0.8553            0.8476            0.8553        0.4289        6.7820\n",
      "     21        0.1490       0.8816            0.8776            0.8816        0.4034        6.7916\n",
      "     22        0.1485       0.8624            0.8556            0.8624        0.3883        6.7727\n",
      "     23        0.1531       0.8505            0.8482            0.8505        0.4754        6.7793\n",
      "     24        0.1910       0.8289            0.8252            0.8289        0.5900        6.7417\n",
      "     25        \u001b[36m0.1215\u001b[0m       0.8768            0.8740            0.8768        0.3984        6.7767\n",
      "     26        0.1476       0.8086            0.8015            0.8086        0.7304        6.7702\n",
      "     27        0.1614       0.8110            0.7979            0.8110        0.7598        6.7577\n",
      "     28        0.2804       0.8553            0.8483            0.8553        0.4848        6.7675\n",
      "     29        0.1898       0.8146            0.8070            0.8146        0.7949        6.7935\n",
      "     30        0.1842       0.8541            0.8466            0.8541        0.5909        6.7656\n",
      "     31        0.1562       0.8612            0.8563            0.8612        0.4648        6.7458\n",
      "     32        0.1283       0.8971            0.8930            0.8971        0.3416        6.7641\n",
      "     33        \u001b[36m0.0870\u001b[0m       0.8971            0.8943            0.8971        0.3028        6.8173\n",
      "     34        \u001b[36m0.0697\u001b[0m       0.8900            0.8868            0.8900        0.3759        6.7827\n",
      "     35        \u001b[36m0.0602\u001b[0m       0.8720            0.8710            0.8720        0.4308        6.7725\n",
      "     36        0.0960       0.8708            0.8652            0.8708        0.4142        6.7806\n",
      "     37        0.1039       0.8720            0.8708            0.8720        0.4477        6.7511\n",
      "     38        0.0638       0.8911            0.8858            0.8911        0.3940        6.7940\n",
      "     39        \u001b[36m0.0406\u001b[0m       0.8959            0.8926            0.8959        0.3332        6.7299\n",
      "     40        0.0436       0.8840            0.8808            0.8840        0.3867        6.7653\n",
      "     41        \u001b[36m0.0272\u001b[0m       0.8900            0.8866            0.8900        0.3817        6.7697\n",
      "     42        \u001b[36m0.0232\u001b[0m       0.8911            0.8868            0.8911        0.4325        6.7221\n",
      "     43        0.0713       0.8780            0.8758            0.8780        0.4399        6.7889\n",
      "     44        0.0378       \u001b[32m0.9031\u001b[0m            \u001b[35m0.9006\u001b[0m            \u001b[31m0.9031\u001b[0m        0.3336     +  6.7647\n",
      "     45        0.0290       0.8900            0.8867            0.8900        0.3817        6.8327\n",
      "     46        \u001b[36m0.0185\u001b[0m       0.9007            0.8962            0.9007        0.3711        6.8362\n",
      "     47        0.0198       0.8995            0.8962            0.8995        0.3472        6.8147\n",
      "     48        \u001b[36m0.0143\u001b[0m       0.8720            0.8648            0.8720        0.4083        6.7683\n",
      "     49        0.1351       0.8744            0.8709            0.8744        0.4670        6.7319\n",
      "     50        0.0686       0.8708            0.8687            0.8708        0.4459        6.7692\n",
      "     51        0.0629       0.8852            0.8824            0.8852        0.3890        6.7613\n",
      "     52        0.0548       0.8756            0.8715            0.8756        0.4467        6.7932\n",
      "     53        0.1152       0.8636            0.8595            0.8636        0.6244        6.7853\n",
      "     54        0.0503       0.9007            0.8976            0.9007        0.3953        6.7676\n",
      "     55        0.0436       0.8864            0.8837            0.8864        0.4173        6.7981\n",
      "     56        0.0205       0.9031            0.9004            0.9031        0.3754        6.7819\n",
      "     57        0.0161       0.9007            0.8980            0.9007        0.3840        6.7568\n",
      "     58        \u001b[36m0.0098\u001b[0m       0.9007            0.8978            0.9007        0.3822        6.7551\n",
      "     59        0.0117       0.8959            0.8929            0.8959        0.3908        6.7474\n",
      "     60        0.0245       0.8911            0.8888            0.8911        0.4360        6.7665\n",
      "     61        0.0123       0.9007            0.8979            0.9007        0.4066        6.7590\n",
      "     62        \u001b[36m0.0059\u001b[0m       0.9007            0.8979            0.9007        0.3805        6.7894\n",
      "     63        0.0060       0.9007            0.8979            0.9007        0.3901        6.7602\n",
      "     64        \u001b[36m0.0054\u001b[0m       \u001b[32m0.9079\u001b[0m            \u001b[35m0.9050\u001b[0m            \u001b[31m0.9079\u001b[0m        0.3922     +  6.7814\n",
      "     65        0.0246       0.8780            0.8745            0.8780        0.4874        6.8019\n",
      "     66        0.0129       0.9019            0.8996            0.9019        0.4532        6.8071\n",
      "     67        0.0072       0.8923            0.8883            0.8923        0.4186        6.8166\n",
      "     68        0.0075       0.8971            0.8945            0.8971        0.4023        6.7657\n",
      "     69        0.0070       0.8959            0.8942            0.8959        0.4144        6.7497\n",
      "     70        0.0062       0.8935            0.8906            0.8935        0.4024        6.7582\n",
      "     71        \u001b[36m0.0043\u001b[0m       0.8983            0.8955            0.8983        0.3882        6.7449\n",
      "     72        \u001b[36m0.0029\u001b[0m       0.9031            0.9005            0.9031        0.3925        6.7548\n",
      "     73        \u001b[36m0.0029\u001b[0m       0.9007            0.8984            0.9007        0.4120        6.7551\n",
      "     74        0.0032       \u001b[32m0.9103\u001b[0m            \u001b[35m0.9079\u001b[0m            \u001b[31m0.9103\u001b[0m        0.3914     +  6.7788\n",
      "     75        0.0032       0.9091            0.9067            0.9091        0.3865        6.8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     76        \u001b[36m0.0024\u001b[0m       0.9043            0.9023            0.9043        0.3930        6.8059\n",
      "     77        0.0117       0.8876            0.8852            0.8876        0.4279        6.7862\n",
      "     78        0.0058       0.8995            0.8961            0.8995        0.4037        6.7494\n",
      "     79        0.0026       0.8959            0.8933            0.8959        0.4366        6.7654\n",
      "     80        \u001b[36m0.0019\u001b[0m       0.9019            0.8992            0.9019        0.4099        6.7602\n",
      "     81        0.0022       0.8983            0.8957            0.8983        0.4249        6.7774\n",
      "     82        0.0021       0.9079            0.9054            0.9079        0.4127        6.7489\n",
      "     83        0.0118       0.9019            0.8992            0.9019        0.4161        6.7723\n",
      "     84        0.0049       0.8983            0.8961            0.8983        0.4128        6.7492\n",
      "     85        0.0029       0.9019            0.8997            0.9019        0.4035        6.7667\n",
      "     86        0.0141       0.8923            0.8894            0.8923        0.4186        6.7551\n",
      "     87        0.0034       0.8911            0.8887            0.8911        0.4103        6.7790\n",
      "     88        0.0031       0.8935            0.8909            0.8935        0.4203        6.7722\n",
      "     89        \u001b[36m0.0019\u001b[0m       0.8971            0.8942            0.8971        0.4204        6.7859\n",
      "     90        0.0031       0.8971            0.8946            0.8971        0.4116        6.7761\n",
      "     91        0.0020       0.9007            0.8982            0.9007        0.4131        6.7875\n",
      "     92        0.0020       0.8959            0.8936            0.8959        0.4175        6.7482\n",
      "     93        \u001b[36m0.0017\u001b[0m       0.8983            0.8960            0.8983        0.4177        6.7856\n",
      "     94        \u001b[36m0.0016\u001b[0m       0.8995            0.8972            0.8995        0.4185        6.7567\n",
      "     95        0.0039       0.9007            0.8974            0.9007        0.4092        6.7726\n",
      "     96        0.0017       0.8959            0.8931            0.8959        0.4055        6.7305\n",
      "     97        0.0019       0.8983            0.8951            0.8983        0.4096        6.7710\n",
      "     98        0.0105       0.8995            0.8971            0.8995        0.4285        6.7513\n",
      "     99        0.0065       0.9031            0.9004            0.9031        0.4051        6.7783\n",
      "    100        \u001b[36m0.0015\u001b[0m       0.9055            0.9029            0.9055        0.3972        6.7699\n",
      "    101        \u001b[36m0.0014\u001b[0m       0.9007            0.8981            0.9007        0.3944        6.7590\n",
      "    102        0.0029       0.9043            0.9016            0.9043        0.4008        6.7513\n",
      "    103        0.0082       0.8995            0.8970            0.8995        0.4335        6.7505\n",
      "    104        0.0028       0.9079            0.9053            0.9079        0.4045        6.7751\n",
      "    105        0.0024       0.9019            0.8986            0.9019        0.4126        6.7529\n",
      "    106        0.0025       0.9091            0.9064            0.9091        0.4067        6.7713\n",
      "    107        0.0017       0.9043            0.9013            0.9043        0.3929        6.7973\n",
      "    108        0.0023       0.9079            0.9047            0.9079        0.3908        6.7719\n",
      "    109        0.0015       \u001b[32m0.9127\u001b[0m            \u001b[35m0.9103\u001b[0m            \u001b[31m0.9127\u001b[0m        0.3992     +  6.7462\n",
      "    110        0.0015       0.9103            0.9082            0.9103        0.3987        6.8216\n",
      "    111        \u001b[36m0.0012\u001b[0m       0.9127            \u001b[35m0.9105\u001b[0m            0.9127        0.3969     +  6.8392\n",
      "    112        0.0016       0.9019            0.8991            0.9019        0.4025        6.8211\n",
      "    113        0.0018       0.9115            0.9086            0.9115        0.4007        6.7891\n",
      "    114        0.0015       0.9079            0.9050            0.9079        0.4161        6.7686\n",
      "    115        0.0014       \u001b[32m0.9139\u001b[0m            \u001b[35m0.9112\u001b[0m            \u001b[31m0.9139\u001b[0m        0.4036     +  6.7570\n",
      "    116        0.0014       0.9115            0.9090            0.9115        0.4168        6.8373\n",
      "    117        0.0013       0.9043            0.9012            0.9043        0.4201        6.8254\n",
      "    118        0.0018       0.9115            0.9088            0.9115        0.4226        6.7822\n",
      "    119        0.0013       0.9067            0.9040            0.9067        0.4225        6.7647\n",
      "    120        0.0013       0.9091            0.9063            0.9091        0.4116        6.7669\n",
      "    121        0.0013       0.9115            0.9086            0.9115        0.4157        6.7739\n",
      "    122        \u001b[36m0.0010\u001b[0m       0.9055            0.9027            0.9055        0.4159        6.7549\n",
      "    123        0.0012       0.9055            0.9026            0.9055        0.4130        6.7984\n",
      "    124        \u001b[36m0.0010\u001b[0m       0.9043            0.9013            0.9043        0.4130        6.7754\n",
      "    125        0.0020       0.9055            0.9024            0.9055        0.4176        6.7769\n",
      "    126        0.0012       0.9043            0.9013            0.9043        0.4093        6.7512\n",
      "    127        0.0013       0.9007            0.8974            0.9007        0.4194        6.7784\n",
      "    128        0.0010       0.9115            0.9089            0.9115        0.4032        6.7509\n",
      "    129        0.0010       0.9067            0.9040            0.9067        0.4080        6.7734\n",
      "    130        0.0012       0.9103            0.9077            0.9103        0.4103        6.7647\n",
      "    131        \u001b[36m0.0009\u001b[0m       0.9043            0.9015            0.9043        0.4132        6.7592\n",
      "    132        0.0010       0.9127            0.9101            0.9127        0.4068        6.7854\n",
      "    133        0.0011       0.9103            0.9076            0.9103        0.4058        6.7940\n",
      "    134        0.0024       0.9139            \u001b[35m0.9113\u001b[0m            0.9139        0.4021     +  6.7533\n",
      "    135        0.0022       0.9055            0.9031            0.9055        0.4100        6.8874\n",
      "    136        0.0013       0.9031            0.9005            0.9031        0.4142        6.8225\n",
      "    137        0.0010       0.9079            0.9050            0.9079        0.4161        6.7840\n",
      "    138        0.0017       0.9055            0.9029            0.9055        0.4177        6.7343\n",
      "    139        0.0010       0.9067            0.9039            0.9067        0.4251        6.7520\n",
      "    140        0.0022       0.9043            0.9015            0.9043        0.4239        6.7410\n",
      "    141        \u001b[36m0.0009\u001b[0m       0.9007            0.8983            0.9007        0.4267        6.7582\n",
      "    142        0.0012       0.9067            0.9043            0.9067        0.4176        6.7530\n",
      "    143        0.0017       0.9067            0.9042            0.9067        0.4286        6.7945\n",
      "    144        0.0014       0.9103            0.9077            0.9103        0.4237        6.7842\n",
      "    145        0.0011       0.9067            0.9038            0.9067        0.4186        6.7632\n",
      "    146        0.0014       0.9079            0.9052            0.9079        0.4036        6.7701\n",
      "    147        \u001b[36m0.0009\u001b[0m       0.9055            0.9028            0.9055        0.4108        6.8303\n",
      "    148        0.0010       0.9031            0.9006            0.9031        0.4076        6.7637\n",
      "    149        0.0031       0.9031            0.9002            0.9031        0.4199        6.7501\n",
      "    150        0.0010       0.8995            0.8967            0.8995        0.4225        6.7697\n",
      "    151        0.0016       0.9079            0.9057            0.9079        0.4140        6.7650\n",
      "    152        0.0018       0.9007            0.8981            0.9007        0.4269        6.7808\n",
      "    153        0.0011       0.9055            0.9029            0.9055        0.4127        6.7522\n",
      "    154        \u001b[36m0.0008\u001b[0m       0.9043            0.9018            0.9043        0.4147        6.7938\n",
      "    155        0.0009       0.9079            0.9053            0.9079        0.4104        6.7694\n",
      "    156        0.0012       0.9007            0.8974            0.9007        0.4172        6.7996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157        0.0010       0.9031            0.9004            0.9031        0.4198        6.7740\n",
      "    158        0.0012       0.9031            0.9004            0.9031        0.4220        6.7849\n",
      "    159        0.0011       0.9043            0.9018            0.9043        0.4232        6.7671\n",
      "    160        0.0010       0.9043            0.9012            0.9043        0.4163        6.7662\n",
      "    161        0.0010       0.9055            0.9030            0.9055        0.4131        6.7550\n",
      "    162        0.0011       0.9043            0.9014            0.9043        0.4172        6.7728\n",
      "    163        0.0009       0.9103            0.9076            0.9103        0.4073        6.7593\n",
      "    164        0.0011       0.9019            0.8991            0.9019        0.4233        6.7784\n",
      "    165        0.0013       0.9067            0.9042            0.9067        0.4135        6.7873\n",
      "    166        0.0017       0.9031            0.9002            0.9031        0.4219        6.7413\n",
      "    167        \u001b[36m0.0008\u001b[0m       0.9043            0.9019            0.9043        0.4083        6.8212\n",
      "    168        0.0009       0.9055            0.9031            0.9055        0.4130        6.7424\n",
      "    169        0.0012       0.9043            0.9014            0.9043        0.4164        6.7798\n",
      "    170        0.0012       0.9067            0.9040            0.9067        0.4088        6.7545\n",
      "    171        0.0011       0.9031            0.9008            0.9031        0.4216        6.7490\n",
      "    172        0.0021       0.9031            0.9005            0.9031        0.4250        6.7454\n",
      "    173        0.0009       0.9055            0.9030            0.9055        0.4155        6.7897\n",
      "    174        \u001b[36m0.0008\u001b[0m       0.9007            0.8981            0.9007        0.4122        6.7863\n",
      "    175        0.0011       0.9055            0.9028            0.9055        0.4139        6.7547\n",
      "    176        0.0013       0.9067            0.9043            0.9067        0.4188        6.7677\n",
      "    177        0.0010       0.9055            0.9027            0.9055        0.4199        6.7698\n",
      "    178        0.0009       0.9055            0.9031            0.9055        0.4117        6.7788\n",
      "    179        0.0010       0.9067            0.9043            0.9067        0.4132        6.7948\n",
      "    180        0.0014       0.9067            0.9042            0.9067        0.4140        6.8000\n",
      "    181        0.0016       0.9091            0.9065            0.9091        0.4202        6.7857\n",
      "    182        0.0011       0.9079            0.9056            0.9079        0.4214        6.7589\n",
      "    183        0.0010       0.9055            0.9029            0.9055        0.4173        6.7587\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.0087, 0.0114, 0.0085]), 'p99': tensor([0.0113, 0.0872, 0.0700])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.8396\u001b[0m       \u001b[32m0.6376\u001b[0m            \u001b[35m0.6311\u001b[0m            \u001b[31m0.6376\u001b[0m        \u001b[94m1.5434\u001b[0m     +  6.9446\n",
      "      2        \u001b[36m0.4677\u001b[0m       \u001b[32m0.7177\u001b[0m            \u001b[35m0.7016\u001b[0m            \u001b[31m0.7177\u001b[0m        \u001b[94m0.9039\u001b[0m     +  6.8280\n",
      "      3        0.4753       0.6794            0.6427            0.6794        1.0765        6.7887\n",
      "      4        \u001b[36m0.4330\u001b[0m       \u001b[32m0.8170\u001b[0m            \u001b[35m0.8116\u001b[0m            \u001b[31m0.8170\u001b[0m        \u001b[94m0.5655\u001b[0m     +  6.8016\n",
      "      5        \u001b[36m0.3296\u001b[0m       0.8014            0.7871            0.8014        \u001b[94m0.5644\u001b[0m        6.7576\n",
      "      6        \u001b[36m0.2935\u001b[0m       \u001b[32m0.8313\u001b[0m            \u001b[35m0.8241\u001b[0m            \u001b[31m0.8313\u001b[0m        0.6114     +  6.7640\n",
      "      7        0.2988       \u001b[32m0.8660\u001b[0m            \u001b[35m0.8601\u001b[0m            \u001b[31m0.8660\u001b[0m        \u001b[94m0.4507\u001b[0m     +  6.7841\n",
      "      8        \u001b[36m0.2575\u001b[0m       0.8337            0.8319            0.8337        0.5240        6.7881\n",
      "      9        0.3433       0.7333            0.6994            0.7333        1.1144        6.8047\n",
      "     10        0.3346       0.8565            0.8494            0.8565        0.4535        6.7692\n",
      "     11        0.2831       0.8182            0.8102            0.8182        0.5311        6.7882\n",
      "     12        \u001b[36m0.2442\u001b[0m       0.8158            0.8046            0.8158        0.6447        6.7743\n",
      "     13        0.2658       0.8230            0.8106            0.8230        0.5345        6.7700\n",
      "     14        0.2750       0.8624            0.8592            0.8624        0.4554        6.7678\n",
      "     15        \u001b[36m0.2120\u001b[0m       \u001b[32m0.8780\u001b[0m            \u001b[35m0.8727\u001b[0m            \u001b[31m0.8780\u001b[0m        \u001b[94m0.4357\u001b[0m     +  6.7806\n",
      "     16        \u001b[36m0.1978\u001b[0m       0.8289            0.8245            0.8289        0.5447        6.8810\n",
      "     17        0.2214       0.8278            0.8082            0.8278        0.5701        6.8331\n",
      "     18        0.2565       0.8433            0.8363            0.8433        0.4936        6.7761\n",
      "     19        \u001b[36m0.1732\u001b[0m       \u001b[32m0.8876\u001b[0m            \u001b[35m0.8858\u001b[0m            \u001b[31m0.8876\u001b[0m        \u001b[94m0.3830\u001b[0m     +  6.7521\n",
      "     20        0.1751       0.7990            0.7815            0.7990        0.7109        6.8231\n",
      "     21        0.2198       0.8672            0.8652            0.8672        0.4360        6.8180\n",
      "     22        \u001b[36m0.1543\u001b[0m       0.8038            0.8028            0.8038        0.6804        6.7515\n",
      "     23        0.1579       0.8684            0.8646            0.8684        0.4887        6.7451\n",
      "     24        \u001b[36m0.1174\u001b[0m       0.8624            0.8597            0.8624        0.4563        6.7693\n",
      "     25        \u001b[36m0.0988\u001b[0m       0.8505            0.8442            0.8505        0.6377        6.7754\n",
      "     26        \u001b[36m0.0929\u001b[0m       0.8816            0.8796            0.8816        0.4309        6.7598\n",
      "     27        0.1054       0.8600            0.8530            0.8600        0.5283        6.7652\n",
      "     28        0.0979       0.8361            0.8328            0.8361        0.5641        6.7936\n",
      "     29        0.1102       0.7895            0.7902            0.7895        0.6512        6.7602\n",
      "     30        0.2077       0.8529            0.8474            0.8529        0.5614        6.7804\n",
      "     31        0.1330       0.8577            0.8534            0.8577        0.4803        6.7344\n",
      "     32        0.2051       0.8301            0.8249            0.8301        0.6464        6.8081\n",
      "     33        0.1164       0.8708            0.8681            0.8708        0.4095        6.7538\n",
      "     34        0.1177       0.8768            0.8739            0.8768        0.4045        6.7664\n",
      "     35        0.1006       0.8720            0.8689            0.8720        0.4238        6.7716\n",
      "     36        \u001b[36m0.0653\u001b[0m       0.8744            0.8716            0.8744        0.4388        6.8035\n",
      "     37        0.0705       0.8648            0.8593            0.8648        0.5570        6.7489\n",
      "     38        0.0802       0.8660            0.8608            0.8660        0.5080        6.7370\n",
      "     39        0.0920       0.8457            0.8419            0.8457        0.6491        6.8035\n",
      "     40        0.0970       0.8373            0.8329            0.8373        0.5637        6.7520\n",
      "     41        0.0746       0.8553            0.8512            0.8553        0.5449        6.7684\n",
      "     42        \u001b[36m0.0567\u001b[0m       0.8648            0.8597            0.8648        0.5234        6.7624\n",
      "     43        0.0639       0.8756            0.8720            0.8756        0.5198        6.7763\n",
      "     44        \u001b[36m0.0263\u001b[0m       0.8816            0.8782            0.8816        0.5005        6.7813\n",
      "     45        0.0325       0.8780            0.8735            0.8780        0.5656        6.7394\n",
      "     46        0.0272       0.8840            0.8803            0.8840        0.5335        6.7739\n",
      "     47        \u001b[36m0.0205\u001b[0m       \u001b[32m0.8923\u001b[0m            \u001b[35m0.8892\u001b[0m            \u001b[31m0.8923\u001b[0m        0.5101     +  6.7673\n",
      "     48        0.0401       0.8900            0.8870            0.8900        0.5259        6.7921\n",
      "     49        0.0711       0.8852            0.8827            0.8852        0.5932        6.7619\n",
      "     50        0.0529       0.8732            0.8705            0.8732        0.6261        6.7592\n",
      "     51        0.0654       0.8756            0.8704            0.8756        0.5610        6.7651\n",
      "     52        0.0391       0.8648            0.8613            0.8648        0.5729        6.7440\n",
      "     53        \u001b[36m0.0201\u001b[0m       \u001b[32m0.8935\u001b[0m            \u001b[35m0.8912\u001b[0m            \u001b[31m0.8935\u001b[0m        0.4857     +  6.7486\n",
      "     54        0.0721       0.8840            0.8820            0.8840        0.5082        6.8799\n",
      "     55        0.0448       0.8720            0.8690            0.8720        0.5922        6.8552\n",
      "     56        0.0785       0.8840            0.8818            0.8840        0.4920        6.8219\n",
      "     57        0.0319       0.8852            0.8825            0.8852        0.4877        6.7919\n",
      "     58        0.0214       0.8888            0.8861            0.8888        0.5001        6.7780\n",
      "     59        \u001b[36m0.0126\u001b[0m       0.8911            0.8881            0.8911        0.5083        6.7471\n",
      "     60        0.0153       0.8684            0.8660            0.8684        0.6125        6.7413\n",
      "     61        0.0743       0.8720            0.8700            0.8720        0.5764        6.7662\n",
      "     62        0.0472       0.8816            0.8790            0.8816        0.5633        6.7733\n",
      "     63        0.0232       0.8768            0.8729            0.8768        0.5959        6.7797\n",
      "     64        0.0797       0.8553            0.8505            0.8553        0.6598        6.7812\n",
      "     65        0.0718       0.8696            0.8654            0.8696        0.5476        6.7659\n",
      "     66        0.0278       0.8864            0.8834            0.8864        0.4990        6.7578\n",
      "     67        0.0147       0.8864            0.8833            0.8864        0.4796        6.8130\n",
      "     68        0.0271       0.8828            0.8792            0.8828        0.5503        6.7582\n",
      "     69        0.0153       0.8804            0.8770            0.8804        0.5444        6.7610\n",
      "     70        \u001b[36m0.0090\u001b[0m       0.8900            0.8866            0.8900        0.5216        6.7963\n",
      "     71        0.0291       0.8852            0.8839            0.8852        0.5009        6.7650\n",
      "     72        0.0095       \u001b[32m0.9019\u001b[0m            \u001b[35m0.8990\u001b[0m            \u001b[31m0.9019\u001b[0m        0.4744     +  6.8004\n",
      "     73        \u001b[36m0.0087\u001b[0m       0.8923            0.8887            0.8923        0.4888        6.8233\n",
      "     74        \u001b[36m0.0052\u001b[0m       0.8888            0.8858            0.8888        0.5233        6.7872\n",
      "     75        \u001b[36m0.0047\u001b[0m       0.8947            0.8916            0.8947        0.4814        6.7729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     76        \u001b[36m0.0031\u001b[0m       0.8900            0.8873            0.8900        0.5139        6.7651\n",
      "     77        0.0045       0.8947            0.8917            0.8947        0.5033        6.7850\n",
      "     78        0.0032       0.8923            0.8892            0.8923        0.5246        6.7753\n",
      "     79        \u001b[36m0.0028\u001b[0m       0.8911            0.8880            0.8911        0.5400        6.7622\n",
      "     80        0.0038       0.8935            0.8897            0.8935        0.5324        6.7539\n",
      "     81        0.0173       0.8816            0.8768            0.8816        0.5719        6.8073\n",
      "     82        0.0117       0.8995            0.8969            0.8995        0.5010        6.8118\n",
      "     83        0.0217       0.8888            0.8859            0.8888        0.5694        6.7621\n",
      "     84        0.0601       0.8708            0.8656            0.8708        0.6312        6.7789\n",
      "     85        0.0441       0.8780            0.8725            0.8780        0.5956        6.7577\n",
      "     86        0.0225       0.8840            0.8814            0.8840        0.5368        6.8212\n",
      "     87        0.0090       0.8888            0.8854            0.8888        0.5095        6.7775\n",
      "     88        0.0056       0.8900            0.8868            0.8900        0.5202        6.7878\n",
      "     89        0.0066       0.8900            0.8869            0.8900        0.5005        6.7839\n",
      "     90        0.0042       0.8888            0.8857            0.8888        0.5027        6.7706\n",
      "     91        0.0033       0.8900            0.8868            0.8900        0.5042        6.7439\n",
      "     92        0.0040       0.8876            0.8847            0.8876        0.5018        6.7575\n",
      "     93        0.0038       0.8900            0.8870            0.8900        0.5187        6.7924\n",
      "     94        0.0031       0.8923            0.8892            0.8923        0.5091        6.7740\n",
      "     95        0.0034       0.8911            0.8881            0.8911        0.5051        6.7911\n",
      "     96        0.0036       0.8935            0.8909            0.8935        0.5063        6.7431\n",
      "     97        0.0048       0.8923            0.8893            0.8923        0.5205        6.7219\n",
      "     98        0.0057       0.8935            0.8899            0.8935        0.5283        6.7825\n",
      "     99        0.0038       0.8983            0.8951            0.8983        0.5149        6.7988\n",
      "    100        0.0030       0.8935            0.8904            0.8935        0.5382        6.7398\n",
      "    101        \u001b[36m0.0023\u001b[0m       0.8983            0.8953            0.8983        0.5241        6.7690\n",
      "    102        \u001b[36m0.0021\u001b[0m       0.8923            0.8890            0.8923        0.5187        6.7540\n",
      "    103        0.0046       0.8959            0.8928            0.8959        0.5359        6.7571\n",
      "    104        0.0030       0.8923            0.8889            0.8923        0.5223        6.7469\n",
      "    105        0.0050       0.8947            0.8915            0.8947        0.5091        6.7861\n",
      "    106        0.0045       0.8971            0.8938            0.8971        0.5114        6.7322\n",
      "    107        0.0027       0.8923            0.8887            0.8923        0.5139        6.7743\n",
      "    108        0.0027       0.8971            0.8936            0.8971        0.5185        6.7740\n",
      "    109        0.0028       0.8995            0.8960            0.8995        0.5194        6.7766\n",
      "    110        0.0029       0.8923            0.8888            0.8923        0.5166        6.7612\n",
      "    111        0.0027       0.8959            0.8927            0.8959        0.5156        6.7747\n",
      "    112        \u001b[36m0.0020\u001b[0m       0.8911            0.8880            0.8911        0.5117        6.7941\n",
      "    113        0.0026       0.8947            0.8917            0.8947        0.5035        6.7809\n",
      "    114        \u001b[36m0.0017\u001b[0m       0.8935            0.8908            0.8935        0.5096        6.7774\n",
      "    115        0.0025       0.8935            0.8906            0.8935        0.5078        6.7782\n",
      "    116        0.0031       0.8959            0.8931            0.8959        0.5202        6.7665\n",
      "    117        0.0023       0.8959            0.8931            0.8959        0.5265        6.7747\n",
      "    118        0.0022       0.8935            0.8906            0.8935        0.5265        6.7834\n",
      "    119        0.0034       0.8959            0.8927            0.8959        0.5320        6.8159\n",
      "    120        0.0022       0.8947            0.8918            0.8947        0.5283        6.7877\n",
      "    121        0.0022       0.8971            0.8943            0.8971        0.5351        6.8079\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.0087, 0.0115, 0.0085]), 'p99': tensor([0.0113, 0.0863, 0.0699])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7762\u001b[0m       \u001b[32m0.5754\u001b[0m            \u001b[35m0.5015\u001b[0m            \u001b[31m0.5754\u001b[0m        \u001b[94m2.0429\u001b[0m     +  6.9193\n",
      "      2        \u001b[36m0.5020\u001b[0m       \u001b[32m0.7739\u001b[0m            \u001b[35m0.7395\u001b[0m            \u001b[31m0.7739\u001b[0m        \u001b[94m0.7402\u001b[0m     +  6.8336\n",
      "      3        \u001b[36m0.4428\u001b[0m       \u001b[32m0.8122\u001b[0m            \u001b[35m0.8086\u001b[0m            \u001b[31m0.8122\u001b[0m        \u001b[94m0.5485\u001b[0m     +  6.8022\n",
      "      4        \u001b[36m0.3366\u001b[0m       0.8074            0.7906            0.8074        0.5854        6.7729\n",
      "      5        0.3638       0.8002            0.7950            0.8002        0.5962        6.7672\n",
      "      6        \u001b[36m0.3151\u001b[0m       \u001b[32m0.8409\u001b[0m            \u001b[35m0.8331\u001b[0m            \u001b[31m0.8409\u001b[0m        \u001b[94m0.4803\u001b[0m     +  6.7896\n",
      "      7        0.3253       0.8361            0.8293            0.8361        \u001b[94m0.4446\u001b[0m        6.7995\n",
      "      8        \u001b[36m0.2570\u001b[0m       \u001b[32m0.8600\u001b[0m            \u001b[35m0.8551\u001b[0m            \u001b[31m0.8600\u001b[0m        \u001b[94m0.3886\u001b[0m     +  6.7846\n",
      "      9        \u001b[36m0.2405\u001b[0m       0.7859            0.7731            0.7859        0.6894        6.7721\n",
      "     10        \u001b[36m0.2316\u001b[0m       0.6806            0.6311            0.6806        1.2271        6.7750\n",
      "     11        0.2778       0.8313            0.8202            0.8313        0.5707        6.7683\n",
      "     12        \u001b[36m0.2250\u001b[0m       0.8026            0.7912            0.8026        0.5674        6.7928\n",
      "     13        \u001b[36m0.1906\u001b[0m       0.7548            0.7371            0.7548        1.0463        6.7791\n",
      "     14        0.2910       0.7392            0.6928            0.7392        1.0709        6.7552\n",
      "     15        0.2208       0.7476            0.7376            0.7476        1.1275        6.8025\n",
      "     16        0.2006       0.8134            0.8044            0.8134        0.5911        6.7462\n",
      "     17        0.2691       0.8361            0.8358            0.8361        0.5041        6.7889\n",
      "     18        0.2118       0.8541            0.8512            0.8541        0.4449        6.7554\n",
      "     19        \u001b[36m0.1822\u001b[0m       0.8481            0.8411            0.8481        0.4917        6.7986\n",
      "     20        \u001b[36m0.1600\u001b[0m       0.8313            0.8141            0.8313        0.6048        6.7604\n",
      "     21        0.2152       0.8600            0.8536            0.8600        0.4856        6.7543\n",
      "     22        0.2038       0.8469            0.8410            0.8469        0.4584        6.7953\n",
      "     23        \u001b[36m0.1337\u001b[0m       0.8577            \u001b[35m0.8553\u001b[0m            0.8577        0.4555     +  6.7546\n",
      "     24        \u001b[36m0.1127\u001b[0m       0.8421            0.8392            0.8421        0.6377        6.8341\n",
      "     25        0.1821       0.8433            0.8428            0.8433        0.4725        6.8509\n",
      "     26        0.2005       0.8409            0.8373            0.8409        0.5054        6.8337\n",
      "     27        0.1399       0.8457            0.8363            0.8457        0.5462        6.7732\n",
      "     28        0.1252       \u001b[32m0.8648\u001b[0m            \u001b[35m0.8596\u001b[0m            \u001b[31m0.8648\u001b[0m        0.4732     +  6.7447\n",
      "     29        \u001b[36m0.1082\u001b[0m       0.8289            0.8235            0.8289        0.5651        6.8264\n",
      "     30        \u001b[36m0.0792\u001b[0m       0.8457            0.8392            0.8457        0.6676        6.8327\n",
      "     31        0.1998       0.8242            0.8183            0.8242        0.6532        6.8341\n",
      "     32        0.2249       0.8421            0.8354            0.8421        0.5333        6.7900\n",
      "     33        0.1330       \u001b[32m0.8720\u001b[0m            \u001b[35m0.8694\u001b[0m            \u001b[31m0.8720\u001b[0m        0.4144     +  6.7829\n",
      "     34        0.1218       0.8589            0.8565            0.8589        0.4567        6.8658\n",
      "     35        0.1335       0.8612            0.8559            0.8612        0.4647        6.7437\n",
      "     36        0.1970       0.8409            0.8362            0.8409        0.5868        6.7600\n",
      "     37        0.1202       \u001b[32m0.8816\u001b[0m            \u001b[35m0.8797\u001b[0m            \u001b[31m0.8816\u001b[0m        \u001b[94m0.3789\u001b[0m     +  6.7687\n",
      "     38        0.0837       0.8672            0.8639            0.8672        0.4045        6.8181\n",
      "     39        0.1043       0.8445            0.8382            0.8445        0.5089        6.7956\n",
      "     40        \u001b[36m0.0778\u001b[0m       0.8744            0.8706            0.8744        0.4277        6.8122\n",
      "     41        \u001b[36m0.0525\u001b[0m       0.8720            0.8687            0.8720        0.4346        6.7839\n",
      "     42        \u001b[36m0.0310\u001b[0m       0.8744            0.8704            0.8744        0.4690        6.7819\n",
      "     43        0.0376       0.8708            0.8662            0.8708        0.4889        6.7850\n",
      "     44        0.0410       0.8636            0.8583            0.8636        0.5375        6.7868\n",
      "     45        \u001b[36m0.0233\u001b[0m       0.8804            0.8757            0.8804        0.5025        6.7703\n",
      "     46        \u001b[36m0.0216\u001b[0m       0.8720            0.8655            0.8720        0.6234        6.7667\n",
      "     47        0.0353       \u001b[32m0.8876\u001b[0m            \u001b[35m0.8844\u001b[0m            \u001b[31m0.8876\u001b[0m        0.5088     +  6.7744\n",
      "     48        0.0522       0.8648            0.8620            0.8648        0.5444        6.8605\n",
      "     49        0.0292       0.8756            0.8727            0.8756        0.5173        6.8611\n",
      "     50        \u001b[36m0.0142\u001b[0m       0.8708            0.8660            0.8708        0.5597        6.8092\n",
      "     51        0.0204       0.8732            0.8706            0.8732        0.5473        6.7886\n",
      "     52        \u001b[36m0.0123\u001b[0m       0.8828            0.8800            0.8828        0.5960        6.7815\n",
      "     53        0.0307       0.8696            0.8672            0.8696        0.5813        6.7796\n",
      "     54        0.0207       0.8684            0.8630            0.8684        0.5879        6.7704\n",
      "     55        0.0260       0.8672            0.8649            0.8672        0.5644        6.7775\n",
      "     56        0.0523       0.8529            0.8507            0.8529        0.6203        6.7936\n",
      "     57        0.0329       0.8577            0.8529            0.8577        0.6324        6.7807\n",
      "     58        0.1061       0.8565            0.8526            0.8565        0.6153        6.7734\n",
      "     59        0.0437       0.8744            0.8712            0.8744        0.5200        6.7862\n",
      "     60        0.0369       0.8696            0.8659            0.8696        0.5285        6.7923\n",
      "     61        0.0330       0.8732            0.8683            0.8732        0.5612        6.7969\n",
      "     62        0.0446       0.8720            0.8695            0.8720        0.5064        6.7815\n",
      "     63        0.0209       0.8708            0.8672            0.8708        0.5499        6.8099\n",
      "     64        0.0232       0.8828            0.8788            0.8828        0.5718        6.7595\n",
      "     65        \u001b[36m0.0094\u001b[0m       0.8792            0.8758            0.8792        0.5513        6.7776\n",
      "     66        \u001b[36m0.0065\u001b[0m       0.8816            0.8782            0.8816        0.5458        6.7723\n",
      "     67        \u001b[36m0.0056\u001b[0m       0.8756            0.8727            0.8756        0.5567        6.7688\n",
      "     68        0.0081       0.8780            0.8747            0.8780        0.5444        6.8072\n",
      "     69        0.0092       0.8768            0.8738            0.8768        0.5669        6.7747\n",
      "     70        0.0087       0.8696            0.8672            0.8696        0.5631        6.7457\n",
      "     71        \u001b[36m0.0055\u001b[0m       0.8684            0.8658            0.8684        0.5663        6.7677\n",
      "     72        \u001b[36m0.0046\u001b[0m       0.8744            0.8714            0.8744        0.5669        6.7601\n",
      "     73        \u001b[36m0.0041\u001b[0m       0.8720            0.8687            0.8720        0.6167        6.7548\n",
      "     74        0.0057       0.8660            0.8622            0.8660        0.6174        6.7937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     75        0.0049       0.8756            0.8725            0.8756        0.6165        6.7599\n",
      "     76        0.0083       0.8684            0.8656            0.8684        0.6114        6.7484\n",
      "     77        0.0078       0.8720            0.8685            0.8720        0.5889        6.7453\n",
      "     78        0.0099       0.8732            0.8692            0.8732        0.6056        6.7834\n",
      "     79        0.0042       0.8684            0.8651            0.8684        0.6028        6.7635\n",
      "     80        0.0043       0.8756            0.8725            0.8756        0.5910        6.7457\n",
      "     81        \u001b[36m0.0039\u001b[0m       0.8768            0.8734            0.8768        0.5857        6.7402\n",
      "     82        \u001b[36m0.0035\u001b[0m       0.8780            0.8745            0.8780        0.5912        6.7488\n",
      "     83        0.0074       0.8792            0.8749            0.8792        0.5728        6.7742\n",
      "     84        0.0044       0.8756            0.8717            0.8756        0.5663        6.7839\n",
      "     85        0.0047       0.8732            0.8701            0.8732        0.5786        6.8049\n",
      "     86        0.0041       0.8768            0.8730            0.8768        0.5916        6.7861\n",
      "     87        0.0039       0.8720            0.8687            0.8720        0.6426        6.8002\n",
      "     88        0.0052       0.8708            0.8676            0.8708        0.6208        6.7653\n",
      "     89        \u001b[36m0.0031\u001b[0m       0.8720            0.8688            0.8720        0.6230        6.7510\n",
      "     90        \u001b[36m0.0029\u001b[0m       0.8744            0.8714            0.8744        0.6189        6.7950\n",
      "     91        0.0050       0.8756            0.8724            0.8756        0.6168        6.8127\n",
      "     92        0.0047       0.8696            0.8662            0.8696        0.6195        6.7548\n",
      "     93        0.0043       0.8708            0.8672            0.8708        0.6318        6.7720\n",
      "     94        0.0035       0.8780            0.8751            0.8780        0.6075        6.7682\n",
      "     95        0.0033       0.8744            0.8712            0.8744        0.6169        6.8041\n",
      "     96        0.0038       0.8696            0.8659            0.8696        0.6323        6.8022\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n",
      "DeepLift [0, 1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.1811, 0.0000, 0.0080]), 'p99': tensor([0.2077, 0.0056, 0.0105])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m1.1275\u001b[0m       \u001b[32m0.4545\u001b[0m            \u001b[35m0.4336\u001b[0m            \u001b[31m0.4545\u001b[0m        \u001b[94m1.8522\u001b[0m     +  6.9186\n",
      "      2        \u001b[36m0.8495\u001b[0m       \u001b[32m0.5813\u001b[0m            \u001b[35m0.5803\u001b[0m            \u001b[31m0.5813\u001b[0m        \u001b[94m1.0136\u001b[0m     +  6.8359\n",
      "      3        \u001b[36m0.7596\u001b[0m       0.4761            0.4374            0.4761        1.4852        6.7882\n",
      "      4        \u001b[36m0.6861\u001b[0m       \u001b[32m0.6220\u001b[0m            \u001b[35m0.5842\u001b[0m            \u001b[31m0.6220\u001b[0m        1.1086     +  6.7546\n",
      "      5        \u001b[36m0.6611\u001b[0m       0.5526            0.5231            0.5526        1.2548        6.8106\n",
      "      6        \u001b[36m0.6461\u001b[0m       0.6208            \u001b[35m0.6035\u001b[0m            0.6208        1.0627     +  6.7935\n",
      "      7        0.6601       0.6089            0.5465            0.6089        1.1234        6.8349\n",
      "      8        \u001b[36m0.5954\u001b[0m       \u001b[32m0.7010\u001b[0m            \u001b[35m0.6853\u001b[0m            \u001b[31m0.7010\u001b[0m        \u001b[94m0.7312\u001b[0m     +  6.7199\n",
      "      9        \u001b[36m0.5234\u001b[0m       0.6758            0.6335            0.6758        0.9072        6.7757\n",
      "     10        0.6009       0.6950            \u001b[35m0.6891\u001b[0m            0.6950        0.7762     +  6.7570\n",
      "     11        0.5420       0.6663            0.6414            0.6663        0.9108        6.8217\n",
      "     12        \u001b[36m0.4807\u001b[0m       0.6782            0.6348            0.6782        0.8369        6.8775\n",
      "     13        \u001b[36m0.4481\u001b[0m       0.6543            0.6472            0.6543        0.9598        6.8104\n",
      "     14        0.4717       0.4940            0.4682            0.4940        1.8645        6.7531\n",
      "     15        0.5469       \u001b[32m0.7069\u001b[0m            0.6617            \u001b[31m0.7069\u001b[0m        0.8592        6.7883\n",
      "     16        \u001b[36m0.4416\u001b[0m       0.6651            0.6233            0.6651        0.9047        6.7842\n",
      "     17        0.4778       0.6782            0.6792            0.6782        0.8176        6.7748\n",
      "     18        0.4513       0.6495            0.6449            0.6495        0.9880        6.7488\n",
      "     19        \u001b[36m0.3774\u001b[0m       0.6902            0.6871            0.6902        0.8929        6.7377\n",
      "     20        0.3820       0.7033            \u001b[35m0.7008\u001b[0m            0.7033        0.7954     +  6.7747\n",
      "     21        \u001b[36m0.3207\u001b[0m       0.6758            0.6666            0.6758        0.9109        6.8274\n",
      "     22        \u001b[36m0.3048\u001b[0m       \u001b[32m0.7141\u001b[0m            \u001b[35m0.7081\u001b[0m            \u001b[31m0.7141\u001b[0m        0.8288     +  6.8148\n",
      "     23        \u001b[36m0.2895\u001b[0m       0.6699            0.6639            0.6699        1.0065        6.7791\n",
      "     24        \u001b[36m0.2532\u001b[0m       0.7129            0.6869            0.7129        0.9560        6.7819\n",
      "     25        0.3022       0.6914            0.6797            0.6914        1.0377        6.7522\n",
      "     26        0.2876       0.6687            0.6400            0.6687        1.1222        6.7273\n",
      "     27        \u001b[36m0.2342\u001b[0m       0.6734            0.6468            0.6734        1.3204        6.7796\n",
      "     28        0.3463       0.6531            0.6275            0.6531        1.3005        6.7352\n",
      "     29        0.2770       0.6998            0.6926            0.6998        0.9552        6.7628\n",
      "     30        0.3066       0.7022            0.6711            0.7022        1.0723        6.7209\n",
      "     31        0.3185       0.6722            0.6703            0.6722        1.0032        6.7423\n",
      "     32        0.3505       0.6842            0.6674            0.6842        0.9986        6.7212\n",
      "     33        0.2575       0.6950            0.6929            0.6950        0.9080        6.7674\n",
      "     34        \u001b[36m0.1454\u001b[0m       \u001b[32m0.7237\u001b[0m            \u001b[35m0.7189\u001b[0m            \u001b[31m0.7237\u001b[0m        0.9297     +  6.7736\n",
      "     35        \u001b[36m0.1228\u001b[0m       \u001b[32m0.7344\u001b[0m            0.7157            \u001b[31m0.7344\u001b[0m        1.0314        6.8141\n",
      "     36        \u001b[36m0.1177\u001b[0m       0.7201            0.7146            0.7201        1.0257        6.8449\n",
      "     37        \u001b[36m0.0930\u001b[0m       0.7261            0.7171            0.7261        1.0103        6.8018\n",
      "     38        0.0964       0.7201            0.7183            0.7201        1.1008        6.7494\n",
      "     39        \u001b[36m0.0893\u001b[0m       0.7153            0.6975            0.7153        1.2520        6.7684\n",
      "     40        \u001b[36m0.0638\u001b[0m       0.7117            0.7016            0.7117        1.1776        6.7441\n",
      "     41        \u001b[36m0.0604\u001b[0m       0.7297            \u001b[35m0.7270\u001b[0m            0.7297        1.1680     +  6.7530\n",
      "     42        0.1308       0.7129            0.7074            0.7129        1.1201        6.8643\n",
      "     43        0.1336       0.7333            0.7237            0.7333        1.1097        6.8017\n",
      "     44        0.0858       0.6998            0.6938            0.6998        1.1387        6.7694\n",
      "     45        0.1287       0.6926            0.6673            0.6926        1.3995        6.7694\n",
      "     46        0.0940       0.7105            0.7082            0.7105        1.1383        6.7313\n",
      "     47        0.0622       0.7177            0.7132            0.7177        1.2747        6.7502\n",
      "     48        0.0694       0.7141            0.7092            0.7141        1.2380        6.7492\n",
      "     49        \u001b[36m0.0319\u001b[0m       0.7033            0.6967            0.7033        1.2305        6.7534\n",
      "     50        \u001b[36m0.0219\u001b[0m       0.7177            0.7150            0.7177        1.2280        6.7731\n",
      "     51        \u001b[36m0.0215\u001b[0m       0.7189            0.7135            0.7189        1.2627        6.7367\n",
      "     52        0.0298       0.7069            0.6869            0.7069        1.5555        6.7417\n",
      "     53        \u001b[36m0.0177\u001b[0m       0.7213            0.7198            0.7213        1.3107        6.7555\n",
      "     54        \u001b[36m0.0122\u001b[0m       0.7177            0.7127            0.7177        1.3466        6.7500\n",
      "     55        0.0249       0.7225            0.7200            0.7225        1.3554        6.7045\n",
      "     56        0.0134       0.7045            0.7017            0.7045        1.3775        6.7477\n",
      "     57        \u001b[36m0.0113\u001b[0m       0.7213            0.7179            0.7213        1.3833        6.7571\n",
      "     58        \u001b[36m0.0104\u001b[0m       0.7285            0.7270            0.7285        1.3816        6.7630\n",
      "     59        \u001b[36m0.0090\u001b[0m       0.7237            0.7196            0.7237        1.3624        6.7561\n",
      "     60        \u001b[36m0.0082\u001b[0m       0.7165            0.7121            0.7165        1.4402        6.7387\n",
      "     61        0.0128       0.7177            0.7141            0.7177        1.4776        6.7745\n",
      "     62        0.0616       0.7141            0.7042            0.7141        1.7249        6.7566\n",
      "     63        0.0398       0.7022            0.7007            0.7022        1.4829        6.7675\n",
      "     64        0.0270       0.7153            0.7122            0.7153        1.4589        6.7686\n",
      "     65        0.0175       \u001b[32m0.7380\u001b[0m            \u001b[35m0.7284\u001b[0m            \u001b[31m0.7380\u001b[0m        1.5858     +  6.7695\n",
      "     66        0.0093       0.7309            0.7262            0.7309        1.4677        6.8308\n",
      "     67        \u001b[36m0.0059\u001b[0m       0.7261            0.7222            0.7261        1.4462        6.8218\n",
      "     68        0.0084       0.7225            0.7190            0.7225        1.4587        6.7862\n",
      "     69        0.0184       0.7285            0.7232            0.7285        1.4735        6.7885\n",
      "     70        0.0085       0.7297            0.7230            0.7297        1.4637        6.7690\n",
      "     71        0.0073       0.7261            0.7214            0.7261        1.4529        6.7773\n",
      "     72        0.0106       0.7321            0.7235            0.7321        1.4697        6.7474\n",
      "     73        \u001b[36m0.0050\u001b[0m       0.7261            0.7189            0.7261        1.4729        6.7460\n",
      "     74        \u001b[36m0.0044\u001b[0m       0.7225            0.7187            0.7225        1.4597        6.7621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     75        \u001b[36m0.0042\u001b[0m       0.7285            0.7248            0.7285        1.4620        6.7974\n",
      "     76        0.0044       0.7213            0.7169            0.7213        1.4689        6.7185\n",
      "     77        0.0083       0.7249            0.7207            0.7249        1.5064        6.7338\n",
      "     78        0.0152       0.7297            0.7215            0.7297        1.5252        6.7663\n",
      "     79        0.0064       0.7213            0.7155            0.7213        1.5130        6.7397\n",
      "     80        0.0070       0.7237            0.7189            0.7237        1.5014        6.7652\n",
      "     81        0.0105       0.7153            0.7097            0.7153        1.5148        6.7739\n",
      "     82        0.0070       0.7261            0.7220            0.7261        1.5173        6.7502\n",
      "     83        0.0051       0.7201            0.7167            0.7201        1.5098        6.7569\n",
      "     84        \u001b[36m0.0039\u001b[0m       0.7249            0.7195            0.7249        1.5185        6.7521\n",
      "     85        \u001b[36m0.0030\u001b[0m       0.7189            0.7144            0.7189        1.5104        6.7736\n",
      "     86        0.0032       0.7201            0.7164            0.7201        1.5259        6.7527\n",
      "     87        \u001b[36m0.0030\u001b[0m       0.7165            0.7126            0.7165        1.5100        6.7414\n",
      "     88        0.0034       0.7153            0.7112            0.7153        1.5060        6.7414\n",
      "     89        \u001b[36m0.0027\u001b[0m       0.7189            0.7140            0.7189        1.5319        6.7816\n",
      "     90        0.0028       0.7153            0.7110            0.7153        1.5169        6.7728\n",
      "     91        \u001b[36m0.0021\u001b[0m       0.7105            0.7063            0.7105        1.5255        6.7480\n",
      "     92        \u001b[36m0.0015\u001b[0m       0.7129            0.7091            0.7129        1.5073        6.7800\n",
      "     93        0.0022       0.7189            0.7149            0.7189        1.5250        6.7632\n",
      "     94        0.0020       0.7201            0.7154            0.7201        1.5243        6.7740\n",
      "     95        0.0019       0.7153            0.7099            0.7153        1.5222        6.7737\n",
      "     96        0.0022       0.7105            0.7064            0.7105        1.5378        6.7581\n",
      "     97        0.0042       0.7177            0.7146            0.7177        1.5483        6.7215\n",
      "     98        0.0018       0.7201            0.7150            0.7201        1.5468        6.7302\n",
      "     99        0.0021       0.7225            0.7182            0.7225        1.5455        6.7579\n",
      "    100        0.0017       0.7189            0.7163            0.7189        1.5469        6.7395\n",
      "    101        0.0027       0.7225            0.7192            0.7225        1.5570        6.7520\n",
      "    102        0.0027       0.7201            0.7172            0.7201        1.5865        6.7598\n",
      "    103        0.0032       0.7141            0.7102            0.7141        1.5524        6.7939\n",
      "    104        0.0030       0.7153            0.7103            0.7153        1.6368        6.7640\n",
      "    105        0.0041       0.7285            0.7245            0.7285        1.5959        6.7661\n",
      "    106        0.0025       0.7249            0.7203            0.7249        1.5862        6.7491\n",
      "    107        0.0023       0.7368            \u001b[35m0.7335\u001b[0m            0.7368        1.6150     +  6.7653\n",
      "    108        0.0024       0.7285            0.7241            0.7285        1.5917        6.8236\n",
      "    109        0.0018       0.7285            0.7247            0.7285        1.5881        6.8010\n",
      "    110        0.0019       0.7249            0.7207            0.7249        1.5675        6.7823\n",
      "    111        0.0061       0.7273            0.7230            0.7273        1.6241        6.7452\n",
      "    112        0.0026       0.7273            0.7219            0.7273        1.5591        6.7681\n",
      "    113        0.0021       0.7309            0.7271            0.7309        1.5391        6.7430\n",
      "    114        0.0018       0.7285            0.7233            0.7285        1.5646        6.7708\n",
      "    115        0.0017       0.7356            0.7314            0.7356        1.5663        6.7433\n",
      "    116        0.0028       0.7249            0.7204            0.7249        1.5294        6.7387\n",
      "    117        0.0027       0.7285            0.7230            0.7285        1.5492        6.7507\n",
      "    118        0.0035       0.7309            0.7262            0.7309        1.5753        6.7562\n",
      "    119        0.0019       0.7201            0.7162            0.7201        1.5255        6.7524\n",
      "    120        0.0018       0.7261            0.7225            0.7261        1.5597        6.7357\n",
      "    121        0.0017       0.7261            0.7229            0.7261        1.5597        6.7526\n",
      "    122        0.0029       0.7333            0.7296            0.7333        1.5497        6.7770\n",
      "    123        \u001b[36m0.0014\u001b[0m       0.7368            \u001b[35m0.7338\u001b[0m            0.7368        1.5406     +  6.7328\n",
      "    124        0.0014       0.7297            0.7260            0.7297        1.5543        6.8498\n",
      "    125        0.0015       0.7225            0.7186            0.7225        1.5431        6.8165\n",
      "    126        0.0019       0.7261            0.7221            0.7261        1.5544        6.8049\n",
      "    127        0.0014       0.7261            0.7224            0.7261        1.5470        6.7534\n",
      "    128        0.0028       0.7237            0.7201            0.7237        1.5656        6.7117\n",
      "    129        0.0020       0.7309            0.7260            0.7309        1.5384        6.7472\n",
      "    130        0.0026       0.7285            0.7240            0.7285        1.5505        6.7344\n",
      "    131        0.0017       0.7344            0.7295            0.7344        1.5412        6.7397\n",
      "    132        0.0022       0.7309            0.7271            0.7309        1.5783        6.7695\n",
      "    133        0.0020       0.7285            0.7251            0.7285        1.5530        6.7828\n",
      "    134        0.0017       0.7297            0.7261            0.7297        1.5419        6.7647\n",
      "    135        0.0015       0.7321            0.7286            0.7321        1.5530        6.7836\n",
      "    136        0.0017       0.7273            0.7233            0.7273        1.5539        6.7495\n",
      "    137        0.0019       0.7285            0.7246            0.7285        1.5489        6.7682\n",
      "    138        0.0015       0.7249            0.7211            0.7249        1.5565        6.7341\n",
      "    139        0.0025       0.7297            0.7260            0.7297        1.5399        6.7693\n",
      "    140        0.0058       0.7309            0.7265            0.7309        1.5685        6.7424\n",
      "    141        0.0021       0.7321            0.7278            0.7321        1.5485        6.7823\n",
      "    142        0.0025       0.7309            0.7276            0.7309        1.5280        6.7321\n",
      "    143        0.0017       0.7237            0.7199            0.7237        1.5528        6.7471\n",
      "    144        0.0028       0.7177            0.7137            0.7177        1.5593        6.7704\n",
      "    145        0.0015       0.7249            0.7211            0.7249        1.5468        6.7808\n",
      "    146        0.0014       0.7297            0.7253            0.7297        1.5701        6.7477\n",
      "    147        0.0048       0.7213            0.7183            0.7213        1.5760        6.7665\n",
      "    148        0.0026       0.7237            0.7195            0.7237        1.5429        6.7714\n",
      "    149        0.0016       0.7249            0.7215            0.7249        1.5353        6.7768\n",
      "    150        0.0016       0.7225            0.7174            0.7225        1.5319        6.7640\n",
      "    151        0.0017       0.7237            0.7202            0.7237        1.5339        6.7293\n",
      "    152        0.0018       0.7344            0.7298            0.7344        1.5656        6.7609\n",
      "    153        0.0015       0.7273            0.7225            0.7273        1.5547        6.7754\n",
      "    154        0.0017       0.7333            0.7291            0.7333        1.5420        6.7667\n",
      "    155        0.0017       0.7237            0.7198            0.7237        1.5388        6.7669\n",
      "    156        0.0019       0.7237            0.7201            0.7237        1.5334        6.7648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157        \u001b[36m0.0012\u001b[0m       0.7333            0.7298            0.7333        1.5572        6.7745\n",
      "    158        0.0018       0.7285            0.7259            0.7285        1.5498        6.7536\n",
      "    159        0.0018       0.7189            0.7142            0.7189        1.5530        6.7509\n",
      "    160        0.0021       0.7261            0.7220            0.7261        1.5453        6.7704\n",
      "    161        0.0025       0.7285            0.7237            0.7285        1.5626        6.7468\n",
      "    162        0.0033       0.7321            0.7282            0.7321        1.5746        6.7330\n",
      "    163        0.0015       0.7321            0.7281            0.7321        1.5443        6.7709\n",
      "    164        0.0018       0.7309            0.7273            0.7309        1.5485        6.7627\n",
      "    165        0.0019       0.7249            0.7208            0.7249        1.5446        6.7627\n",
      "    166        0.0031       0.7261            0.7222            0.7261        1.5772        6.7149\n",
      "    167        0.0034       0.7297            0.7260            0.7297        1.5715        6.7857\n",
      "    168        0.0019       0.7285            0.7250            0.7285        1.5456        6.7302\n",
      "    169        0.0019       0.7273            0.7238            0.7273        1.5512        6.7668\n",
      "    170        0.0023       0.7225            0.7183            0.7225        1.5551        6.7442\n",
      "    171        0.0016       0.7297            0.7249            0.7297        1.5446        6.7602\n",
      "    172        0.0022       0.7333            0.7295            0.7333        1.5683        6.7854\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:22<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([1.8099e-01, 9.0445e-06, 8.0586e-03]), 'p99': tensor([0.2076, 0.0056, 0.0104])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m1.1241\u001b[0m       \u001b[32m0.4749\u001b[0m            \u001b[35m0.3927\u001b[0m            \u001b[31m0.4749\u001b[0m        \u001b[94m1.6021\u001b[0m     +  6.9556\n",
      "      2        \u001b[36m0.7676\u001b[0m       0.4689            \u001b[35m0.4261\u001b[0m            0.4689        1.6694     +  6.8464\n",
      "      3        \u001b[36m0.7163\u001b[0m       \u001b[32m0.6555\u001b[0m            \u001b[35m0.6353\u001b[0m            \u001b[31m0.6555\u001b[0m        \u001b[94m0.9251\u001b[0m     +  6.8016\n",
      "      4        \u001b[36m0.6515\u001b[0m       0.5742            0.5351            0.5742        1.3371        6.7972\n",
      "      5        \u001b[36m0.6110\u001b[0m       \u001b[32m0.6615\u001b[0m            0.5913            \u001b[31m0.6615\u001b[0m        0.9644        6.7650\n",
      "      6        0.6460       0.6483            0.6221            0.6483        0.9466        6.7686\n",
      "      7        \u001b[36m0.5722\u001b[0m       0.6352            0.6062            0.6352        1.0022        6.7771\n",
      "      8        0.6147       0.5981            0.5568            0.5981        1.1577        6.7504\n",
      "      9        \u001b[36m0.5700\u001b[0m       0.6411            0.6000            0.6411        0.9256        6.8120\n",
      "     10        \u001b[36m0.5439\u001b[0m       0.6029            0.5582            0.6029        1.1334        6.7496\n",
      "     11        \u001b[36m0.4877\u001b[0m       \u001b[32m0.6914\u001b[0m            \u001b[35m0.6783\u001b[0m            \u001b[31m0.6914\u001b[0m        \u001b[94m0.8235\u001b[0m     +  6.7705\n",
      "     12        \u001b[36m0.4293\u001b[0m       0.6017            0.5916            0.6017        1.1674        6.9215\n",
      "     13        0.5273       0.6089            0.5593            0.6089        1.2402        6.8038\n",
      "     14        0.4668       \u001b[32m0.7093\u001b[0m            \u001b[35m0.6991\u001b[0m            \u001b[31m0.7093\u001b[0m        0.8721     +  6.7794\n",
      "     15        0.5218       0.6459            0.6132            0.6459        0.9172        6.8473\n",
      "     16        0.4534       0.7057            0.6784            0.7057        0.8405        6.8083\n",
      "     17        \u001b[36m0.4102\u001b[0m       0.4079            0.3648            0.4079        3.5755        6.7735\n",
      "     18        0.4844       0.5873            0.5709            0.5873        1.4159        6.7521\n",
      "     19        0.4135       \u001b[32m0.7141\u001b[0m            0.6941            \u001b[31m0.7141\u001b[0m        0.8630        6.7865\n",
      "     20        0.4586       0.7129            0.6813            0.7129        0.8438        6.7539\n",
      "     21        \u001b[36m0.3700\u001b[0m       \u001b[32m0.7333\u001b[0m            \u001b[35m0.7169\u001b[0m            \u001b[31m0.7333\u001b[0m        \u001b[94m0.7259\u001b[0m     +  6.7761\n",
      "     22        \u001b[36m0.3268\u001b[0m       0.6974            0.6583            0.6974        0.9220        6.8309\n",
      "     23        \u001b[36m0.2558\u001b[0m       0.6998            0.6883            0.6998        0.9030        6.8178\n",
      "     24        0.2897       0.7189            0.7111            0.7189        0.8554        6.7820\n",
      "     25        0.2997       0.7189            \u001b[35m0.7191\u001b[0m            0.7189        0.9836     +  6.7953\n",
      "     26        0.3580       0.6842            0.6737            0.6842        0.9844        6.8190\n",
      "     27        0.4389       0.6519            0.6138            0.6519        1.2661        6.7924\n",
      "     28        0.3257       0.6830            0.6708            0.6830        1.0418        6.7766\n",
      "     29        0.3163       0.7249            0.7019            0.7249        0.8771        6.7745\n",
      "     30        \u001b[36m0.2279\u001b[0m       0.6962            0.6888            0.6962        0.8960        6.7906\n",
      "     31        \u001b[36m0.1976\u001b[0m       0.7249            0.7191            0.7249        0.9017        6.7616\n",
      "     32        \u001b[36m0.1959\u001b[0m       0.7309            0.7160            0.7309        0.7951        6.7573\n",
      "     33        \u001b[36m0.1361\u001b[0m       0.7261            \u001b[35m0.7229\u001b[0m            0.7261        0.9368     +  6.7813\n",
      "     34        0.1429       \u001b[32m0.7380\u001b[0m            \u001b[35m0.7305\u001b[0m            \u001b[31m0.7380\u001b[0m        0.9012     +  6.8697\n",
      "     35        \u001b[36m0.1182\u001b[0m       0.7285            0.7208            0.7285        0.9222        6.8186\n",
      "     36        0.1247       0.7129            0.6987            0.7129        1.0151        6.7808\n",
      "     37        \u001b[36m0.1055\u001b[0m       0.7380            0.7300            0.7380        1.0670        6.7916\n",
      "     38        \u001b[36m0.0749\u001b[0m       \u001b[32m0.7416\u001b[0m            \u001b[35m0.7337\u001b[0m            \u001b[31m0.7416\u001b[0m        1.0713     +  6.7778\n",
      "     39        0.1160       0.7153            0.7148            0.7153        1.2517        6.8158\n",
      "     40        0.1043       0.7392            \u001b[35m0.7343\u001b[0m            0.7392        1.1144     +  6.8002\n",
      "     41        0.1556       0.7177            0.7034            0.7177        1.1255        6.8111\n",
      "     42        0.1042       0.7177            0.6897            0.7177        1.3095        6.8068\n",
      "     43        0.1520       0.6699            0.6632            0.6699        1.3813        6.7875\n",
      "     44        0.1702       0.6926            0.6698            0.6926        1.2536        6.7751\n",
      "     45        0.0917       0.7416            \u001b[35m0.7359\u001b[0m            0.7416        1.0185     +  6.7654\n",
      "     46        \u001b[36m0.0605\u001b[0m       \u001b[32m0.7440\u001b[0m            \u001b[35m0.7386\u001b[0m            \u001b[31m0.7440\u001b[0m        1.0459     +  6.8110\n",
      "     47        \u001b[36m0.0444\u001b[0m       \u001b[32m0.7512\u001b[0m            \u001b[35m0.7469\u001b[0m            \u001b[31m0.7512\u001b[0m        1.1003     +  6.7975\n",
      "     48        0.0455       0.7464            0.7359            0.7464        1.1508        6.8384\n",
      "     49        \u001b[36m0.0421\u001b[0m       0.7476            0.7450            0.7476        1.1067        6.8160\n",
      "     50        \u001b[36m0.0245\u001b[0m       \u001b[32m0.7548\u001b[0m            0.7462            \u001b[31m0.7548\u001b[0m        1.1657        6.7807\n",
      "     51        \u001b[36m0.0231\u001b[0m       0.7500            0.7452            0.7500        1.2115        6.7989\n",
      "     52        \u001b[36m0.0150\u001b[0m       \u001b[32m0.7572\u001b[0m            \u001b[35m0.7514\u001b[0m            \u001b[31m0.7572\u001b[0m        1.2492     +  6.7568\n",
      "     53        0.0306       \u001b[32m0.7584\u001b[0m            \u001b[35m0.7564\u001b[0m            \u001b[31m0.7584\u001b[0m        1.2412     +  6.8593\n",
      "     54        0.0215       0.7524            0.7436            0.7524        1.3205        6.7617\n",
      "     55        0.1078       0.7356            0.7283            0.7356        1.2010        6.7439\n",
      "     56        0.0428       0.7512            0.7438            0.7512        1.1804        6.7339\n",
      "     57        0.0386       0.7416            0.7355            0.7416        1.2101        6.7620\n",
      "     58        0.0268       0.7512            0.7388            0.7512        1.2684        6.8002\n",
      "     59        0.0215       0.7440            0.7383            0.7440        1.2317        6.7856\n",
      "     60        0.0247       0.7476            0.7423            0.7476        1.1962        6.7859\n",
      "     61        0.0176       0.7512            0.7435            0.7512        1.2249        6.7756\n",
      "     62        0.0205       0.7452            0.7404            0.7452        1.2106        6.7929\n",
      "     63        \u001b[36m0.0127\u001b[0m       0.7488            0.7424            0.7488        1.2459        6.7607\n",
      "     64        \u001b[36m0.0105\u001b[0m       0.7416            0.7354            0.7416        1.2775        6.7908\n",
      "     65        0.0122       0.7500            0.7411            0.7500        1.2797        6.7927\n",
      "     66        0.0151       0.7452            0.7400            0.7452        1.3049        6.7618\n",
      "     67        0.0126       0.7321            0.7270            0.7321        1.3480        6.7574\n",
      "     68        \u001b[36m0.0099\u001b[0m       0.7416            0.7332            0.7416        1.3455        6.7664\n",
      "     69        0.0108       0.7416            0.7352            0.7416        1.3924        6.7869\n",
      "     70        0.0337       0.7464            0.7387            0.7464        1.3638        6.7840\n",
      "     71        0.0135       0.7404            0.7371            0.7404        1.3425        6.7675\n",
      "     72        0.0104       0.7356            0.7301            0.7356        1.3184        6.7999\n",
      "     73        0.0152       0.7309            0.7278            0.7309        1.3203        6.7714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74        0.0151       0.7452            0.7393            0.7452        1.3132        6.7643\n",
      "     75        0.0129       0.7404            0.7367            0.7404        1.3293        6.7693\n",
      "     76        \u001b[36m0.0086\u001b[0m       0.7416            0.7355            0.7416        1.3314        6.7875\n",
      "     77        0.0099       0.7380            0.7328            0.7380        1.3078        6.7743\n",
      "     78        \u001b[36m0.0065\u001b[0m       0.7464            0.7424            0.7464        1.3244        6.7699\n",
      "     79        \u001b[36m0.0065\u001b[0m       0.7464            0.7394            0.7464        1.3218        6.7648\n",
      "     80        \u001b[36m0.0058\u001b[0m       0.7476            0.7418            0.7476        1.3390        6.7954\n",
      "     81        0.0082       0.7380            0.7328            0.7380        1.3589        6.7855\n",
      "     82        0.0070       0.7440            0.7375            0.7440        1.3396        6.7456\n",
      "     83        0.0078       0.7356            0.7336            0.7356        1.3431        6.7651\n",
      "     84        \u001b[36m0.0052\u001b[0m       0.7452            0.7395            0.7452        1.3613        6.7836\n",
      "     85        0.0056       0.7392            0.7336            0.7392        1.3429        6.7970\n",
      "     86        0.0062       0.7488            0.7428            0.7488        1.3466        6.7799\n",
      "     87        \u001b[36m0.0039\u001b[0m       0.7536            0.7483            0.7536        1.3408        6.7563\n",
      "     88        0.0059       0.7488            0.7447            0.7488        1.3655        6.7650\n",
      "     89        0.0109       0.7476            0.7422            0.7476        1.3710        6.8084\n",
      "     90        0.0061       0.7524            0.7435            0.7524        1.3398        6.7756\n",
      "     91        0.0059       0.7416            0.7340            0.7416        1.3602        6.7568\n",
      "     92        0.0039       0.7452            0.7396            0.7452        1.3705        6.7838\n",
      "     93        0.0041       0.7404            0.7342            0.7404        1.3718        6.7901\n",
      "     94        0.0042       0.7500            0.7449            0.7500        1.3533        6.7685\n",
      "     95        \u001b[36m0.0036\u001b[0m       0.7500            0.7444            0.7500        1.3591        6.7862\n",
      "     96        0.0046       0.7500            0.7447            0.7500        1.3747        6.8008\n",
      "     97        \u001b[36m0.0029\u001b[0m       0.7476            0.7429            0.7476        1.3769        6.7823\n",
      "     98        0.0044       0.7524            0.7461            0.7524        1.3739        6.7563\n",
      "     99        0.0036       0.7452            0.7385            0.7452        1.3431        6.7983\n",
      "    100        0.0042       0.7464            0.7431            0.7464        1.3577        6.7769\n",
      "    101        0.0038       0.7464            0.7429            0.7464        1.3535        6.7690\n",
      "    102        0.0044       0.7452            0.7406            0.7452        1.3634        6.8056\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.1810, 0.0000, 0.0081]), 'p99': tensor([0.2076, 0.0057, 0.0104])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m1.0874\u001b[0m       \u001b[32m0.4833\u001b[0m            \u001b[35m0.4291\u001b[0m            \u001b[31m0.4833\u001b[0m        \u001b[94m2.2107\u001b[0m     +  6.9324\n",
      "      2        \u001b[36m0.7877\u001b[0m       \u001b[32m0.5993\u001b[0m            \u001b[35m0.5959\u001b[0m            \u001b[31m0.5993\u001b[0m        \u001b[94m1.0383\u001b[0m     +  6.8179\n",
      "      3        \u001b[36m0.7220\u001b[0m       \u001b[32m0.6495\u001b[0m            \u001b[35m0.6498\u001b[0m            \u001b[31m0.6495\u001b[0m        \u001b[94m0.9003\u001b[0m     +  6.7698\n",
      "      4        \u001b[36m0.6436\u001b[0m       0.5467            0.5078            0.5467        1.1548        6.7549\n",
      "      5        \u001b[36m0.6417\u001b[0m       0.6053            0.5799            0.6053        1.1069        6.7345\n",
      "      6        \u001b[36m0.5939\u001b[0m       0.5395            0.5162            0.5395        1.3922        6.7740\n",
      "      7        \u001b[36m0.5542\u001b[0m       \u001b[32m0.6711\u001b[0m            \u001b[35m0.6675\u001b[0m            \u001b[31m0.6711\u001b[0m        0.9475     +  6.7438\n",
      "      8        \u001b[36m0.4994\u001b[0m       \u001b[32m0.6794\u001b[0m            \u001b[35m0.6773\u001b[0m            \u001b[31m0.6794\u001b[0m        \u001b[94m0.8123\u001b[0m     +  6.8063\n",
      "      9        0.5071       0.6232            0.6107            0.6232        1.0785        6.8202\n",
      "     10        0.5870       0.6112            0.5518            0.6112        1.1816        6.8001\n",
      "     11        0.5585       0.6208            0.6007            0.6208        1.0551        6.7512\n",
      "     12        0.5292       0.6340            0.6178            0.6340        1.0792        6.7254\n",
      "     13        \u001b[36m0.4638\u001b[0m       \u001b[32m0.6902\u001b[0m            0.6518            \u001b[31m0.6902\u001b[0m        0.8234        6.7221\n",
      "     14        0.4683       0.6722            0.6705            0.6722        0.9843        6.7546\n",
      "     15        \u001b[36m0.4579\u001b[0m       0.6651            0.6308            0.6651        0.9703        6.7355\n",
      "     16        \u001b[36m0.4043\u001b[0m       \u001b[32m0.7321\u001b[0m            \u001b[35m0.7203\u001b[0m            \u001b[31m0.7321\u001b[0m        \u001b[94m0.7578\u001b[0m     +  6.7625\n",
      "     17        \u001b[36m0.3262\u001b[0m       0.6962            0.6792            0.6962        0.8931        6.8293\n",
      "     18        0.3999       0.6878            0.6797            0.6878        0.9124        6.8198\n",
      "     19        0.4374       0.6962            0.6875            0.6962        0.9004        6.8021\n",
      "     20        0.4650       0.6746            0.6296            0.6746        1.0319        6.7800\n",
      "     21        0.3368       0.6866            0.6881            0.6866        0.8304        6.7515\n",
      "     22        0.3544       0.6926            0.6937            0.6926        0.9501        6.7442\n",
      "     23        \u001b[36m0.3227\u001b[0m       0.6770            0.6612            0.6770        1.0951        6.7895\n",
      "     24        \u001b[36m0.2763\u001b[0m       0.6878            0.6699            0.6878        1.0220        6.7601\n",
      "     25        0.3152       0.6962            0.6844            0.6962        1.0061        6.7890\n",
      "     26        0.2935       0.7213            0.7071            0.7213        0.8479        6.7357\n",
      "     27        \u001b[36m0.2736\u001b[0m       0.7153            0.7019            0.7153        0.8811        6.7124\n",
      "     28        \u001b[36m0.2059\u001b[0m       0.6842            0.6804            0.6842        1.0319        6.7843\n",
      "     29        0.2350       0.6950            0.6706            0.6950        1.2126        6.7741\n",
      "     30        0.3229       0.6770            0.6699            0.6770        0.9855        6.7638\n",
      "     31        0.2158       0.6746            0.6492            0.6746        1.0934        6.7824\n",
      "     32        \u001b[36m0.1835\u001b[0m       0.6734            0.6610            0.6734        1.2087        6.7625\n",
      "     33        0.2278       0.6400            0.6254            0.6400        1.5400        6.7526\n",
      "     34        0.2594       0.6734            0.6568            0.6734        1.0988        6.7559\n",
      "     35        \u001b[36m0.1832\u001b[0m       0.7010            0.6803            0.7010        1.1542        6.7904\n",
      "     36        \u001b[36m0.1400\u001b[0m       0.7033            0.7016            0.7033        1.1350        6.7691\n",
      "     37        0.2157       0.6890            0.6823            0.6890        1.2812        6.7866\n",
      "     38        0.1658       0.6531            0.6285            0.6531        1.3396        6.7463\n",
      "     39        0.3076       0.6746            0.6757            0.6746        1.2331        6.7923\n",
      "     40        0.1702       0.7081            0.6790            0.7081        1.1191        6.7781\n",
      "     41        0.1439       0.6663            0.6395            0.6663        1.3684        6.7790\n",
      "     42        0.2185       0.6950            0.6965            0.6950        1.1920        6.7780\n",
      "     43        0.1821       0.7225            0.7110            0.7225        1.0177        6.7794\n",
      "     44        \u001b[36m0.1031\u001b[0m       \u001b[32m0.7333\u001b[0m            \u001b[35m0.7213\u001b[0m            \u001b[31m0.7333\u001b[0m        1.0261     +  6.7685\n",
      "     45        \u001b[36m0.0875\u001b[0m       \u001b[32m0.7392\u001b[0m            \u001b[35m0.7359\u001b[0m            \u001b[31m0.7392\u001b[0m        1.0286     +  6.8535\n",
      "     46        \u001b[36m0.0719\u001b[0m       0.7201            0.7145            0.7201        1.0902        6.8005\n",
      "     47        \u001b[36m0.0543\u001b[0m       0.7249            0.7221            0.7249        1.1423        6.7477\n",
      "     48        0.0684       0.7165            0.7181            0.7165        1.2131        6.7708\n",
      "     49        \u001b[36m0.0479\u001b[0m       0.7261            0.7194            0.7261        1.1359        6.7746\n",
      "     50        \u001b[36m0.0473\u001b[0m       0.7333            0.7278            0.7333        1.1881        6.7253\n",
      "     51        \u001b[36m0.0335\u001b[0m       0.7297            0.7249            0.7297        1.2215        6.7860\n",
      "     52        \u001b[36m0.0294\u001b[0m       0.7297            0.7251            0.7297        1.2947        6.7423\n",
      "     53        \u001b[36m0.0243\u001b[0m       0.7201            0.7187            0.7201        1.3458        6.7779\n",
      "     54        \u001b[36m0.0182\u001b[0m       0.7129            0.7043            0.7129        1.3306        6.7275\n",
      "     55        0.0574       0.7165            0.7118            0.7165        1.3852        6.7797\n",
      "     56        0.0734       0.7081            0.7001            0.7081        1.4004        6.8047\n",
      "     57        0.0379       0.7249            0.7163            0.7249        1.2968        6.7632\n",
      "     58        0.0273       0.7273            0.7227            0.7273        1.2986        6.7599\n",
      "     59        0.0193       0.7333            0.7225            0.7333        1.3076        6.7631\n",
      "     60        0.0344       0.7153            0.7083            0.7153        1.3898        6.7683\n",
      "     61        0.0204       0.7141            0.7063            0.7141        1.4048        6.7944\n",
      "     62        \u001b[36m0.0131\u001b[0m       0.7177            0.7058            0.7177        1.4219        6.7901\n",
      "     63        \u001b[36m0.0116\u001b[0m       0.7249            0.7199            0.7249        1.3587        6.7733\n",
      "     64        \u001b[36m0.0085\u001b[0m       0.7368            0.7316            0.7368        1.3990        6.7674\n",
      "     65        \u001b[36m0.0062\u001b[0m       0.7177            0.7104            0.7177        1.4380        6.7682\n",
      "     66        0.0078       0.7297            0.7244            0.7297        1.4396        6.7395\n",
      "     67        0.0067       0.7249            0.7173            0.7249        1.4536        6.7726\n",
      "     68        \u001b[36m0.0062\u001b[0m       0.7261            0.7188            0.7261        1.4657        6.7693\n",
      "     69        \u001b[36m0.0043\u001b[0m       0.7141            0.7102            0.7141        1.4649        6.7825\n",
      "     70        \u001b[36m0.0041\u001b[0m       0.7165            0.7117            0.7165        1.4543        6.7914\n",
      "     71        \u001b[36m0.0023\u001b[0m       0.7237            0.7185            0.7237        1.4549        6.7806\n",
      "     72        0.0039       0.7177            0.7124            0.7177        1.4714        6.7619\n",
      "     73        0.0031       0.7177            0.7131            0.7177        1.5088        6.7579\n",
      "     74        0.0092       0.7249            0.7168            0.7249        1.5144        6.7697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     75        0.0049       \u001b[32m0.7440\u001b[0m            \u001b[35m0.7399\u001b[0m            \u001b[31m0.7440\u001b[0m        1.4974     +  6.7753\n",
      "     76        0.0293       0.6998            0.6886            0.6998        1.7176        6.8339\n",
      "     77        0.0126       0.7177            0.7176            0.7177        1.5941        6.8122\n",
      "     78        0.0066       0.7428            0.7359            0.7428        1.5130        6.7465\n",
      "     79        0.0045       0.7321            0.7259            0.7321        1.5167        6.7540\n",
      "     80        0.0039       0.7333            0.7293            0.7333        1.5188        6.7663\n",
      "     81        0.0040       0.7285            0.7239            0.7285        1.5688        6.7479\n",
      "     82        0.0062       0.7285            0.7257            0.7285        1.5480        6.7595\n",
      "     83        0.0026       0.7213            0.7194            0.7213        1.5867        6.8056\n",
      "     84        0.0026       0.7249            0.7229            0.7249        1.5528        6.7764\n",
      "     85        0.0031       0.7249            0.7218            0.7249        1.5528        6.7574\n",
      "     86        0.0027       0.7177            0.7143            0.7177        1.5680        6.7452\n",
      "     87        0.0025       0.7237            0.7197            0.7237        1.5551        6.7680\n",
      "     88        \u001b[36m0.0019\u001b[0m       0.7225            0.7186            0.7225        1.5525        6.7580\n",
      "     89        0.0039       0.7261            0.7227            0.7261        1.5632        6.7971\n",
      "     90        0.0052       0.7321            0.7278            0.7321        1.5592        6.7763\n",
      "     91        0.0056       0.7321            0.7295            0.7321        1.6022        6.7821\n",
      "     92        0.0043       0.7309            0.7283            0.7309        1.5693        6.7880\n",
      "     93        0.0029       0.7309            0.7258            0.7309        1.5249        6.7984\n",
      "     94        0.0035       0.7249            0.7188            0.7249        1.5291        6.7649\n",
      "     95        0.0032       0.7356            0.7304            0.7356        1.5403        6.7732\n",
      "     96        0.0068       0.7261            0.7189            0.7261        1.5624        6.7325\n",
      "     97        0.0038       0.7297            0.7235            0.7297        1.5548        6.7907\n",
      "     98        0.0053       0.7333            0.7291            0.7333        1.5714        6.7389\n",
      "     99        0.0038       0.7321            0.7281            0.7321        1.5585        6.7403\n",
      "    100        0.0029       0.7321            0.7280            0.7321        1.5547        6.7586\n",
      "    101        0.0039       0.7285            0.7250            0.7285        1.5557        6.7899\n",
      "    102        0.0033       0.7237            0.7217            0.7237        1.5843        6.7738\n",
      "    103        0.0025       0.7213            0.7190            0.7213        1.6051        6.7743\n",
      "    104        0.0027       0.7189            0.7156            0.7189        1.5799        6.7470\n",
      "    105        0.0024       0.7201            0.7177            0.7201        1.5652        6.7810\n",
      "    106        0.0021       0.7273            0.7238            0.7273        1.5805        6.7883\n",
      "    107        0.0025       0.7177            0.7162            0.7177        1.5758        6.7459\n",
      "    108        0.0025       0.7285            0.7241            0.7285        1.5530        6.7692\n",
      "    109        0.0021       0.7261            0.7223            0.7261        1.5608        6.7579\n",
      "    110        0.0025       0.7249            0.7208            0.7249        1.5599        6.7983\n",
      "    111        0.0029       0.7309            0.7278            0.7309        1.5901        6.7667\n",
      "    112        0.0029       0.7297            0.7258            0.7297        1.5764        6.7814\n",
      "    113        0.0020       0.7297            0.7263            0.7297        1.5847        6.8041\n",
      "    114        0.0025       0.7237            0.7195            0.7237        1.6079        6.7815\n",
      "    115        0.0026       0.7261            0.7208            0.7261        1.5697        6.8024\n",
      "    116        0.0019       0.7273            0.7244            0.7273        1.5877        6.7419\n",
      "    117        0.0023       0.7249            0.7222            0.7249        1.5835        6.7489\n",
      "    118        \u001b[36m0.0018\u001b[0m       0.7297            0.7264            0.7297        1.5724        6.7716\n",
      "    119        0.0020       0.7297            0.7264            0.7297        1.5555        6.8071\n",
      "    120        0.0026       0.7344            0.7308            0.7344        1.5596        6.7854\n",
      "    121        0.0023       0.7261            0.7222            0.7261        1.5880        6.7539\n",
      "    122        0.0019       0.7249            0.7213            0.7249        1.5803        6.7500\n",
      "    123        \u001b[36m0.0015\u001b[0m       0.7261            0.7219            0.7261        1.5676        6.7652\n",
      "    124        0.0020       0.7297            0.7247            0.7297        1.5592        6.7754\n",
      "Stopping since valid_f1_macro has not improved in the last 50 epochs.\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:22<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'p01': tensor([0.1810, 0.0000, 0.0080]), 'p99': tensor([0.2077, 0.0056, 0.0105])}\n",
      "  epoch    train_loss    valid_acc    valid_f1_macro    valid_f1_micro    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ----------------  ----------------  ------------  ----  ------\n",
      "      1        \u001b[36m1.1287\u001b[0m       \u001b[32m0.4629\u001b[0m            \u001b[35m0.3919\u001b[0m            \u001b[31m0.4629\u001b[0m        \u001b[94m2.1929\u001b[0m     +  6.9010\n",
      "      2        \u001b[36m0.8062\u001b[0m       \u001b[32m0.5969\u001b[0m            \u001b[35m0.5022\u001b[0m            \u001b[31m0.5969\u001b[0m        \u001b[94m1.1571\u001b[0m     +  6.8219\n",
      "      3        \u001b[36m0.7190\u001b[0m       \u001b[32m0.6053\u001b[0m            \u001b[35m0.5607\u001b[0m            \u001b[31m0.6053\u001b[0m        \u001b[94m1.0505\u001b[0m     +  6.7654\n",
      "      4        \u001b[36m0.6374\u001b[0m       0.5383            0.5139            0.5383        1.4373        6.7984\n",
      "      5        \u001b[36m0.6085\u001b[0m       \u001b[32m0.6364\u001b[0m            \u001b[35m0.5715\u001b[0m            \u001b[31m0.6364\u001b[0m        \u001b[94m1.0409\u001b[0m     +  6.7704\n",
      "      6        0.6306       0.6100            \u001b[35m0.5947\u001b[0m            0.6100        \u001b[94m0.9813\u001b[0m     +  6.7365\n",
      "      7        \u001b[36m0.5217\u001b[0m       0.5945            0.5570            0.5945        1.2042        6.8092\n",
      "      8        0.5420       \u001b[32m0.6388\u001b[0m            \u001b[35m0.6130\u001b[0m            \u001b[31m0.6388\u001b[0m        \u001b[94m0.9659\u001b[0m     +  6.7742\n",
      "      9        \u001b[36m0.5187\u001b[0m       \u001b[32m0.6734\u001b[0m            \u001b[35m0.6238\u001b[0m            \u001b[31m0.6734\u001b[0m        0.9803     +  6.7748\n",
      "     10        \u001b[36m0.4993\u001b[0m       \u001b[32m0.6782\u001b[0m            \u001b[35m0.6495\u001b[0m            \u001b[31m0.6782\u001b[0m        0.9679     +  6.8048\n",
      "     11        \u001b[36m0.4684\u001b[0m       \u001b[32m0.7129\u001b[0m            \u001b[35m0.6929\u001b[0m            \u001b[31m0.7129\u001b[0m        \u001b[94m0.8037\u001b[0m     +  6.7894\n",
      "     12        \u001b[36m0.4596\u001b[0m       0.6734            0.6596            0.6734        0.9479        6.9036\n",
      "     13        \u001b[36m0.4312\u001b[0m       0.6854            0.6556            0.6854        0.8948        6.8416\n",
      "     14        0.4332       0.6591            0.6240            0.6591        1.1033        6.8062\n",
      "     15        \u001b[36m0.3741\u001b[0m       0.6148            0.5811            0.6148        1.2644        6.7949\n",
      "     16        \u001b[36m0.3546\u001b[0m       0.6950            0.6762            0.6950        0.9650        6.7864\n",
      "     17        0.3953       0.6854            0.6804            0.6854        0.8584        6.7749\n",
      "     18        \u001b[36m0.3492\u001b[0m       0.6842            0.6810            0.6842        1.0068        6.7608\n",
      "     19        \u001b[36m0.3311\u001b[0m       0.6974            0.6862            0.6974        0.8692        6.7684\n",
      "     20        \u001b[36m0.3208\u001b[0m       0.6663            0.6518            0.6663        1.1814        6.7634\n",
      "     21        0.3799       0.6866            0.6559            0.6866        1.2846        6.7594\n",
      "     22        0.3587       0.7010            0.6752            0.7010        0.9316        6.7603\n",
      "     23        \u001b[36m0.2673\u001b[0m       0.6543            0.6449            0.6543        1.0452        6.7819\n",
      "     24        0.3658       0.6830            0.6670            0.6830        1.2178        6.7592\n",
      "     25        0.3492       \u001b[32m0.7225\u001b[0m            \u001b[35m0.7231\u001b[0m            \u001b[31m0.7225\u001b[0m        0.9322     +  6.7958\n",
      "     26        \u001b[36m0.2481\u001b[0m       0.6938            0.6737            0.6938        1.0465        6.8444\n",
      "     27        \u001b[36m0.2053\u001b[0m       0.6914            0.6716            0.6914        1.1799        6.8444\n",
      "     28        \u001b[36m0.1725\u001b[0m       0.7153            0.7098            0.7153        1.0622        6.7884\n",
      "     29        0.1873       0.7081            0.7018            0.7081        1.0507        6.7827\n",
      "     30        0.1879       \u001b[32m0.7237\u001b[0m            0.7176            \u001b[31m0.7237\u001b[0m        0.9756        6.7877\n",
      "     31        \u001b[36m0.1637\u001b[0m       0.6675            0.6453            0.6675        1.3572        6.7834\n",
      "     32        0.1866       0.6651            0.6543            0.6651        1.3347        6.7821\n",
      "     33        0.3260       0.6495            0.6359            0.6495        1.4874        6.7692\n",
      "     34        0.2270       0.6734            0.6489            0.6734        1.3307        6.7552\n",
      "     35        0.2434       0.7069            0.6991            0.7069        1.0296        6.7404\n",
      "     36        0.1953       0.7165            0.6984            0.7165        1.1223        6.7334\n",
      "     37        0.2118       0.6854            0.6791            0.6854        1.0780        6.7709\n",
      "     38        \u001b[36m0.1536\u001b[0m       0.7153            0.7060            0.7153        1.0829        6.7624\n",
      "     39        \u001b[36m0.1212\u001b[0m       0.7225            0.7065            0.7225        0.9891        6.7616\n",
      "     40        \u001b[36m0.0947\u001b[0m       \u001b[32m0.7309\u001b[0m            0.7195            \u001b[31m0.7309\u001b[0m        1.1970        6.7671\n",
      "     41        \u001b[36m0.0629\u001b[0m       0.7165            0.7119            0.7165        1.1169        6.7633\n",
      "     42        0.0840       0.7105            0.6950            0.7105        1.1641        6.7690\n",
      "     43        \u001b[36m0.0617\u001b[0m       0.7213            0.7123            0.7213        1.2254        6.7851\n",
      "     44        \u001b[36m0.0550\u001b[0m       0.6962            0.6909            0.6962        1.1866        6.7533\n",
      "     45        0.0792       0.7189            0.7111            0.7189        1.3190        6.7694\n",
      "     46        \u001b[36m0.0548\u001b[0m       0.7177            0.6944            0.7177        1.2899        6.7944\n",
      "     47        \u001b[36m0.0412\u001b[0m       0.6998            0.6920            0.6998        1.2779        6.7751\n",
      "     48        \u001b[36m0.0244\u001b[0m       0.7189            0.7143            0.7189        1.2799        6.7752\n",
      "     49        0.0271       0.7069            0.6954            0.7069        1.4216        6.7787\n",
      "     50        0.0251       0.7297            \u001b[35m0.7238\u001b[0m            0.7297        1.3029     +  6.7430\n",
      "     51        0.0289       0.7225            0.7164            0.7225        1.3976        6.8589\n",
      "     52        \u001b[36m0.0215\u001b[0m       \u001b[32m0.7321\u001b[0m            \u001b[35m0.7241\u001b[0m            \u001b[31m0.7321\u001b[0m        1.4727     +  6.8245\n",
      "     53        0.0988       0.7093            0.6915            0.7093        1.4212        6.8178\n",
      "     54        0.0587       0.6734            0.6705            0.6734        1.5662        6.8252\n",
      "     55        0.0641       0.6830            0.6757            0.6830        1.5429        6.7623\n",
      "     56        0.0736       0.6782            0.6577            0.6782        1.6603        6.7629\n",
      "     57        0.0544       0.7165            0.7118            0.7165        1.4517        6.7553\n",
      "     58        0.0456       0.6854            0.6651            0.6854        1.5797        6.7560\n",
      "     59        0.0238       0.7165            0.7131            0.7165        1.4006        6.7712\n",
      "     60        \u001b[36m0.0184\u001b[0m       0.7033            0.6987            0.7033        1.4435        6.7726\n",
      "     61        \u001b[36m0.0182\u001b[0m       0.6998            0.6897            0.6998        1.4569        6.7663\n",
      "     62        0.0206       0.7177            0.6998            0.7177        1.5384        6.7591\n",
      "     63        0.0279       0.6986            0.6980            0.6986        1.6024        6.7752\n",
      "     64        0.0518       0.6902            0.6854            0.6902        1.6085        6.7704\n",
      "     65        0.0231       0.7153            0.7089            0.7153        1.5473        6.7639\n",
      "     66        \u001b[36m0.0105\u001b[0m       0.7153            0.7076            0.7153        1.5402        6.7232\n",
      "     67        \u001b[36m0.0081\u001b[0m       0.7237            0.7177            0.7237        1.4711        6.7857\n",
      "     68        0.0135       0.7105            0.7004            0.7105        1.6632        6.7913\n",
      "     69        \u001b[36m0.0053\u001b[0m       0.7069            0.6990            0.7069        1.5620        6.7862\n",
      "     70        \u001b[36m0.0047\u001b[0m       0.7141            0.7094            0.7141        1.5631        6.7627\n",
      "     71        \u001b[36m0.0037\u001b[0m       0.7093            0.7060            0.7093        1.5455        6.7626\n",
      "     72        0.0045       0.7081            0.7025            0.7081        1.5795        6.7610\n",
      "     73        \u001b[36m0.0024\u001b[0m       0.7177            0.7100            0.7177        1.5946        6.7823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74        0.0024       0.7153            0.7090            0.7153        1.5821        6.7845\n",
      "     75        0.0040       0.7153            0.7070            0.7153        1.6064        6.7698\n",
      "     76        0.0110       0.7177            0.7157            0.7177        1.6569        6.7619\n",
      "     77        0.0045       0.7261            0.7209            0.7261        1.5634        6.7783\n",
      "     78        0.0045       0.7177            0.7135            0.7177        1.6217        6.7536\n",
      "     79        0.0026       0.7189            0.7148            0.7189        1.6372        6.7932\n",
      "     80        0.0026       0.7189            0.7138            0.7189        1.6775        6.7567\n",
      "     81        0.0025       0.7249            0.7180            0.7249        1.6287        6.7782\n",
      "     82        0.0035       0.7261            0.7214            0.7261        1.6393        6.8166\n",
      "     83        0.0137       0.7153            0.7075            0.7153        1.6227        6.7850\n",
      "     84        0.0055       0.7237            0.7205            0.7237        1.6014        6.7553\n",
      "     85        0.0032       0.7201            0.7154            0.7201        1.5990        6.7460\n",
      "     86        \u001b[36m0.0024\u001b[0m       0.7225            0.7174            0.7225        1.6098        6.7686\n",
      "     87        0.0043       0.7273            0.7221            0.7273        1.6242        6.8030\n",
      "     88        \u001b[36m0.0020\u001b[0m       0.7225            0.7147            0.7225        1.6396        6.7523\n",
      "     89        0.0042       0.7213            0.7138            0.7213        1.6219        6.7582\n",
      "     90        0.0056       0.7165            0.7106            0.7165        1.6941        6.7917\n",
      "     91        0.0032       0.7057            0.7028            0.7057        1.7295        6.7857\n",
      "     92        0.0053       0.7129            0.7087            0.7129        1.7024        6.7754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from scifAI.dl.models import PretrainedModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_top_channels = 3\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed_value, shuffle=True)\n",
    "\n",
    "\n",
    "results_with_best_channels = pd.DataFrame(columns = [\"method\",\n",
    "                                                     \"f1_micro\",\n",
    "                                                     \"f1_macro\",\n",
    "                                                     \"accuracy\"])\n",
    "\n",
    "for met in interpretation_methods:\n",
    "    selected_channels = select_top_channels(channel_importance, met , num_top_channels)\n",
    "    channels = np.asarray([ \"Ch\" + str(i) for i in selected_channels])\n",
    "    \n",
    "    print(met, selected_channels)\n",
    "    num_of_all_channels = len(channels)\n",
    "    all_channels = np.arange(num_of_all_channels)\n",
    "    for train_index, test_index in skf.split(metadata.index.tolist(), metadata[\"label\"]):\n",
    "        train_index, validation_index, _, _ = train_test_split(train_index, \n",
    "                                                    metadata.loc[train_index,\"label\"].index.tolist(), \n",
    "                                                    stratify = metadata.loc[train_index,\"label\"].tolist(),\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=seed_value)\n",
    "        label_map = dict(zip(sorted(set(metadata.loc[train_index, \"label\"])), \n",
    "                     np.arange(len(set(metadata.loc[train_index, \"label\"])))))\n",
    "\n",
    "        set_of_interesting_classes = metadata.label.unique().tolist()\n",
    "\n",
    "        num_classes = len(metadata.label.unique())\n",
    "        \n",
    "        train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor,\n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform=transforms.Compose([ ] ))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        statistics = get_statistics(train_loader, selected_channels=selected_channels)\n",
    "        \n",
    "        stats = dict()\n",
    "        stats[\"lower_bound\"] = torch.tensor([statistics['p01'][0], \n",
    "                                             statistics['p01'][1], \n",
    "                                             statistics['p01'][2]])\n",
    "\n",
    "        stats[\"upper_bound\"] = torch.tensor([statistics['p99'][0], \n",
    "                                             statistics['p99'][1], \n",
    "                                             statistics['p99'][2]])\n",
    "\n",
    "        train_transform = [ \n",
    "                MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                        max_in =  stats[\"upper_bound\"] , \n",
    "                                        min_out =  0. , \n",
    "                                        max_out =  1.),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                AddGaussianNoise(mean=0., std=0.01),\n",
    "        ]\n",
    "\n",
    "        validation_transform =  [ \n",
    "                MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                        max_in =  stats[\"upper_bound\"] , \n",
    "                                        min_out =  0. , \n",
    "                                        max_out =  1.),\n",
    "        ]\n",
    "\n",
    "        test_transform =  [ \n",
    "                MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                        max_in =  stats[\"upper_bound\"] , \n",
    "                                        min_out =  0. , \n",
    "                                        max_out =  1.),\n",
    "        ]\n",
    "        \n",
    "        train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor, \n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform= transforms.Compose(train_transform))\n",
    "\n",
    "        validation_dataset = DatasetGenerator(metadata=metadata.loc[validation_index,:],\n",
    "                                              label_map=label_map,\n",
    "                                              selected_channels=selected_channels,\n",
    "                                              scaling_factor=scaling_factor,\n",
    "                                              reshape_size=reshape_size,\n",
    "                                              transform=transforms.Compose(validation_transform))\n",
    "        \n",
    "        test_dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                        label_map=label_map,\n",
    "                                        selected_channels=selected_channels,\n",
    "                                        scaling_factor=scaling_factor,\n",
    "                                        reshape_size=reshape_size,\n",
    "                                        transform=\n",
    "                                        transforms.Compose(test_transform))\n",
    "        \n",
    "        \n",
    "\n",
    "        resnet18_modified = PretrainedModel(num_channels = len(selected_channels),\n",
    "                                             num_classes = len(set_of_interesting_classes), \n",
    "                                             pretrained = True)\n",
    "        \n",
    "        lr_scheduler = LRScheduler(policy='ReduceLROnPlateau', factor=0.5, patience=5)\n",
    "\n",
    "        epoch_scoring_f1_micro = EpochScoring(\"f1_micro\", \n",
    "                                     name =  \"valid_f1_micro\", \n",
    "                                     on_train = False,\n",
    "                                     lower_is_better = False)\n",
    "\n",
    "        epoch_scoring_f1_macro = EpochScoring(\"f1_macro\", \n",
    "                                     name =  \"valid_f1_macro\", \n",
    "                                     on_train = False,\n",
    "                                     lower_is_better = False)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='valid_f1_macro', \n",
    "                                       patience=50, \n",
    "                                       threshold=0.0001, \n",
    "                                       threshold_mode='rel', \n",
    "                                       lower_is_better=False)\n",
    "\n",
    "        checkpoint = Checkpoint(f_params='synapse_formation_DL_method_comparison.pth',\n",
    "                                monitor='valid_f1_macro_best', load_best=True)\n",
    "        \n",
    "        net = NeuralNetClassifier(    \n",
    "            resnet18_modified, \n",
    "            criterion=nn.CrossEntropyLoss,\n",
    "            lr=0.001,\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=1000,\n",
    "            optimizer=optim.Adam,\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__num_workers=num_workers,\n",
    "            iterator_valid__shuffle=False,\n",
    "            iterator_valid__num_workers=2,\n",
    "            callbacks=[lr_scheduler, epoch_scoring_f1_micro, \n",
    "                       epoch_scoring_f1_macro, \n",
    "                       early_stopping, checkpoint],\n",
    "            train_split=predefined_split(validation_dataset),\n",
    "            device=\"cuda\",\n",
    "            warm_start=True)\n",
    "        net = net.fit(train_dataset, y = None)\n",
    "        net.module.load_state_dict(torch.load('synapse_formation_DL_method_comparison.pth')) \n",
    "        \n",
    "        inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "        preds = net.predict(test_dataset)\n",
    "        preds =  [inv_label_map[int(t)] for t in preds]\n",
    "        \n",
    "        results_with_best_channels = results_with_best_channels.append({\n",
    "            \"method\":met,\n",
    "            \"f1_micro\":f1_score(test_dataset.metadata.label, preds, average=\"micro\"),\n",
    "            \"f1_macro\":f1_score(test_dataset.metadata.label, preds, average=\"macro\"),\n",
    "            \"accuracy\":accuracy_score(test_dataset.metadata.label, preds),\n",
    "        },ignore_index = True)\n",
    "        net = None\n",
    "        resnet18_modified = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"---------------------------\"*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_best_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we calculate the statistics of every channel to later use for nomalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(data = results_with_best_channels, \n",
    "            x = \"method\", \n",
    "            y = \"f1_macro\",\n",
    "           palette = [\"gray\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_best_channels.to_csv(\"results_with_best_channels.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc85c72049d21f557e30689d2619ffc4ab3f684c40ae26d281fe4539bf4644b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
