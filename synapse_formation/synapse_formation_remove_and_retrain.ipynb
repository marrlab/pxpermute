{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scifAI\n",
    "from scifAI.dl.utils import calculate_weights, train_validation_test_split, get_statistics\n",
    "from scifAI.dl.dataset import DatasetGenerator\n",
    "from scifAI.dl.custom_transforms import ShuffleChannel\n",
    "from scifAI.dl.models import PretrainedModel, resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aaaec10abd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata prepration starts...\n",
      "Experiment_1 Donor_1 condition_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15311/15311 [00:02<00:00, 5567.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...metadata prepration ended.\n",
      "CPU times: user 242 ms, sys: 156 ms, total: 398 ms\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_path = \"/pstore/data/DS4/Apoptotic_cell_detection/\"\n",
    "metadata = scifAI.metadata_generator(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = \"models\"\n",
    "log_dir = \"logs\"\n",
    "scaling_factor = 255.\n",
    "reshape_size = 32\n",
    "train_transform = [\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(45)\n",
    "        ]\n",
    "test_transform = [ ]\n",
    "num_classes = len(metadata.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_channels = np.arange(2)\n",
    "num_channels = len(selected_channels)\n",
    "channels =np.asarray([ \"Ch\" + str(i) for i in selected_channels])\n",
    "num_of_all_channels = len(channels)\n",
    "all_channels = np.arange(num_of_all_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "device=\"cuda\"\n",
    "dataset_name = \"apoptotic cells\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the model\n",
    "lrscheduler = LRScheduler(\n",
    "    policy='StepLR', step_size=7, gamma=0.5)\n",
    "number_epochs = 2\n",
    "lr = 0.001\n",
    "momentum=0.9\n",
    "optimizer = optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logging\n",
    "now = datetime.now()\n",
    "timestamp = datetime.timestamp(now)\n",
    "logging.basicConfig(filename=os.path.join(log_dir, 'remove_and_retrain_{}_{}.txt'.format(dataset_name, timestamp)), level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_init = [\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(45)\n",
    "        ]\n",
    "test_transform_init = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = dict(zip(sorted(set(metadata[\"label\"])), np.arange(len(set(metadata[\"label\"])))))\n",
    "num_classes = len(label_map.keys())\n",
    "class_names_targets = [c for c in label_map.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_load_normalize_data(random_state=seed_value, selected_channels=[]):\n",
    "    train_index, validation_index, test_index = train_validation_test_split(metadata.index, metadata[\"label\"], random_state=seed_value)\n",
    "    \n",
    "    # caclculate statistics\n",
    "    train_transform = train_transform_init.copy()\n",
    "    test_transform = test_transform_init.copy()\n",
    "    train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor,\n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform=transforms.Compose(train_transform))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    statistics = get_statistics(train_loader, selected_channels=selected_channels)\n",
    "    \n",
    "    # normalize data\n",
    "    train_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"mean\"]))\n",
    "    test_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"mean\"]))\n",
    "  \n",
    "    \n",
    "    train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor, \n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform= transforms.Compose(train_transform))\n",
    "    validation_dataset = DatasetGenerator(metadata=metadata.loc[validation_index,:],\n",
    "                                      label_map=label_map,\n",
    "                                      selected_channels=selected_channels,\n",
    "                                      scaling_factor=scaling_factor,\n",
    "                                      reshape_size=reshape_size,\n",
    "                                      transform=transforms.Compose(test_transform))\n",
    "    test_dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                    label_map=label_map,\n",
    "                                    selected_channels=selected_channels,\n",
    "                                    scaling_factor=scaling_factor,\n",
    "                                    reshape_size=reshape_size,\n",
    "                                    transform=transforms.Compose(test_transform))\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, validation_dataset, num_channels, selected_channels, seed):\n",
    "    model_saved_name = '{}_net_{}_seed_{}.pth'.format(dataset_name, '_'.join(map(str,selected_channels)), seed)\n",
    "    checkpoint = Checkpoint(f_params=model_saved_name, monitor='valid_loss_best', dirname='models')\n",
    "    net = NeuralNetClassifier(\n",
    "        PretrainedModel, \n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=number_epochs,\n",
    "        module__output_features=num_classes,\n",
    "        module__num_classes=num_classes,\n",
    "        module__num_channels=num_channels, \n",
    "        optimizer=optimizer,\n",
    "        optimizer__momentum=momentum,\n",
    "        iterator_train__shuffle=False,\n",
    "        iterator_train__num_workers=num_workers,\n",
    "        iterator_valid__shuffle=False,\n",
    "        iterator_valid__num_workers=num_workers,\n",
    "        callbacks=[lrscheduler, checkpoint],\n",
    "        train_split=predefined_split(validation_dataset),\n",
    "        device='cuda' # comment to train on cpu\n",
    "    )\n",
    "    net.fit(train_dataset, y=None)\n",
    "    \n",
    "    return model_saved_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_eval_model(num_channels, test_dataset, path_to_the_cp=\"\"):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    # load model\n",
    "    model = PretrainedModel(num_classes, num_channels)\n",
    "    checkpoint = torch.load(os.path.join(model_dir, path_to_the_cp))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # evaluate\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "    y_true_proba = list()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "            outputs = model(inputs)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            true_proba = np.array([j[i] for (i,j) in zip(pred, outputs.cpu())])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (labels.reshape(-1) == predicted).sum().item()\n",
    "            for i in range(len(pred)):\n",
    "                y_true.append(labels[i].item())\n",
    "                y_pred.append(pred[i].item())\n",
    "                y_true_proba.append(true_proba[i].item())\n",
    "    \n",
    "    # save result\n",
    "    logging.info(classification_report(y_true, y_pred, target_names=class_names_targets, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsubsets(s, n_elements):\n",
    "    return list(itertools.combinations(s, n_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:11<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'min': tensor([0.]), 'p01': tensor([0.]), 'p05': tensor([0.]), 'p25': tensor([2.9906]), 'p50': tensor([3.1347]), 'p75': tensor([3.1831]), 'p95': tensor([3.4469]), 'p99': tensor([3.7467]), 'max': tensor([5.2018]), 'mean': tensor([2.7733]), 'std': tensor([1.0245])}\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1        \u001b[36m0.7253\u001b[0m       \u001b[32m0.4796\u001b[0m        \u001b[35m0.7347\u001b[0m     +  0.0010  5.6865\n",
      "      2        \u001b[36m0.6889\u001b[0m       \u001b[32m0.5649\u001b[0m        \u001b[35m0.6858\u001b[0m     +  0.0010  5.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:10<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics used: {'min': tensor([0.]), 'p01': tensor([0.]), 'p05': tensor([0.]), 'p25': tensor([0.0425]), 'p50': tensor([0.0728]), 'p75': tensor([0.1598]), 'p95': tensor([0.7868]), 'p99': tensor([2.1963]), 'max': tensor([250.7146]), 'mean': tensor([0.2146]), 'std': tensor([1.8042])}\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1        \u001b[36m0.6486\u001b[0m       \u001b[32m0.6180\u001b[0m        \u001b[35m0.6354\u001b[0m     +  0.0010  5.7036\n",
      "      2        \u001b[36m0.4094\u001b[0m       \u001b[32m0.6869\u001b[0m        0.8362        0.0010  5.7473\n"
     ]
    }
   ],
   "source": [
    "s = set(all_channels)\n",
    "n_retrain = 1\n",
    "for number_removed_channels in np.array([1]):\n",
    "    all_combinations = findsubsets(s, num_of_all_channels - number_removed_channels)\n",
    "    for channel_comb in all_combinations:\n",
    "        for n in range(n_retrain):\n",
    "            channel_comb = np.asarray(channel_comb)\n",
    "            logging.info(\"Train new model: iteration {}, channels: {}\".format(str(n), '_'.join(map(str, channel_comb))))\n",
    "            num_channels = len(channel_comb)\n",
    "            train_dataset, val_dataset, test_dataset = split_load_normalize_data(random_state=seed_value, selected_channels=channel_comb)\n",
    "            model_path = train_model(train_dataset, val_dataset, num_channels, channel_comb, n)\n",
    "            load_and_eval_model(num_channels, test_dataset, model_path)\n",
    "            os.remove(os.path.join(model_dir, model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the running time to ever step for later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
